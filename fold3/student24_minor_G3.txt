Computers represent information in bits and carry out operations on those symbols using deterministic hardware rules. Words and bytes encode integers and floating-point values through standardized formats, enabling consistent arithmetic. The CPU repeats fetch–decode–execute: it fetches an instruction, interprets its opcode and operands, computes in arithmetic or logic units, and places results into registers or memory. Efficiency comes from pipelining, speculation, and vector execution that apply one instruction to many data items. A hierarchical memory system—from registers through caches to main memory and storage—balances capacity and latency while relying on locality.

Operating systems bridge hardware and applications by scheduling threads, protecting address spaces with virtual memory, and exposing system calls for files, devices, and networking. Peripherals digitize continuous signals via sampling and buffering, while GPUs accelerate massively parallel computations. File systems organize persistent data and recover from faults. Networking protocols carry packets across diverse links and ensure delivery semantics. Altogether, the machine translates programs into orchestrated electrical activity, providing reproducible behavior and scalable performance for a wide range of tasks.