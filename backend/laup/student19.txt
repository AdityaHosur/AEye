The game underwent major development in the 18th century to become England's national sport.[41] Its success was underwritten by the twin necessities of patronage and betting.[42] Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury.[citation needed] The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season.[43] Bowling underwent an evolution around 1760 when bowlers began to pitch (bounce) the ball instead of rolling or skimming it towards the batter. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old "hockey stick" shape.[44][citation needed]

The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point.[citation needed] MCC quickly became the sport's premier club and the custodian of the Laws of Cricket. New Laws introduced in the latter part of the 18th century include the three-stump wicket and leg before wicket (lbw).[45]

The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial.[46] Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839.[47] In December 1889, the eight leading county clubs formed the official County Championship, which began in 1890.[48]


The first recorded photo of a cricket match taken on 25 July 1857 by Roger Fenton
The most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865. It was especially during the career of Grace that the distinction between amateurs and professionals became blurred by the existence of players like him who were nominally amateur but, in terms of their financial gain, de facto professional. Grace himself was said to have been paid more money for playing cricket than any professional.[citation needed]

The last two decades before the First World War have been called the "Golden Age of cricket". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.[49]

Cricket becomes an international sport

The first English team to tour overseas, on board ship to North America, 1859
In 1844, the first international match took place between what were essentially club teams,[50] from the United States and Canada, in New York; Canada won.[51][52] In 1859, a team of English players went to North America on the first overseas tour.[53] Meanwhile, the British Empire had been instrumental in spreading the game overseas, and by the middle of the 19th century it had become well established in Australia, the Caribbean, British India (which also includes present-day Pakistan and Bangladesh), New Zealand, North America and South Africa.[54]

In 1862, an English team made the first tour of Australia.[55] The first Australian team to travel overseas consisted of Aboriginal stockmen who toured England in 1868.[56]

In 1876–77, an England team took part in what was retrospectively recognised as the first-ever Test match at the Melbourne Cricket Ground against Australia.[57] The rivalry between England and Australia gave birth to The Ashes in 1882, which remains Test cricket's most famous contest.[58] Test cricket began to expand in 1888–89 when South Africa played England.[59]

Cricket in the 20th century

Don Bradman of Australia had a record Test batting average of 99.94.
The inter-war years were dominated by Australia's Don Bradman, statistically the greatest Test batter of all time. To curb his dominance, England employed bodyline tactics during the 1932–33 Ashes series. These involved bowling at the body of the batter and setting a field, resulting in batters having to choose between being hit or risk getting out. This series moved cricket from a game to a matter of national importance, with diplomatic cables being passed between the two countries over the incident.[60]

During this time, the number of Test nations continued to grow, with the West Indies, New Zealand and India being admitted as full Test members within a four-year period from 1928 to 1932.

An enforced break during the Second World War stopped Test Cricket for a time, although the Partition of India caused Pakistan to gain Test status in 1952. As teams began to travel more, the game quickly grew from 500 tests in 84 years to 1000 within the next 23.

Cricket entered a new era in 1963 when English counties introduced the limited overs variant.[61] As it was sure to produce a result, limited overs cricket was lucrative, and the number of matches increased.[62] The first Limited Overs International was played in 1971, and the governing International Cricket Council (ICC), seeing its potential, staged the first limited overs Cricket World Cup in 1975.[63]

Sri Lanka joined the ranks in 1982. Meanwhile, South Africa was banned by the ICC due to apartheid from 1970 until 1992. 1992 also brought about the introduction of the Zimbabwe team.[64]

Cricket in the 21st century

The Indian Premier League (IPL) was launched in 2008. It has become one of the richest sports leagues in the world, and has greatly increased the importance of T20 cricket and franchise leagues.[65]
The 21st century brought with it the Bangladesh Team, who made their Test debut in 2000. The game itself also grew, with a new format made up of 20-over innings being created. This format, called T20 cricket, quickly became a highly popular format, putting the longer formats at risk. The new shorter format also introduced franchise cricket, with new tournaments like the Indian Premier League and the Australian Big Bash League. The ICC has selected the T20 format as cricket's growth format, and has introduced a T20 World Cup which is played every two years;[66] T20 cricket has also been increasingly accepted into major events such as the Asian Games.[67] The resultant growth has seen cricket's fanbase cross one billion people, with 90% of them in South Asia.[2] T20's success has also spawned even shorter formats, such as 10-over cricket (T10) and 100-ball cricket, though not without controversy.[68]

Outside factors have also taken their toll on cricket. For example, the 2008 Mumbai attacks led India and Pakistan to suspend their bilateral series indefinitely. The 2009 attack on the Sri Lankan team during their tour of Pakistan led to Pakistan being unable to host matches until 2019.[69][70][71][72]

In 2017, Afghanistan and Ireland became the 11th and 12th Test nations.[73][74]

Laws and gameplay
Main article: Laws of Cricket

A typical cricket field
In cricket, the rules of the game are codified in The Laws of Cricket (hereinafter called "the Laws"), which has a global remit. There are 42 Laws (always written with a capital "L"). The earliest known version of the code was drafted in 1744, and since 1788, it has been owned and maintained by its custodian, the Marylebone Cricket Club (MCC) in London.[75]

Playing area
Main articles: Cricket field, Cricket pitch, Crease (cricket), and Wicket
Cricket is a bat-and-ball game played on a cricket field (see image of cricket pitch and creases) between two teams of eleven players each.[76] The field is usually circular or oval in shape, and the edge of the playing area is marked by a boundary, which may be a fence, part of the stands, a rope, a painted line, or a combination of these; the boundary must if possible be marked along its entire length.[77]

In the approximate centre of the field is a rectangular pitch (see image, below) on which a wooden target called a wicket is sited at each end; the wickets are placed 22 yards (20 m) apart.[78] The pitch is a flat surface 10 feet (3.0 m) wide, with very short grass that tends to be worn away as the game progresses (cricket can also be played on artificial surfaces, notably matting). Each wicket is made of three wooden stumps topped by two bails.[79]


Cricket pitch and creases
As illustrated, the pitch is marked at each end with four white painted lines: a bowling crease, a popping crease and two return creases. The three stumps are aligned centrally on the bowling crease, which is eight feet eight inches long. The popping crease is drawn four feet in front of the bowling crease and parallel to it; although it is drawn as a 12 ft (3.7 m) line (six feet on either side of the wicket), it is, in fact, unlimited in length. The return creases are drawn at right angles to the popping crease so that they intersect the ends of the bowling crease; each return crease is drawn as an 8 ft (2.4 m) line, so that it extends four feet behind the bowling crease, but is also, in fact, unlimited in length.[80]

Match structure
Main article: Innings
Before a match begins, the team captains (who are also players) toss a coin to decide which team will bat first and so take the first innings.[81] "Innings" is the term used for each phase of play in the match.[81] In each innings, one team bats, attempting to score runs, while the other team bowls and fields the ball, attempting to restrict the scoring and dismiss the batters.[82][83] When the first innings ends, the teams change roles; there can be two to four innings depending upon the type of match. A match with four scheduled innings is played over three to five days; a match with two scheduled innings is usually completed in a single day.[81] During an innings, all eleven members of the fielding team take the field, but usually only two members of the batting team are on the field at any given time.[a] The order of batters is usually announced just before the match, but it can be varied.[76]

The main objective of each team is to score more runs than their opponents, but in some forms of cricket, it is also necessary to dismiss all but one of the opposition batters (making their team 'all out') in their final innings in order to win the match, which would otherwise be drawn (not ending with a winner or tie.)[86]

Clothing and equipment
Main article: Cricket clothing and equipment

English cricketer W. G. Grace "taking guard" in 1883. His pads and bat are very similar to those used today. The gloves have evolved somewhat. Many modern players use more defensive equipment than were available to Grace, most notably helmets and arm guards.
The wicket-keeper (a specialised fielder behind the batter) and the batters wear protective gear because of the hardness of the ball, which can be delivered at speeds of more than 145 kilometres per hour (90 mph) and presents a major health and safety concern. Protective clothing includes pads (designed to protect the knees and shins), batting gloves or wicket-keeper's gloves for the hands, a safety helmet for the head, and a box for male players inside the trousers (to protect the crotch area).[87] Some batters wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The only fielders allowed to wear protective gear are those in positions very close to the batter (i.e., if they are alongside or in front of him), but they cannot wear gloves or external leg guards.[88]

Subject to certain variations, on-field clothing generally includes a collared shirt with short or long sleeves; long trousers; woolen pullover (if needed); cricket cap (for fielding) or a safety helmet; and spiked shoes or boots to increase traction. The kit is traditionally all white, and this remains the case in Test and first-class cricket, but in limited overs cricket, team colours are now worn instead.[89]

Bat and ball
Main articles: Cricket bat and Cricket ball
New white ball
Used red ball
Used pink ball
The three types of cricket balls used in international matches, all of the same size:
i) A new white ball. White balls are mainly used in limited overs cricket, especially in matches played at night, under floodlights (left).
ii) A used red ball. Red balls are used in day Test cricket, first-class cricket and some other forms of cricket (center).

iii) A used pink ball. Pink balls are used in day/night Test cricket (right).
The essence of the sport is that a bowler delivers (i.e., bowls) the ball from their end of the pitch towards the batter who, armed with a bat, is "on strike" at the other end (see next sub-section: Basic gameplay).

The bat is made of wood, usually Salix alba (white willow), and has the shape of a blade topped by a cylindrical handle. The blade must not be more than 4.25 inches (10.8 cm) wide and the total length of the bat not more than 38 inches (97 cm). There is no standard for the weight, which is usually between 2 lb 7 oz and 3 lb (1.1 and 1.4 kg).[90][91]

The ball is a hard leather-seamed spheroid, with a circumference of 9 inches (23 cm). The ball has a "seam": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent and helps the bowler propel it in a less predictable manner. During matches, the quality of the ball deteriorates to a point where it is no longer usable; during the course of this deterioration, its behaviour in flight will change and can influence the outcome of the match. Players will, therefore, attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva was legal, even when the polishing was deliberately done on one side only to increase the ball's swing through the air. The use of saliva has since been made illegal due to the COVID-19 pandemic.[92] The acts of rubbing other substances into the ball, scratching the surface or picking at the seams constitute illegal ball tampering.[93]

Player roles
Basic gameplay: bowler to batter
During normal play, thirteen players and two umpires are on the field. Two of the players are batters and the rest are all eleven members of the fielding team. The other nine players in the batting team are off the field in the pavilion. The image with overlay below shows what is happening when a ball is being bowled and which of the personnel are on or close to the pitch.[94]

123456778910111212
1	Umpire
2	Wicket
3	Non-striking batter
4	Bowler
5	Ball
6	Pitch
7	Popping crease
8	Striking batter
9	Wicket
10	Wicket-keeper
11	First slip
12	Return crease
In the photo, the two batters (3 and 8, wearing yellow) have taken position at each end of the pitch (6). Three members of the fielding team (4, 10 and 11, wearing dark blue) are in shot. One of the two umpires (1, wearing white hat) is stationed behind the wicket (2) at the bowler's (4) end of the pitch. The bowler (4) is bowling the ball (5) from his end of the pitch to the batter (8) at the other end who is called the "striker". The other batter (3) at the bowling end is called the "non-striker". The wicket-keeper (10), who is a specialist, is positioned behind the striker's wicket (9), and behind him stands one of the fielders in a position called "first slip" (11). While the bowler and the first slip are wearing conventional kit only, the two batters and the wicket-keeper are wearing protective gear, including safety helmets, padded gloves and leg guards (pads). The wicket-keeper is the only fielding player able to wear protective gloves.

While the umpire (1) in shot stands at the bowler's end of the pitch, his colleague stands in the outfield, usually in or near the fielding position called "square leg", so that he is in line with the popping crease (7) at the striker's end of the pitch. The bowling crease (not numbered) is the one on which the wicket is located between the return creases (12). The bowler (4) intends to hit the wicket (9) with the ball (5) or at least prevent the striker (8) from scoring runs. The striker (8) intends, by using his bat, to defend his wicket and, if possible, hit the ball away from the pitch in order to score runs.

Some players are skilled in both batting and bowling, so are termed all-rounders. Bowlers are classified according to their style and speed, generally as fast bowlers, seam bowlers or spinners. Batters are classified according to whether they are right-handed or left-handed, with switch-hitting uncommon and largely utilised as a tactic, where a batter changes stance shortly before the bowler releases the ball.[95]

Overs
Main article: Over (cricket)
The Laws state that, throughout an innings, "the ball shall be bowled from each end alternately in overs of 6 balls".[96] The name "over" came about because the umpire calls "Over!" when six legal balls (deliveries) have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batters do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs which are termed a "spell"; if the captain wants a bowler to "change ends", another bowler must temporarily fill in so that the change is not immediate. The batters do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice versa. The umpires also change positions so that the one who was at "square leg" now stands behind the wicket at the nonstriker's end and vice versa.[96]

Fielding
Main article: Fielding (cricket)

Fielding positions in cricket for a right-handed batter
Of the eleven fielders, three are in shot in the image above. The other eight are elsewhere on the field, their positions determined on a tactical basis by the captain or the bowler. Fielders often change position between deliveries, again as directed by the captain or bowler.[88]

If a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of the aforementioned fielder, but the substitute cannot bowl or act as a captain, except in the case of concussion substitutes in international cricket.[85] The substitute leaves the field when the injured player is fit to return.[97] The Laws of Cricket were updated in 2017 to allow substitutes to act as wicket-keepers.[98]

Batting and scoring
Main articles: Batting (cricket), Run (cricket), and Extra (cricket)

The directions in which a right-handed batter, facing down the page, intends to send the ball when playing various cricketing shots. The diagram for a left-handed batter is a mirror image of this one.
Batters take turns to bat via a batting order which is decided beforehand by the team captain and presented to the umpires, though the order remains flexible when the captain officially nominates the team.[76] Substitute batters are generally not allowed,[97] except in the case of concussion substitutes in international cricket.[85]

In order to begin batting the batter first adopts a batting stance. Standardly, this involves adopting a slight crouch with the feet pointing across the front of the wicket, looking in the direction of the bowler, and holding the bat so it passes over the feet and so its tip can rest on the ground near to the toes of the back foot.[99]

A skilled batter can use a wide array of "shots" or "strokes" in both defensive and attacking mode. The idea is to hit the ball to the best effect with the flat surface of the bat's blade. If the ball touches the side of the bat, it is called an "edge". The batter does not have to play a shot and can allow the ball to go through to the wicket-keeper. Equally, the batter does not have to attempt a run when hitting the ball with their bat. Batters do not always seek to hit the ball as hard as possible, and a good player can score runs by simply making a deft stroke with a turn of the wrists, or by simply "blocking" the ball but directing it away from fielders so that the player has time to take a run. A wide variety of shots are played, the batter's repertoire including strokes named according to the style of swing and the direction aimed: e.g., "cut", "drive", "hook", and "pull".[100]


Sachin Tendulkar is the only player to have scored one hundred international centuries.
The batter on strike (i.e., the "striker") must prevent the ball from hitting the wicket and try to score runs by hitting the ball with their bat so that the batter and their partner have time to switch places, with each of them running from one end of the pitch to the other before the fielding side can return the ball and attempt a run out (throwing the ball at one of the wickets before the run is scored.) To register a run, both runners must touch the ground behind the popping crease with either their bats or their bodies (the batters carry their bats as they run) before a fielder can throw the ball at the nearby wicket. Each completed For other uses, see Theatre (disambiguation). "Theatrical" redirects here. For the racehorse, see Theatrical (horse).




Clockwise, from left to right:
Sarah Bernhardt in 1899 as Hamlet in Shakespeare's eponymous tragedy
The character Sun Wukong at the Peking opera from Journey to the West
Eduardo De Filippo as Pulcinella, a character from the Commedia dell'arte
Koothu, an ancient Indian form of performing art that originated in early Tamilakam
Part of a series on
Performing arts

AcrobaticsBalletCircus skillsClownDanceGymnasticsMagicMimeMusicOperaProfessional wrestlingPuppetrySpeechStand-up comedyStreet performanceTheatreVentriloquism
vte
Theatre or theater[a] is a collaborative form of performing art that uses live performers, usually actors, to present experiences of a real or imagined event before a live audience in a specific place, often a stage. The performers may communicate this experience to the audience through combinations of gesture, speech, song, music, and dance. It is the oldest form of drama, though live theatre has now been joined by modern recorded forms. Elements of art, such as painted scenery and stagecraft such as lighting are used to enhance the physicality, presence and immediacy of the experience.[1] Places, normally buildings, where performances regularly take place are also called "theatres" (or "theaters"), as derived from the Ancient Greek θέατρον (théatron, "a place for viewing"), itself from θεάομαι (theáomai, "to see", "to watch", "to observe").

Modern Western theatre comes, in large measure, from the theatre of ancient Greece, from which it borrows technical terminology, classification into genres, and many of its themes, stock characters, and plot elements. Theatre artist Patrice Pavis defines theatricality, theatrical language, stage writing and the specificity of theatre as synonymous expressions that differentiate theatre from the other performing arts, literature and the arts in general.[2][b]

A theatre company is an organisation that produces theatrical performances,[3] as distinct from a theatre troupe (or acting company), which is a group of theatrical performers working together.[4][5]

Modern theatre includes performances of plays and musical theatre. The art forms of ballet and opera are also theatre and use many conventions such as acting, costumes and staging. They were influential in the development of musical theatre.

History of theatre
Main article: History of theatre
Classical, Hellenistic Greece and Magna Graecia
Main article: Theatre of ancient Greece

The best-preserved example of a classical Greek theatre, the Ancient Theatre of Epidaurus, has a circular orchêstra and probably gives the best idea of the original shape of the Athenian theatre, though it dates from the 4th century BC.[6]

Greek Theater of Taormina, Sicily, Magna Graecia, present-day Italy
The city-state of Athens is where Western theatre originated.[7][8][9][c] It was part of a broader culture of theatricality and performance in classical Greece that included festivals, religious rituals, politics, law, athletics and gymnastics, music, poetry, weddings, funerals, and symposia.[10][9][11][12][d]

Participation in the city-state's many festivals—and mandatory attendance at the City Dionysia as an audience member (or even as a participant in the theatrical productions) in particular—was an important part of citizenship.[14] Civic participation also involved the evaluation of the rhetoric of orators evidenced in performances in the law-court or political assembly, both of which were understood as analogous to the theatre and increasingly came to absorb its dramatic vocabulary.[15][16] The Greeks also developed the concepts of dramatic criticism and theatre architecture.[17][18][19][failed verification] Actors were either amateur or at best semi-professional.[20] The theatre of ancient Greece consisted of three types of drama: tragedy, comedy, and the satyr play.[21]

The origins of theatre in ancient Greece, according to Aristotle (384–322 BCE), the first theoretician of theatre, are to be found in the festivals that honoured Dionysus. The performances were given in semi-circular auditoria cut into hillsides, capable of seating 10,000–20,000 people. The stage consisted of a dancing floor (orchestra), dressing room and scene-building area (skene). Since the words were the most important part, good acoustics and clear delivery were paramount. The actors (always men) wore masks appropriate to the characters they represented, and each might play several parts.[22]

Athenian tragedy—the oldest surviving form of tragedy—is a type of dance-drama that formed an important part of the theatrical culture of the city-state.[7][8][9][23][24][e] Having emerged sometime during the 6th century BCE, it flowered during the 5th century BCE (from the end of which it began to spread throughout the Greek world), and continued to be popular until the beginning of the Hellenistic period.[26][27][8][f]

No tragedies from the 6th century BCE and only 32 of the more than a thousand that were performed in during the 5th century BCE have survived.[29][30][g] We have complete texts extant by Aeschylus, Sophocles, and Euripides.[31][h] The origins of tragedy remain obscure, though by the 5th century BCE it was institutionalized in competitions (agon) held as part of festivities celebrating Dionysus (the god of wine and fertility).[32][33] As contestants in the City Dionysia's competition (the most prestigious of the festivals to stage drama) playwrights were required to present a tetralogy of plays (though the individual works were not necessarily connected by story or theme), which usually consisted of three tragedies and one satyr play.[34][35][i] The performance of tragedies at the City Dionysia may have begun as early as 534 BCE; official records (didaskaliai) begin from 501 BCE, when the satyr play was introduced.[36][34][j]

Most Athenian tragedies dramatize events from Greek mythology, though The Persians—which stages the Persian response to news of their military defeat at the Battle of Salamis in 480 BCE—is the notable exception in the surviving drama.[34][k] When Aeschylus won first prize for it at the City Dionysia in 472 BCE, he had been writing tragedies for more than 25 years, yet its tragic treatment of recent history is the earliest example of drama to survive.[34][38] More than 130 years later, the philosopher Aristotle analysed 5th-century Athenian tragedy in the oldest surviving work of dramatic theory—his Poetics (c. 335 BCE).

Athenian comedy is conventionally divided into three periods, "Old Comedy", "Middle Comedy", and "New Comedy". Old Comedy survives today largely in the form of the eleven surviving plays of Aristophanes, while Middle Comedy is largely lost (preserved only in relatively short fragments in authors such as Athenaeus of Naucratis). New Comedy is known primarily from the substantial papyrus fragments of Menander. Aristotle defined comedy as a representation of laughable people that involves some kind of blunder or ugliness that does not cause pain or disaster.[l]

In addition to the categories of comedy and tragedy at the City Dionysia, the festival also included the Satyr Play. Finding its origins in rural, agricultural rituals dedicated to Dionysus, the satyr play eventually found its way to Athens in its most well-known form. Satyr's themselves were tied to the god Dionysus as his loyal woodland companions, often engaging in drunken revelry and mischief at his side. The satyr play itself was classified as tragicomedy, erring on the side of the more modern burlesque traditions of the early twentieth century. The plotlines of the plays were typically concerned with the dealings of the pantheon of Gods and their involvement in human affairs, backed by the chorus of Satyrs. However, according to Webster, satyr actors did not always perform typical satyr actions and would break from the acting traditions assigned to the character type of a mythical forest creature.[39]

The Greek colonists in Southern Italy, the so-called Magna Graecia, brought theatrical art from their motherland.[40] The Greek Theatre of Syracuse, the Greek Theatre of Segesta [it], the Greek Theatre of Tindari [it], the Greek Theatre of Hippana [it], the Greek Theatre of Akrai [it], the Greek Theatre of Monte Jato [it], the Greek Theatre of Morgantina [it] and the most famous Greek Theater of Taormina, amply demonstrate this. Only fragments of original dramaturgical works are left, but the tragedies of the three great giants Aeschylus, Sophocles and Euripides and the comedies of Aristophanes are known.[41] Some famous playwrights in the Greek language came directly from Magna Graecia. Others, such as Aeschylus and Epicharmus, worked for a long time in Sicily. Epicharmus can be considered Syracusan in all respects, having worked all his life with the tyrants of Syracuse. His comedy preceded that of the more famous Aristophanes by staging the gods for the first time in comedy. While Aeschylus, after a long stay in the Sicilian colonies, died in Sicily in the colony of Gela in 456 BC. Epicarmus and Phormis, both of 6th century BC, are the basis, for Aristotle, of the invention of the Greek comedy, as he says in his book on Poetics:[42]

As for the composition of the stories (Epicharmus and Phormis) it came in the beginning from Sicily

— Aristotle, Poetics
Other native dramatic authors of Magna Graecia, in addition to the Syracusan Formides mentioned, are Achaeus of Syracuse, Apollodorus of Gela, Philemon of Syracuse and his son Philemon the younger. From Calabria, precisely from the colony of Thurii, came the playwright Alexis. While Rhinthon, although Sicilian from Syracuse, worked almost exclusively for the colony of Taranto in Apulia.[43]

Roman theatre
Main article: Theatre of ancient Rome

Roman theatre of Benevento

Roman mosaic depicting actors and an aulos player (House of the Tragic Poet, Pompeii).
Western theatre developed and expanded considerably under the Romans. The Roman historian Livy wrote that the Romans first experienced theatre in the 4th century BC, with a performance by Etruscan actors.[44] Beacham argues that Romans had been familiar with "pre-theatrical practices" for some time before that recorded contact.[45] The theatre of ancient Rome was a thriving and diverse art form, ranging from festival performances of street theatre, nude dancing, and acrobatics, to the staging of Plautus's broadly appealing situation comedies, to the high-style, verbally elaborate tragedies of Seneca. Although Rome had a native tradition of performance, the Hellenization of Roman culture in the 3rd century BC had a profound and energizing effect on Roman theatre and encouraged the development of Latin literature of the highest quality for the stage.

Following the expansion of the Roman Republic (509–27 BC) into several Greek territories between 270 and 240 BC, Rome encountered Greek drama.[46] From the later years of the republic and by means of the Roman Empire (27 BC-476 AD), theatre spread west across Europe, around the Mediterranean and reached England; Roman theatre was more varied, extensive and sophisticated than that of any culture before it.[47] While Greek drama continued to be performed throughout the Roman period, the year 240 BC marks the beginning of regular Roman drama.[46][m] From the beginning of the empire, however, interest in full-length drama declined in favour of a broader variety of theatrical entertainments.[48]

The first important works of Roman literature were the tragedies and comedies that Livius Andronicus wrote from 240 BC.[49] Five years later, Gnaeus Naevius also began to write drama.[49] No plays from either writer have survived. While both dramatists composed in both genres, Andronicus was most appreciated for his tragedies and Naevius for his comedies; their successors tended to specialise in one or the other, which led to a separation of the subsequent development of each type of drama.[49] By the beginning of the 2nd century BC, drama was firmly established in Rome and a guild of writers (collegium poetarum) had been formed.[50]

The Roman comedies that have survived are all fabula palliata (comedies based on Greek subjects) and come from two dramatists: Titus Maccius Plautus (Plautus) and Publius Terentius Afer (Terence).[51] In re-working the Greek originals, the Roman comic dramatists abolished the role of the chorus in dividing the drama into episodes and introduced musical accompaniment to its dialogue (between one-third of the dialogue in the comedies of Plautus and two-thirds in those of Terence).[52] The action of all scenes is set in the exterior location of a street and its complications often follow from eavesdropping.[52] Plautus, the more popular of the two, wrote between 205 and 184 BC and twenty of his comedies survive, of which his farces are best known; he was admired for the wit of his dialogue and his use of a variety of poetic meters.[53] All of the six comedies that Terence wrote between 166 and 160 BC have survived; the complexity of his plots, in which he often combined several Greek originals, was sometimes denounced, but his double-plots enabled a sophisticated presentation of contrasting human behaviour.[53]

No early Roman tragedy survives, though it was highly regarded in its day; historians know of three early tragedians—Quintus Ennius, Marcus Pacuvius and Lucius Accius.[52] From the time of the empire, the work of two tragedians survives—one is an unknown author, while the other is the Stoic philosopher Seneca.[54] Nine of Seneca's tragedies survive, all of which are fabula crepidata (tragedies adapted from Greek originals); his Phaedra, for example, was based on Euripides' Hippolytus.[55] Historians do not know who wrote the only extant example of the fabula praetexta (tragedies based on Roman subjects), Octavia, but in former times it was mistakenly attributed to Seneca due to his appearance as a character in the tragedy.[54]

In contrast to Ancient Greek theatre, the theatre in Ancient Rome did allow female performers. While the majority were employed for dancing and singing, a minority of actresses are known to have performed speaking roles, and there were actresses who achieved wealth, fame and recognition for their art, such as Eucharis, Dionysia, Galeria Copiola and Fabia Arete: they also formed their own acting guild, the Sociae Mimae, which was evidently quite wealthy.[56]
A film[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and some times using other sensory stimulations.[1]

Films are produced by recording actual people and objects with cameras or by creating them using animation techniques and special effects. They comprise a series of individual frames, but when these images are shown rapidly in succession, the illusion of motion is given to the viewer. Flickering between frames is not seen due to an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement.

Films are considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences. The visual elements of cinema need no translation, giving the motion picture a universal power of communication. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the dialogue. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them.

History
Main articles: History of film technology and History of film
Precursors
Main article: Precursors of film
The art of film has drawn on several earlier traditions in fields such as oral storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving or projected images such as shadowgraphy, camera obscura, shadow puppetry and magic lantern.[2][3][4]

1830s–1880s: Before celluloid

Animated GIF of Prof. Stampfer's Stroboscopische Scheibe No. X (Trentsensky & Vieweg 1833)
The stroboscopic animation principle was introduced in 1833 with the stroboscopic disc (better known as the phénakisticope) and later applied in the zoetrope (since 1866), the flip book (since 1868), and the praxinoscope (since 1877), before it became the basic principle for cinematography.[citation needed]

Experiments with early phénakisticope-based animation projectors were made at least as early as 1843 and publicly screened in 1847. Jules Duboscq marketed phénakisticope projection systems in France from c. 1853 until the 1890s.[citation needed]

Photography was introduced in 1839, but initially photographic emulsions needed such long exposures that the recording of moving subjects seemed impossible. At least as early as 1844, photographic series of subjects posed in different positions were created to either suggest a motion sequence or document a range of different viewing angles. The advent of stereoscopic photography, with early experiments in the 1840s and commercial success since the early 1850s, raised interest in completing the photographic medium with the addition of means to capture color and motion. In 1849, Joseph Plateau published about the idea to combine his invention of the phénakisticope with the stereoscope, as suggested to him by stereoscope inventor Charles Wheatstone, and to use photographs of plaster sculptures in different positions to be animated in the combined device. In 1852, Jules Duboscq patented such an instrument as the "Stéréoscope-fantascope, ou Bïoscope", but he only marketed it very briefly, without success. One Bïoscope disc with stereoscopic photographs of a machine is in the Plateau collection of Ghent University, but no instruments or other discs have yet been found.[citation needed]


An animation of the retouched Sallie Garner card from The Horse in Motion series (1878–1879) by Muybridge
By the late 1850s, the first examples of instantaneous photography came about and provided hope that motion photography would soon be possible, but it took a few decades before it was successfully combined with a method to record series of sequential images in real-time. In 1878, Eadweard Muybridge eventually managed to take a series of photographs of a running horse with a battery of cameras in a line along the track and published the results as The Horse in Motion on cabinet cards. Muybridge, as well as Étienne-Jules Marey, Ottomar Anschütz and many others, would create many more chronophotography studies. Muybridge had the contours of dozens of his chronophotographic series traced onto glass discs and projected them with his zoopraxiscope in his lectures from 1880 to 1895.[citation needed]


An Anschütz electrotachyscope American Scientific, 16/11/1889, p. 303
Anschütz made his first instantaneous photographs in 1881. He developed a portable camera that allowed shutter speeds as short as 1/1000 of a second in 1882. The quality of his pictures was generally regarded as much higher than that of the chronophotography works of Muybridge and Étienne-Jules Marey.[5] In 1886, Anschütz developed the Electrotachyscope, an early device that displayed short motion picture loops with 24 glass plate photographs on a 1.5 meter wide rotating wheel that was hand-cranked to a speed of circa 30 frames per second. Different versions were shown at many international exhibitions, fairs, conventions, and arcades from 1887 until at least 1894. Starting in 1891, some 152 examples of a coin-operated peep-box Electrotachyscope model were manufactured by Siemens & Halske in Berlin and sold internationally.[5][6] Nearly 34,000 people paid to see it at the Berlin Exhibition Park in the summer of 1892. Others saw it in London or at the 1893 Chicago World's Fair. On 25 November 1894, Anschütz introduced a Electrotachyscope projector with a 6x8 meter screening in Berlin. Between 22 February and 30 March 1895, a total of circa 7,000 paying customers came to view a 1.5-hour show of some 40 scenes at a 300-seat hall in the old Reichstag building in Berlin.[7]

Duration: 4 minutes and 59 seconds.4:59Subtitles available.CC
Pauvre Pierrot (1892) repainted clip
Émile Reynaud already mentioned the possibility of projecting images of the Praxinoscope in his 1877 patent application. He presented a praxinoscope projection device at the Société française de photographie on 4 June 1880, but did not market his praxinoscope a projection before 1882. He then further developed the device into the Théâtre Optique which could project longer sequences with separate backgrounds, patented in 1888. He created several movies for the machine by painting images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Musée Grévin in Paris.[citation needed]

1880s–1890s: First motion pictures

A frame from Roundhay Garden Scene, the world's earliest surviving film produced using a motion picture camera, by Louis Le Prince, 1888

A frame from L'Arrivée d'un train en gare de La Ciotat, one of earliest films released in theaters, 1896
By the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph a rapid sequence of images using only one lens, allowed action to be captured and stored on a single compact reel of film.[citation needed]

Movies were initially shown publicly to one person at a time through "peep show" devices such as the Electrotachyscope, Kinetoscope and the Mutoscope. Not much later, exhibitors managed to project films on large screens for theatre audiences.[citation needed]

The first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons (using films produced by their Eidoloscope company),[8] by the Skladanowsky brothers, and by French brothers Auguste and Louis Lumière, best known for L'Arrivée d'un train en gare de La Ciotat (1896),[9] with ten of their own productions.[10] Private screenings had preceded these by several months, with Latham's slightly predating the others'.[citation needed]

1910s: Early evolution
The earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Typical films showed employees leaving a factory gate, people walking in the street, and the view from the front of a trolley as it traveled a city's Main Street. According to legend, when a film showed a locomotive at high speed approaching the audience, the audience panicked and ran from the theater. Around the turn of the 20th century, films started stringing several scenes together to tell a story. (The filmmakers who first put several shots or scenes discovered that, when one shot follows another, that act establishes a relationship between the content in the separate shots in the minds of the viewer. It is this relationship that makes all film storytelling possible. In a simple example, if a person is shown looking out a window, whatever the next shot shows, it will be regarded as the view the person was seeing.) Each scene was a single stationary shot with the action occurring before it. The scenes were later broken up into multiple shots photographed from different distances and angles.[citation needed]

Duration: 1 minute and 48 seconds.1:48
A clip from the Charlie Chaplin silent film The Bond (1918)
Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions.[citation needed]

The rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium.[citation needed]

1920s–1960s: Evolution in sound
In the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen.[citation needed] The resulting sound films were initially distinguished from the usual silent "moving pictures" or "movies" by calling them "talking pictures" or "talkies."[11] The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as "the old medium."[citation needed]

Duration: 1 hour, 28 minutes and 57 seconds.1:28:57
Sound in cinema started gaining acceptance with movies like The Jazz Singer (1927)
The evolution of sound in cinema began with the idea of combining moving images with existing phonograph sound technology. Early sound-film systems, such as Thomas Edison's Kinetoscope and the Vitaphone used by Warner Bros., laid the groundwork for synchronized sound in film. The Vitaphone system, produced alongside Bell Telephone Company and Western Electric, faced initial resistance due to expensive equipping costs, but sound in cinema gained acceptance with movies like Don Juan (1926) and The Jazz Singer (1927).[12][13]

American film studios, while Europe standardized on Tobis-Klangfilm and Tri-Ergon systems. This new technology allowed for greater fluidity in film, giving rise to more complex and epic movies like King Kong (1933).[14]

As the television threat emerged in the 1940s and 1950s, the film industry needed to innovate to attract audiences. In terms of sound technology, this meant the development of surround sound and more sophisticated audio systems, such as Cinerama's seven-channel system. However, these advances required a large number of personnel to operate the equipment and maintain the sound experience in theaters.[14]

In 1966, Dolby Laboratories introduced the Dolby A noise reduction system, which became a standard in the recording industry and eliminated the hissing sound associated with earlier standardization efforts. Dolby Stereo, a revolutionary surround sound system, followed and allowed cinema designers to take acoustics into consideration when designing theaters. This innovation enabled audiences in smaller venues to enjoy comparable audio experiences to those in larger city theaters.[15]

Today, the future of sound in film remains uncertain, with potential influences from artificial intelligence, remastered audio, and personal viewing experiences shaping its development.[16][17] However, it is clear that the evolution of sound in cinema has been marked by continuous innovation and a desire to create more immersive and engaging experiences for audiences.[citation needed]

1930s: Evolution in color
A significant technological advancement in film was the introduction of "natural color," where color was captured directly from nature through photography, as opposed to being manually added to black-and-white prints using techniques like hand-coloring or stencil-coloring.[18][19] Early color processes often produced colors that appeared far from "natural".[20] Unlike the rapid transition from silent films to sound films, color's replacement of black-and-white happened more gradually.[21]

The crucial innovation was the three-strip version of the Technicolor process, first used in animated cartoons in 1932.[22][23] The process was later applied to live-action short films, specific sequences in feature films, and finally, for an entire feature film, Becky Sharp, in 1935.[24] Although the process was expensive, the positive public response, as evidenced by increased box office revenue, generally justified the additional cost.[18] Consequently, the number of films made in color gradually increased year after year.[25][26] One of the first mainstream films to use color was The Wizard of Oz (1939).[27][28]

1950s: growing influence of television
Duration: 1 hour, 35 minutes and 53 seconds.1:35:53Subtitles available.CC
Night of the Living Dead (1968) has been listed as one of the most influential horror film ever made.[29]
In the early 1950s, black-and-white television started receiving criticism with many believing that television failed to reach the lofty intellectual and cultural expectations that accompanied its introduction.[30] In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color.[31]

During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of "star" filmmakers such as Peter Bogdanovich, Martin Scorsese and Alfred Hitchcock with his film Psycho (1960).[32]

1960s–present: Modern cinema

Salah Zulfikar, one of the most popular actors in the golden age of Egyptian Cinema.[citation needed]
The decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, New German Cinema wave, Indian New Wave, Japanese New Wave, New Hollywood, and Egyptian New Wave) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century.[citation needed]

Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and that became briefly popular in the early 2010s with films like Avatar (2009).[33] Large-screen cinemas systems using 35mm and 70mm film were developed in the late 2010s, with companies like the IMAX corporation.[34]

Film theory
16 mm spring-wound Bolex H16 Reflex camera
This 16 mm spring-wound Bolex "H16" Reflex camera is a popular entry level camera used in film schools.
"Film theory" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's manifest The Birth of the Sixth Art. The Moscow Film School, the oldest film school in the world, was founded in 1919, in order to teach about and research film theory. Formalist film theory, led by Rudolf Arnheim, Béla Balázs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. André Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life.[citation needed]

Language
Film is considered to have its own language.[citation needed] James Monaco wrote a classic text on film theory, titled "How to Read a Film," that addresses this.[35] Director Ingmar Bergman famously said, "Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream."[36] An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation.[citation needed] This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing.[citation needed] The "Hollywood style" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition.[citation needed]

Montage
Main article: Montage (filmmaking)
Montage is a film editing technique in which separate pieces of film are selected, edited, and assembled to create a new section or sequence within a film. This technique can be used to convey a narrative or to create an emotional or intellectual effect by juxtaposing different shots, often for the purpose of condensing time, space, or information. Montage can involve flashbacks, parallel action, or the interplay of various visual elements to enhance the storytelling or create symbolic meaning.[37]

The concept of montage emerged in the 1920s, with pioneering Soviet filmmakers such as Sergei Eisenstein and Lev Kuleshov developing the theory of montage. Eisenstein's film Battleship Potemkin (1925) is a prime example of the innovative use of montage, where he employed complex juxtapositions of images to create a visceral impact on the audience.[38]

As the art of montage evolved, filmmakers began incorporating musical and visual counterpoint to create a more dynamic and engaging experience for the viewer. The development of scene construction through mise-en-scène, editing, and special effects led to more sophisticated techniques that can be compared to those utilized in opera and ballet.[39]

The French New Wave movement of the late 1950s and 1960s also embraced the montage technique, with filmmakers such as Jean-Luc Godard and François Truffaut using montage to create distinctive and innovative films. This approach continues to be influential in contemporary cinema, with directors employing montage to create memorable sequences in their films.[40]

In contemporary cinema, montage continues to play an essential role in shaping narratives and creating emotional resonance. Filmmakers have adapted the traditional montage technique to suit the evolving aesthetics and storytelling styles of modern cinema.
istory is the systematic study of the past, focusing primarily on the human past. As an academic discipline, it analyses and interprets evidence to construct narratives about what happened and explain why it happened. Some theorists categorize history as a social science, while others see it as part of the humanities or consider it a hybrid discipline. Similar debates surround the purpose of history—for example, whether its main aim is theoretical, to uncover the truth, or practical, to learn lessons from the past. In a more general sense, the term history refers not to an academic field but to the past itself, times in the past, or to individual texts about the past.

Historical research relies on primary and secondary sources to reconstruct past events and validate interpretations. Source criticism is used to evaluate these sources, assessing their authenticity, content, and reliability. Historians strive to integrate the perspectives of several sources to develop a coherent narrative. Different schools of thought, such as positivism, the Annales school, Marxism, and postmodernism, have distinct methodological approaches.

History is a broad discipline encompassing many branches. Some focus on specific time periods, such as ancient history, while others concentrate on particular geographic regions, such as the history of Africa. Thematic categorizations include political history, military history, social history, and economic history. Branches associated with specific research methods and sources include quantitative history, comparative history, and oral history.

History emerged as a field of inquiry in antiquity to replace myth-infused narratives, with influential early traditions originating in Greece, China, and later in the Islamic world. Historical writing evolved throughout the ages and became increasingly professional, particularly during the 19th century, when a rigorous methodology and various academic institutions were established. History is related to many fields, including historiography, philosophy, education, and politics.

Definition
As an academic discipline, history is the study of the past with the main focus on the human past.[1] It conceptualizes and describes what happened by collecting and analysing evidence to construct narratives. These narratives cover not only how events developed over time but also why they happened and in which contexts, providing an explanation of relevant background conditions and causal mechanisms. History further examines the meaning of historical events and the underlying human motives driving them.[2]

In a slightly different sense, history refers to the past events themselves. Under this interpretation, history is what happened rather than the academic field studying what happened. When used as a countable noun, a history is a representation of the past in the form of a history text. History texts are cultural products involving active interpretation and reconstruction. The narratives presented in them can change as historians discover new evidence or reinterpret already-known sources. The past itself, by contrast, is static and unchangeable.[3] Some historians focus on the interpretative and explanatory aspects to distinguish histories from chronicles, arguing that chronicles only catalogue events in chronological order, whereas histories aim at a comprehensive understanding of their causes, contexts, and consequences.[4][a]

History has been primarily concerned with written documents. It focused on recorded history since the invention of writing, leaving prehistory[b] to other fields, such as archaeology.[7] Its scope broadened in the 20th century as historians became interested in the human past before the invention of writing.[8][c]

Historians debate whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather objective evidence, and present arguments based on this evidence. At the same time, history aligns closely with the humanities because of its reliance on subjective aspects associated with interpretation, storytelling, human experience, and cultural heritage.[10] Some historians strongly support one or the other classification while others characterize history as a hybrid discipline that does not belong to one category at the exclusion of the other.[11] History contrasts with pseudohistory, a label used to describe practices that deviate from historiographical standards by relying on disputed historical evidence, selectively ignoring genuine evidence, or using other means to distort the historical record. Often motivated by specific ideological agendas, pseudohistorical practices mimic historical methodology to promote biased, misleading narratives that lack rigorous analysis and scholarly consensus.[12]

Purpose
Various suggestions about the purpose or value of history have been made. Some historians propose that its primary function is the pure discovery of truth about the past. This view emphasizes that the disinterested pursuit of truth is an end in itself, while external purposes, associated with ideology or politics, threaten to undermine the accuracy of historical research by distorting the past. In this role, history also challenges traditional myths lacking factual support.[13][d]

A different perspective suggests that the main value of history lies in the lessons it teaches for the present. This view is based on the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes.[15] A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across different contexts—similar to what one can learn by visiting foreign countries.[16] History can also foster social cohesion by providing people with a collective identity through a shared past, helping to preserve and cultivate cultural heritage and values across generations.[17] For some scholars, including Whig historians and the Marxist scholar E. H. Carr, history is a key to understanding the present[18] and, in Carr's case, shaping the future.[19]

History has sometimes been used for political or ideological purposes, for instance, to justify the status quo by emphasising the respectability of certain traditions or to promote change by highlighting past injustices.[20] In extreme forms, evidence is intentionally ignored or misinterpreted to construct misleading narratives, which can result in pseudohistory or historical denialism.[12][e] Influential examples are Holocaust denial, Armenian genocide denial, Nanjing Massacre denial, and Holodomor denial.[22]

Etymology
Photo of a damaged text written in black ink
Fragment of the Histories by Herodotus, an Ancient Greek historical text[23]
The word history comes from the Ancient Greek term ἵστωρ (histōr), meaning 'learned, wise man'. It gave rise to the Ancient Greek word ἱστορία (historiā), which had a wide meaning associated with inquiry in general and giving testimony. The term was later adopted into Classical Latin as historia. In Hellenistic and Roman times, the meaning of the term shifted, placing more emphasis on narrative aspects and the art of presentation rather than focusing on investigation and testimony.[24]

The word entered Middle English in the 14th century via the Old French term histoire.[25] At this time, it meant 'story, tale', encompassing both factual and fictional narratives. In the 15th century, its meaning shifted to cover the branch of knowledge studying the past in addition to narratives about the past.[26] In the 18th and 19th centuries, the word history became more closely associated with factual accounts and evidence-based inquiry, coinciding with the professionalization of historical inquiry, a meaning still dominant in contemporary usage.[27] The dual meaning, referring to both mere stories and factual accounts of the past, is present in the terms for history in many other European languages. They include the French histoire, the Italian storia, and the German Geschichte.[28]

Methods
Main article: Historical method
The historical method is a set of techniques historians use to research and interpret the past, covering the processes of collecting, evaluating, and synthesizing evidence.[f] It seeks to ensure scholarly rigour, accuracy, and reliability in how historical evidence is chosen, analysed, and interpreted.[30] Historical research often starts with a research question to define the scope of the inquiry. Some research questions focus on a simple description of what happened. Others aim to explain why a particular event occurred, refute an existing theory, or confirm a new hypothesis.[31]

Sources and source criticism
To answer research questions, historians rely on various types of evidence to reconstruct the past and support their conclusions. Historical evidence is usually divided into primary and secondary sources.[32] A primary source is a source that originated during the period that is studied. Primary sources can take various forms, such as official documents, letters, diaries, eyewitness accounts, photographs, and audio or video recordings. They also include historical remains examined in archaeology, geology, and the medical sciences, such as artefacts and fossils unearthed from excavations. Primary sources offer the most direct evidence of historical events.[33]

Photo of archive storage area; on the left, the hand cranks to operate shelving units; on the right, the shelves of one unit containing storage boxes
Archives preserve large quantities of original sources for researchers to access.[34]
A secondary source is a source that analyses or interprets information found in other sources.[35] Whether a document is a primary or a secondary source depends not only on the document itself but also on the purpose for which it is used. For example, if a historian writes a text about slavery based on an analysis of historical documents, then the text is a secondary source on slavery and a primary source on the historian's opinion.[36][g] Consistency with available sources is one of the main standards of historical works. For instance, the discovery of new sources may lead historians to revise or dismiss previously accepted narratives.[38] To find and access primary and secondary sources, historians consult archives, libraries, and museums. Archives play a central role by preserving countless original sources and making them available to researchers in a systematic and accessible manner. Thanks to technological advances, historians increasingly rely on online resources, which offer vast digital databases with methods to search and access specific documents.[39]

Source criticism is the process of analysing and evaluating the information a source provides.[h] Typically, this process begins with external criticism, which evaluates the authenticity of a source. It addresses the questions of when and where the source was created and seeks to identify the author, understand their reasons for producing the source, and determine if it has undergone some type of modification since its creation. Additionally, the process involves distinguishing between original works, copies, and deceptive forgeries.[41]

Internal criticism evaluates the content of a source, typically beginning with the clarification of the meaning within the source. This involves disambiguating individual terms that could be misunderstood but may also require a general translation if the source is written in an unfamiliar language.[i] Once the information content of a source is understood, internal criticism is specifically interested in determining accuracy. Critics ask whether the information is reliable or misrepresents the topic and further question whether the source is comprehensive or omits important details. One way to make these assessments is to evaluate whether the author was able, in principle, to provide a faithful presentation of the studied event. Other approaches include the assessment of the influences of the author's intentions and prejudices, and cross-referencing information with other credible sources. Being aware of the inadequacies of a source helps historians decide whether and which aspects of it to trust, and how to use it to construct a narrative.[43]

Synthesis and schools of thought
The selection, analysis, and criticism of sources result in the validation of a large collection of mostly isolated statements about the past. As a next step, sometimes termed historical synthesis, historians examine how the individual pieces of evidence fit together to form part of a larger story.[j] Constructing this broader perspective is crucial for a comprehensive understanding of the topic as a whole. It is a creative aspect[k] of historical writing that reconstructs, interprets, and explains what happened by showing how different events are connected.[46] In this way, historians address not only which events occurred but also why they occurred and what consequences they had.[47] While there are no universally accepted techniques for this synthesis, historians rely on various interpretative tools and approaches in this process.[48]

Drawing of a seated man in formal dark clothes
Auguste Comte articulated positivism, advocating a science-based approach to history.[49]
One tool to provide an accessible overview of complex developments is the use of periodization, which divides a timeframe into different periods, each organized around central themes or developments that shaped the period. For example, the three-age system is traditionally used to divide early human history into Stone Age, Bronze Age, and Iron Age based on the predominant materials and technologies during these periods.[50] Another methodological tool is the examination of silences, gaps or omissions in the historical record of events that occurred but did not leave significant evidential traces. Silences can happen when contemporaries find information too obvious to document but may also occur if there are specific reasons to withhold or destroy information.[51][l] Conversely, when large datasets are available, quantitative approaches can be used. For instance, economic and social historians commonly employ statistical analysis to identify patterns and trends associated with large groups.[54]

Different schools of thought often come with their own methodological implications for how to write history.[55] Positivists emphasize the scientific nature of historical inquiry, focusing on empirical evidence to discover objective truths.[56] In contrast, postmodernists reject grand narratives that claim to offer a single, objective truth. Instead, they highlight the subjective nature of historical interpretation, which leads to a multiplicity of divergent perspectives.[57] Marxists interpret historical developments as expressions of economic forces and class struggles.[58] The Annales school highlights long-term social and economic trends while relying on quantitative and interdisciplinary methods.[59] Feminist historians study the role of gender in history, with a particular interest in analysing the experiences of women to challenge patriarchal perspectives.[60]
Science is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][page needed][2] Modern science is typically divided into two – or three – major branches:[3] the natural sciences, which study the physical world, and the social sciences, which study individuals and societies.[4][5] While referred to as the formal sciences, the study of logic, mathematics, and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology.[6][7][8][9] Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12]

The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India and Islamic Golden Age.[13]: 12 [14][15][16][13]: 163–192  The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy,[13]: 193–224, 225–253 [17] which was later transformed by the Scientific Revolution that began in the 16th century[18] as new ideas and discoveries departed from previous Greek conceptions and traditions.[13]: 357–368 [19] The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century, many of the institutional and professional features of science began to take shape,[20][21] along with the changing of "natural philosophy" to "natural science".[22]

New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[23][24] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[25] government agencies,[13]: 163–192  and companies.[26] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.

Etymology
The word science has been used in Middle English since the 14th century in the sense of "the state of knowing". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning "knowledge, awareness, understanding", a noun derivative of sciens meaning "knowing", itself the present active participle of sciō, "to know".[27]

There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning "to know", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io meaning "to incise". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning "to not know, be unfamiliar with", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2- from *sḱʰeh2(i)-meaning "to cut".[28]

In the past, science was a synonym for "knowledge" or "study", in keeping with its Latin origin. A person who conducted scientific research was called a "natural philosopher" or "man of science".[29] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[30] crediting it to "some ingenious gentleman" (possibly himself).[31]

History
Main article: History of science
Early history
Main article: Science in the ancient world
Clay tablet with markings, three columns for numbers and one for ordinals
The Plimpton 322 tablet by the Babylonians records Pythagorean triples, written c. 1800 BCE
Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[32][33] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[34] as did religious rituals.[35] Some scholars use the term "protoscience" to label activities in the past that resemble modern science in some but not all features;[36][37][38] however, this label has also been criticised as denigrating,[39] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[40]

Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science.[13]: 12–15 [14] Although the words and concepts of "science" and "nature" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[41][13]: 12  From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system,[42] solved practical problems using geometry,[43] and developed a calendar.[44] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]: 9 

The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[45] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[46] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[45][47] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[45]

Classical antiquity
Main article: Science in classical antiquity
Framed mosaic of philosophers gathering around and conversing
Plato's Academy mosaic, made between 100 BCE and 79 CE, shows many Greek philosophers and scholars.
In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[48] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural "way" in which a plant grows,[49] and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish "nature" and "convention".[50]

The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[51] The Pythagoreans developed a complex number philosophy[52]: 467–468  and contributed significantly to the development of mathematical science.[52]: 465  The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[53][54] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a "canon" (ruler, standard) which established physical criteria or standards of scientific truth.[55] The Greek doctor Hippocrates established the tradition of systematic medical science[56][57] and is known as "The Father of Medicine".[58]

A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinises them for consistency.[59] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism.[60]

In the 4th century BCE, Aristotle created a systematic programme of teleological philosophy.[61] In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it.[62] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[62] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[63][64] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[65] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History.[66][67][68]

Positional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide.[69]

Middle Ages
Main article: History of science § Middle Ages
Picture of a peacock on very old paper
The first page of Vienna Dioscurides depicts a peacock, made in the 6th century.
Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe.[13]: 194  Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge.[70] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[13]: 159  John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus.[13]: 307, 311, 363, 402  His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[13]: 307–308 [71]

During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[72] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists.[73] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world.[74]

Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq[75] and the flourished[76] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study.[a][78][79] Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[80]

By the 11th century most of Europe had become Christian,[13]: 204  and in 1088, the University of Bologna emerged as the first university in Europe.[81] As such, demand for Latin translation of ancient and scientific texts grew,[13]: 204  a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[82] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi.[83]

Renaissance
Main articles: Scientific Revolution and Science in the Renaissance

Drawing of the heliocentric model as proposed by the Copernicus's De revolutionibus orbium coelestiumalt=Drawing of planets' orbit around the Sun
New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[77]: Book I  A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[84]

In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.[85]

Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[84][86] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[87] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[88]

The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[89] Francis Bacon and René Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[90] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature.[91]
Age of Enlightenment
Main article: Science in the Age of Enlightenment

Title page of the 1687 first edition of Philosophiæ Naturalis Principia Mathematica by Isaac Newton
At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica greatly influencing future physicists.[92] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[93]

During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime or pleasing [speculation]".[94]

Science during the Enlightenment was dominated by scientific societies and academies,[95] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population.[96] Enlightenment philosophers turned to a few of their scientific predecessors – Galileo, Kepler, Boyle, and Newton principally – as the guides to every physical and social field of the day.[97][98]

The 18th century saw significant advancements in the practice of medicine[99] and physics;[100] the development of biological taxonomy by Carl Linnaeus;[101] a new understanding of magnetism and electricity;[102] and the maturation of chemistry as a discipline.[103] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[104] Modern sociology largely originated from this movement.[105] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[106]

19th century
Main article: 19th century in science
Sketch of a map with captions
The first diagram of an evolutionary tree made by Charles Darwin in 1837
During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as "biologist", "physicist", and "scientist"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals.[107] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[108]

During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859.[109] Separately, Gregor Mendel presented his paper, "Experiments on Plant Hybridisation" in 1865,[110] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[111]

Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[112] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[113] This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[b]

The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[116] Marie Curie then became the first person to win two Nobel Prizes.[117] In the next year came the discovery of the first subatomic particle, the electron.[118]

20th century
Main article: 20th century in science
Graph showing lower ozone concentration at the South Pole
A computer graph of the ozone hole made in 1987 using data from a space telescope
In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally.[119][120] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies.[121]

During this period scientific experimentation became increasingly larger in scale and funding.[122] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race.[123][124] Substantial international collaborations were also made, despite armed conflicts.[125]

In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[126] The discovery of the cosmic microwave background in 1964[127] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lemaître.[128]

The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics.[129] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[130][131] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling.[132]

21st century
Main article: 21st century § Science and technology
The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[133] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body.[134] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[135] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[136][137] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc.[138]

Branches
Main article: Branches of science
Modern science is commonly divided into three major branches: natural science, social science, and formal science.[3] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[139] Both natural and social sciences are empirical sciences,[140] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[141]

Natural
Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[142] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings.[143] Today, "natural history" suggests observational descriptions aimed at popular audiences.[144]

Social
Two curve crossing over at a point, forming a X shape
Supply and demand curve in economics, crossing over at the optimal equilibrium
Social science is the study of human behaviour and the functioning of societies.[4][5] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[4] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology.[4] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[4]

Formal
Formal science is an area of study that generates knowledge using formal systems.[145][146][147] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[148] It includes mathematics,[149][150] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[8][151][141] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[6][152] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[153] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[154] chemistry,[155] biology,[156] finance,[157] and economics.[158]

Applied
Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[159][12] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[160] Science may contribute to the development of new technologies.[161] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[162][163]

Basic
The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[164][165]

Blue skies
Blue skies research, also called blue sky science, is scientific research in domains where "real-world" applications are not immediately apparent. It has been defined as "research without a clear goal"[166] and "curiosity-driven science". Proponents of this mode of science argue that unanticipated scientific breakthroughs are sometimes more valuable than the outcomes of agenda-driven research, heralding advances in genetics and stem cell biology as examples of unforeseen benefits of research that was originally seen as purely theoretical in scope. Because of the inherently uncertain return on investment, blue-sky projects are sometimes politically and commercially unpopular and tend to lose funding to research perceived as being more reliably profitable or practical.[167]
Research is creative and systematic work undertaken to increase the stock of knowledge.[1] It involves the collection, organization, and analysis of evidence to increase understanding of a topic, characterized by a particular attentiveness to controlling sources of bias and error. These activities are characterized by accounting and controlling for biases. A research project may be an expansion of past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole.

The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, and the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life, technological, etc. The scientific study of research practices is known as meta-research.

A researcher is a person who conducts research, especially in order to discover new information or to reach a new understanding.[2] In order to be a social researcher or a social scientist, one should have enormous knowledge of subjects related to social science that they are specialized in. Similarly, in order to be a natural science researcher, the person should have knowledge of fields related to natural science (physics, chemistry, biology, astronomy, zoology and so on). Professional associations provide one pathway to mature in the research profession.[3]

Etymology

Aristotle, (384–322 BC), an Ancient Greek philosopher and pioneer in developing the scientific method[4]
The word research is derived from the Middle French "recherche", which means "to go about seeking", the term itself being derived from the Old French term "recerchier", a compound word from "re-" + "cerchier", or "sercher", meaning 'search'.[5] The earliest recorded use of the term was in 1577.[5]

Definitions
Research has been defined in a number of different ways, and while there are similarities, there does not appear to be a single, all-encompassing definition that is embraced by all who engage in it.

Research, in its simplest terms, is searching for knowledge and searching for truth. In a formal sense, it is a systematic study of a problem attacked by a deliberately chosen strategy, which starts with choosing an approach to preparing a blueprint (design) and acting upon it in terms of designing research hypotheses, choosing methods and techniques, selecting or developing data collection tools, processing the data, interpretation, and ending with presenting solution(s) of the problem.[6]

Another definition of research is given by John W. Creswell, who states that "research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue". It consists of three steps: pose a question, collect data to answer the question, and present an answer to the question.[7][page needed]

The Merriam-Webster Online Dictionary defines research more generally to also include studying already existing knowledge: "studious inquiry or examination; especially: investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws".[5]

Forms of research
Original research
"Original research" redirects here. For the Wikipedia prohibition against user-generated, unpublished research, see Wikipedia:No original research.
Original research, also called primary research, is research that is not exclusively based on a summary, review, or synthesis of earlier publications on the subject of research. This material is of a primary-source character. The purpose of the original research is to produce new knowledge rather than present the existing knowledge in a new form (e.g., summarized or classified).[8][9] Original research can take various forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.[10]

The degree of originality of the research is among the major criteria for articles to be published in academic journals and usually established by means of peer review.[11] Graduate students are commonly required to perform original research as part of a dissertation.[12]

Scientific research
Main article: Scientific method

This section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these messages)
This section needs additional citations for verification. (July 2025)
This section may require cleanup to meet Wikipedia's quality standards. The specific problem is: unsourced and vague. (July 2025)

Primary scientific research being carried out at the Microscopy Laboratory at the Idaho National Laboratory

Scientific research equipment at MIT

The German maritime research vessel Sonne
Scientific research is a systematic way of gathering data and harnessing curiosity.[citation needed] This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research may be funded by public authorities, charitable organizations, and private organizations. Scientific research can be subdivided by discipline.

Generally, research is understood to follow a certain structural process. Though the order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:

Observations and formation of the topic: Consists of the subject area of one's interest and following that subject area to conduct subject-related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.
Hypothesis: A testable prediction which designates the relationship between two or more variables.
Conceptual definition: Description of a concept by relating it to other concepts.
Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study.
Gathering of data: Consists of identifying a population and selecting samples, gathering information from or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.
Analysis of data: Involves breaking down the individual pieces of data to draw conclusions about it.
Data Interpretation: This can be represented through tables, figures, and pictures, and then described in words.
Test, revising of hypothesis
Conclusion, reiteration if necessary
A common misconception is that a hypothesis will be proven (see, rather, null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.

A useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which states no relationship or difference between the independent or dependent variables.

Research in the humanities
Research in the humanities involves different methods such as for example hermeneutics and semiotics. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead, explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past. Other studies aim to merely examine the occurrence of behaviours in societies and communities, without particularly looking for reasons or motivations to explain these. These studies may be qualitative or quantitative, and can use a variety of approaches, such as queer theory or feminist theory.[13]

Artistic research
Artistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.

The controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines.[14] One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.[15]

Artistic research has been defined by the School of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner – "Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods, and criticality. Through presented documentation, the insights gained shall be placed in a context."[16] Artistic research aims to enhance knowledge and understanding with presentation of the arts.[17] A simpler understanding by Julian Klein defines artistic research as any kind of research employing the artistic mode of perception.[18] For a survey of the central problematics of today's artistic research, see Giaco Schiesser.[19]

According to artist Hakan Topal, in artistic research, "perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities".[20] Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.[21]

The Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR),[22][23] an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC),[24][25][26] a searchable, documentary database of artistic research, to which anyone can contribute.

Patricia Leavy addresses eight arts-based research (ABR) genres: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.[27]

In 2016, the European League of Institutes of the Arts launched The Florence Principles' on the Doctorate in the Arts.[28] The Florence Principles relating to the Salzburg Principles and the Salzburg Recommendations of the European University Association name seven points of attention to specify the Doctorate / PhD in the Arts compared to a scientific doctorate / PhD. The Florence Principles have been endorsed and are supported also by AEC, CILECT, CUMULUS and SAR.

Historical research
Main article: Historical method

Leopold von Ranke (1795–1886), a German historian and a founder of modern source-based history
The historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:[29]

Identification of origin date
Evidence of localization
Recognition of authorship
Analysis of data
Identification of integrity
Attribution of credibility
Documentary research
Main article: Documentary research
Steps in conducting research

Research design and evidence

Research cycle
Research is often conducted using the hourglass model structure of research.[30] The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:[31]

Identification of research problem
Literature review
Specifying the purpose of research
Determining specific research questions
Specification of a conceptual framework, sometimes including a set of hypotheses[32]
Choice of a methodology (for data collection)
Data collection
Verifying data
Analyzing and interpreting the data
Reporting and evaluating research
Communicating the research findings and, possibly, recommendations
The steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps.[33] Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study.[34] The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in rejecting or failing to reject the null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the reverse approach: starting with articulating findings and discussion of them, moving "up" to identification of a research problem that emerges in the findings and literature review. The reverse approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings have fully emerged and been interpreted.

Rudolph Rummel says, "... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results."[35]

Plato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrased in the following way, "If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!"[36]

Research methods

The research room at the New York Public Library, an example of secondary research in progress

Maurice Hilleman, a 20th-century vaccinologist credited with saving more lives than any other scientist of his era[37]
The goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):

Exploratory research, which helps to identify and define a problem or question.
Constructive research, which tests theories and proposes solutions to a problem or question.
Empirical research, which tests the feasibility of a solution using empirical evidence.
There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:

Qualitative research Qualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations.[38] In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement.[39][40]

Quantitative research Quantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).[7] Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism.

The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies.[41] Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest.[41]

If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).[citation needed] If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.[42]

In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data.[41] Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.[43]

Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common.[44] This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.[45]

Big data has brought big impacts on research methods so that now many researchers do not put much effort into data collection; furthermore, methods to analyze easily available huge amounts of data have also been developed.

Non-empirical research Non-empirical (theoretical) research is an approach that involves the development of theory as opposed to using observation and experimentation. As such, non-empirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool of existing and established knowledge. Non-empirical research is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose in science. Typically empirical research produces observations that need to be explained; then theoretical research tries to explain them, and in so doing generates empirically testable hypotheses; these hypotheses are then tested empirically, giving more observations that may need further explanation; and so on. See Scientific method.
In physics, a quantum (pl.: quanta) is the minimum amount of any physical entity (physical property) involved in an interaction. The fundamental notion that a property can be "quantized" is referred to as "the hypothesis of quantization".[1] This means that the magnitude of the physical property can take on only discrete values consisting of integer multiples of one quantum. For example, a photon is a single quantum of light of a specific frequency (or of any other form of electromagnetic radiation). Similarly, the energy of an electron bound within an atom is quantized and can exist only in certain discrete values.[2] Atoms and matter in general are stable because electrons can exist only at discrete energy levels within an atom. Quantization is one of the foundations of the much broader physics of quantum mechanics. Quantization of energy and its influence on how energy and matter interact (quantum electrodynamics) is part of the fundamental framework for understanding and describing nature.

Origin

German physicist and 1918 Nobel Prize for Physics recipient Max Planck (1858–1947)
The modern concept of the quantum in physics originates from December 14, 1900, when Max Planck reported his findings to the German Physical Society. He showed that modelling harmonic oscillators with discrete energy levels resolved a longstanding problem in the theory of blackbody radiation.[3]: 15 [4] In his report, Planck did not use the term quantum in the modern sense. Instead, he used the term Elementarquantum to refer to the "quantum of electricity", now known as the elementary charge. For the smallest unit of energy, he employed the term Energieelement, "energy element", rather than calling it a quantum.[5]

Shortly afterwards, in a paper published in Annalen der Physik,[6] Planck introduced the constant h, which he termed the "quantum of action" (elementares Wirkungsquantum) in 1906.[5] In this paper, Planck also reported more precise values for the elementary charge and the Avogadro–Loschmidt number, the number of molecules in one mole of substance.[7] The constant h is now known as the Planck constant. After his theory was validated, Planck was awarded the Nobel Prize in Physics for his discovery in 1918.[8]


In 1905 Albert Einstein suggested that electromagnetic radiation exists in spatially localized packets which he called "quanta of light" (Lichtquanta).[5][9] Einstein was able to use this hypothesis to recast Planck's treatment of the blackbody problem in a form that also explained the voltages observed in Philipp Lenard's experiments on the photoelectric effect.[3]: 23  Shortly thereafter, the term "energy quantum" was introduced for the quantity hν.[10]

Quantization
Main article: Quantization (physics)
While quantization was first discovered in electromagnetic radiation, it describes a fundamental aspect of energy not just restricted to photons.[11] In the attempt to bring theory into agreement with experiment, Max Planck postulated that electromagnetic energy is absorbed or emitted in discrete packets, or quanta.[12]
Mathematics is a field of study that discovers and organizes methods, theories, and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics).

Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms. Mathematics uses pure reason to prove the properties of objects through proofs, which consist of a succession of applications of deductive rules to already established results. These results, called theorems, include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.[1]

Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications.[2][3]

Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[4] Since its beginning, mathematics was primarily divided into geometry and arithmetics (the manipulation of natural numbers and fractions) until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both.[5] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[6] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics.[7][8]

Areas of mathematics
Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes.[9] Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.[10]

During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus[11]—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics.[12] The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the 17th century.[13]

At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[14][6] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[8] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have "geometry" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[7]

Number theory
Main article: Number theory

This is the Ulam spiral, which illustrates the distribution of prime numbers. The dark diagonal lines in the spiral hint at the hypothesized approximate independence between being prime and being a value of a quadratic polynomial, a conjecture now known as Hardy and Littlewood's Conjecture F.
Number theory began with the manipulation of numbers, that is, natural numbers 
(
N
)
,
{\displaystyle (\mathbb {N} ),} and later expanded to integers 
(
Z
)
{\displaystyle (\mathbb {Z} )} and rational numbers 
(
Q
)
.
{\displaystyle (\mathbb {Q} ).} Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations.[15] Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria.[16] The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.[17]

Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra.[18] Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.[19]

Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), Diophantine analysis, and transcendence theory (problem oriented).[7]

Geometry
Main article: Geometry

On the surface of a sphere, Euclidean geometry only applies as a local approximation. For larger scales the sum of the angles of a triangle is not equal to 180°.
Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.[20]

A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements.[21][22]

The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space.[b][20]

Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically.[23]

Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions.[20]

In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem.[24][6] In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space.[25]

Today's subareas of geometry include:[7]

Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines.
Affine geometry, the study of properties relative to parallelism and independent from the concept of length.
Differential geometry, the study of curves, surfaces, and their generalizations, which are defined using differentiable functions.
Manifold theory, the study of shapes that are not necessarily embedded in a larger space.
Riemannian geometry, the study of distance properties in curved spaces.
Algebraic geometry, the study of curves, surfaces, and their generalizations, which are defined using polynomials.
Topology, the study of properties that are kept under continuous deformations.
Algebraic topology, the use in topology of algebraic methods, mainly homological algebra.
Discrete geometry, the study of finite configurations in geometry.
Convex geometry, the study of convex sets, which takes its importance from its applications in optimization.
Complex geometry, the geometry obtained by replacing real numbers with complex numbers.
Algebra
Main article: Algebra
refer to caption
The quadratic formula, which concisely expresses the solutions of all quadratic equations
A shuffled 3x3 rubik's cube
The Rubik's Cube group is a concrete application of group theory.[26]
Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra.[27][28] Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution.[29] Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side.[30] The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise.[31][32]

Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers.[33] Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas.[34]

Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid.[35] The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether,[36] and popularized by Van der Waerden's book Moderne Algebra.

Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include:[7]

group theory
field theory
vector spaces, whose study is essentially the same as linear algebra
ring theory
commutative algebra, which is the study of commutative rings, includes the study of polynomials, and is a foundational part of algebraic geometry
homological algebra
Lie algebra and Lie group theory
Boolean algebra, which is widely used for the study of the logical structure of computers
The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory.[37] The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.[38]

Calculus and analysis
Main articles: Calculus and Mathematical analysis

A Cauchy sequence consists of elements such that all subsequent terms of a term become arbitrarily close to each other as the sequence progresses (from left to right).
Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz.[39] It is fundamentally the study of the relationship between variables that depend continuously on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results.[40] Presently, "calculus" refers mainly to the elementary part of this theory, and "analysis" is commonly used for advanced parts.[41]

Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include:[7]

Multivariable calculus
Functional analysis, where variables represent varying functions
Integration, measure theory and potential theory, all strongly related with probability theory on a continuum
Ordinary differential equations
Partial differential equations
Numerical analysis, mainly devoted to the computation on computers of solutions of ordinary and partial differential equations that arise in many applications
Discrete mathematics
Main article: Discrete mathematics

A diagram representing a two-state Markov chain. The states are represented by 'A' and 'E'. The numbers are the probability of flipping the state.
Discrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers.[42] Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply.[c] Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics.[43]

The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century.[44] The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems.[45]

Discrete mathematics includes:[7]

Combinatorics, the art of enumerating mathematical objects that satisfy some given constraints. Originally, these objects were elements or subsets of a given set; this has been extended to various objects, which establishes a strong link between combinatorics and other parts of discrete mathematics. For example, discrete geometry includes counting configurations of geometric shapes.
Graph theory and hypergraphs
Coding theory, including error correcting codes and a part of cryptography
Matroid theory
Discrete geometry
Discrete probability distributions
Game theory (although continuous games are also studied, most common games, such as chess and poker are discrete)
Discrete optimization, including combinatorial optimization, integer programming, constraint programming
Mathematical logic and set theory
Main articles: Mathematical logic and Set theory
A blue and pink circle and their intersection labeled
The Venn diagram is a commonly used method to illustrate the relations between sets.
The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century.[46][47] Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.[48]

Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets[49] but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory.[50] In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour.[51]

This became the foundational crisis of mathematics.[52] It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have.[14] For example, in Peano arithmetic, the natural numbers are defined by "zero is a number", "each number has a unique successor", "each number but zero has a unique predecessor", and some rules of reasoning.[53] This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910.[54]

The "nature" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called "intuition"—to guide their study and proofs. The approach allows considering "logics" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system.[55] This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic (which explicitly lacks the law of excluded middle).[56][57]

These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory.[7] Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, formal verification, program analysis, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.[58]

Statistics and other decision sciences
Main articles: Statistics and Probability theory

Whatever the form of a random population distribution (μ), the sampling mean (x̄) tends to a Gaussian distribution and its variance (σ) is given by the central limit theorem of probability theory.[59]
The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods such as, and especially, probability theory. Statisticians generate data with random sampling or randomized experiments.[60]

Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[61] Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics.[62]

Computational mathematics
Main article: Computational mathematics
Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity.[63][64] Part of computational mathematics involves numerical analysis, which is the study of methods for problems in analysis using functional analysis and approximation theory. Numerical analysis broadly includes the study of approximation and discretization, with special focus on rounding errors.[65] Numerical analysis and, more broadly, scientific computing, also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.[7]

History
Main article: History of mathematics
Etymology
The word mathematics comes from the Ancient Greek word máthēma (μάθημα), meaning 'something learned, knowledge, mathematics', and the derived expression mathēmatikḗ tékhnē (μαθηματικὴ τέχνη), meaning 'mathematical science'. It entered the English language during the Late Middle English period through French and Latin.[66]

Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "learners" rather than "mathematicians" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established.[67]

In Latin and English, until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning "astrologers", is sometimes mistranslated as a condemnation of mathematicians.[68]

The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly "all things mathematical", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek.[69] In English, the noun mathematics takes a singular verb. It is often shortened to maths[70] or, in North America, math.[71]

Ancient

The Babylonian mathematical tablet Plimpton 322, dated to 1800 BC
In addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years.[72][73] Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[74] The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC.[75] Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time.[76]

In the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right.[77] Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof.[78] His book, Elements, is widely considered the most successful and influential textbook of all time.[79] The greatest mathematician of antiquity is often held to be Archimedes (c. 287 – c. 212 BC) of Syracuse.[80] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[81] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[82] trigonometry (Hipparchus of Nicaea, 2nd century BC),[83] and the beginnings of algebra (Diophantus, 3rd century AD).[84]


The numerals used in the Bakhshali manuscript, dated between the 2nd century BC and the 2nd century AD
The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics.[85] Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.[86][87]
During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system.[88] Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.[89] The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe.[90]

During the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of logarithms by John Napier in 1614, which greatly simplified numerical calculations, especially for astronomy and marine navigation, the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1643–1727) and Gottfried Leibniz (1646–1716). Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems.[91]


Carl Friedrich Gauss
Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics.[92] In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved.[55]

Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[93]

Symbolic notation and terminology
Main articles: Mathematical notation, Language of mathematics, and Glossary of mathematics

An explanation of the sigma (Σ) summation notation
Mathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas.[94] More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs,[95] such as + (plus), × (multiplication), 
∫
{\textstyle \int } (integral), = (equal), and < (less than).[96] All these symbols are generally grouped according to specific rules to form expressions and formulas.[97] Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses.

Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary.[98]

Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism.[99] Other technical terms are words of the common language that are used in an accurate meaning that may differ slightly from their common meaning. For example, in mathematics, "or" means "one, the other or both", while, in common language, it is either ambiguous or means "one or the other but not both" (in mathematics, the latter is called "exclusive or"). Finally, many mathematical terms are common words that are used with a completely different meaning.[100] This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, "every free module is flat" and "a field is always a ring".

Relationship with sciences
Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws.[101] The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model.[102] Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used.[103] For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.[104]

There is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation.[105] In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied "durch planmässiges Tattonieren" (through systematic experimentation).[106] However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence.[107][108][109][110]

Pure and applied mathematics
Main articles: Applied mathematics and Pure mathematics
Isaac Newton
Gottfried Wilhelm von Leibniz
Isaac Newton (left) and Gottfried Wilhelm Leibniz developed infinitesimal calculus.
Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics.[111] For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture, and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians.[112] However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece.[113] The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.[114]

In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics.[111][115] This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred.[116]

The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere.[117][118] Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the "pure theory".[119][120]

An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis.[121] An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high.[122] For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry.[123]

In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas.[124][125] The Mathematics Subject Classification has a section for "general applied mathematics" but does not mention "pure mathematics".[7] However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge.

Unreasonable effectiveness
The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner.[3] It is the fact that many mathematical theories (even the "purest") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced.[126] Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.

A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem.[127] A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It was almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.[128]

In the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four.[129][130]

A striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon 
Ω
−
.
{\displaystyle \Omega ^{-}.} In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments.[131][132][133]

Specific sciences
Physics
Main article: Relationship between mathematics and physics

Diagram of a pendulum
Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly,[134] and is also considered to be the motivation of major mathematical developments.[135]

Computing
Further information: Theoretical computer science and Computational mathematics
Computing is closely related to mathematics in several ways.[136] Theoretical computer science is considered to be mathematical in nature.[137] Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, and graph theory.[138] In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer.[139]

Biology and chemistry
Main articles: Mathematical and theoretical biology and Mathematical chemistry

The skin of this giant pufferfish exhibits a Turing pattern, which can be modeled by reaction–diffusion systems.
Biology uses probability extensively in fields such as ecology or neurobiology.[140] Most discussion of probability centers on the concept of evolutionary fitness.[140] Ecology heavily uses modeling to simulate population dynamics,[140][141] study ecosystems such as the predator-prey model, measure pollution diffusion,[142] or to assess climate change.[143] The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations.[144]

Statistical hypothesis testing, is run on data from clinical trials to determine whether a new treatment works.[145] Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions.[146]

Earth sciences
Main article: Geomathematics
Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes.[147] Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models.[148][149][150]

Social sciences
Further information: Mathematical economics and Historical dynamics
Areas of mathematics used in the social sciences include probability/statistics and differential equations. These are used in linguistics, economics, sociology,[151] and psychology.[152]


Supply and demand curves, like this one, are a staple of mathematical economics.
Often the fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man').[153] In this model, the individual seeks to maximize their self-interest,[153] and always makes optimal choices using perfect information.[154] This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms. Some reject or criticise the concept of Homo economicus. Economists note that real people have limited information, make poor choices, and care about fairness and altruism, not just personal gain.[155]

Without mathematical modeling, it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions that can be tested against real-world data.[156]

At the start of the 20th century, there was a development to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis.[157] Towards the end of the 19th century, mathematicians extended their analysis into geopolitics.[158] Peter Turchin developed cliodynamics in the 1990s.[159]

Mathematization of the social sciences is not without risk. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences.[160] The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models, can be subject to controversy.[161][162]
Culture (/ˈkʌltʃər/ KUL-chər) is a concept that encompasses the social behavior, institutions, and norms found in human societies, as well as the knowledge, beliefs, arts, laws, customs, capabilities, attitudes, and habits of the individuals in these groups.[1] Culture often originates from or is attributed to a specific region or location.

Humans acquire culture through the learning processes of enculturation and socialization, which is shown by the diversity of cultures across societies. A cultural norm codifies acceptable conduct in society; it serves as a guideline for behavior, dress, language, and demeanor in a situation, which serves as a template for expectations in a social group. Accepting only a monoculture in a social group can bear risks, just as a single species can wither in the face of environmental change, for lack of functional responses to the change.[2] Thus in military culture, valor is counted as a typical behavior for an individual, and duty, honor, and loyalty to the social group are counted as virtues or functional responses in the continuum of conflict. In religion, analogous attributes can be identified in a social group.

Cultural change, or repositioning, is the reconstruction of a cultural concept of a society. Cultures are internally affected by both forces encouraging change and forces resisting change. Cultures are externally affected via contact between societies. Organizations like UNESCO attempt to preserve culture and cultural heritage.

Description

Pygmy music has been polyphonic well before their discovery by non-African explorers of the Baka, Aka, Efe, and other foragers of the Central African forests, in the 1200s, which is at least 200 years before polyphony developed in Europe. Note the multiple lines of singers and dancers. The motifs are independent, with theme and variation interweaving.[3] This type of music is thought to be the first expression of polyphony in world music.
Culture is considered a central concept in anthropology, encompassing the range of phenomena that are transmitted through social learning in human societies. Cultural universals are found in all human societies. These include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science comprise the intangible cultural heritage of a society.[4]

In the humanities, one sense of culture as an attribute of the individual has been the degree to which they have cultivated a particular level of sophistication in the arts, sciences, education, or manners.[5] The level of cultural sophistication has also sometimes been used to distinguish civilizations from less complex societies.[6] Such hierarchical perspectives on culture are also found in class-based distinctions between a high culture of the social elite and a low culture, popular culture, or folk culture of the lower classes, distinguished by stratified access to cultural capital.[7] In common parlance, culture is often used to refer specifically to the symbolic markers used by ethnic groups to distinguish themselves visibly from each other, such as body modification, clothing or jewelry.[8] Mass culture refers to the mass-produced and mass-mediated forms of consumer culture that emerged in the 20th century.[9] Some schools of philosophy, such as Marxism and critical theory, have argued that culture is often used politically as a tool of the elites to manipulate the proletariat and create a false consciousness.[10] Such perspectives are common in the discipline of cultural studies.[11] In the wider social sciences, the theoretical perspective of cultural materialism holds that human symbolic culture arises from the material conditions of human life, and that the basis of culture is found in evolved biological dispositions.[12]

When used as a count noun, a "culture" is the set of customs, traditions, and values of a society or community, such as an ethnic group or nation, and the knowledge acquired over time.[13] In this sense, multiculturalism values the peaceful coexistence and mutual respect between different cultures inhabiting the same planet.[14] Sometimes "culture" is also used to describe specific practices within a subgroup of a society, a subculture (e.g., "bro culture"), or a counterculture.[15] Within cultural anthropology, the ideology and analytical stance of cultural relativism hold that cultures cannot easily be objectively ranked or evaluated because any evaluation is necessarily situated within the value system of a given culture.[16]

Etymology
The modern term culture is based on a term used by the ancient Roman orator Cicero in his Tusculanae Disputationes, where he wrote of a cultivation of the soul or cultura animi,[17] using an agricultural metaphor for the development of a philosophical soul, understood teleologically as the highest possible ideal for human development. Samuel von Pufendorf took over this metaphor in a modern context, meaning something similar, but no longer assuming philosophy was humanity's natural perfection. This use, and that of many writers, "refers to all the ways in which human beings overcome their original barbarism, and through artifice, become fully human".[18]

Edward S. Casey wrote, "The very word culture meant 'place tilled' in Middle English, and the same word goes back to Latin colere, 'to inhabit, care for, till, worship' and cultus, 'A cult, especially a religious one.' To be cultural, to have a culture, is to inhabit a place sufficiently intensely to cultivate it—to be responsible for it, to respond to it, to attend to it caringly."[19]

Culture described by Richard Velkley:[18]

... originally meant the cultivation of the soul or mind, acquires most of its later modern meaning in the writings of the 18th-century German thinkers, who were on various levels developing Rousseau's criticism of "modern liberalism and Enlightenment". Thus a contrast between "culture" and "civilization" is usually implied in these authors, even when not expressed as such.

In the words of anthropologist E. B. Tylor, it is "that complex whole which includes knowledge, belief, art, morals, law, custom and any other capabilities and habits acquired by man as a member of society".[20] Alternatively, in a contemporary variant, "Culture is defined as a social domain that emphasizes the practices, discourses and material expressions, which, over time, express the continuities and discontinuities of social meaning of a life held in common.[21]

The Cambridge English Dictionary states that culture is "the way of life, especially the general customs and beliefs, of a particular group of people at a particular time."[22] Terror management theory posits that culture is a series of activities and worldviews that provide humans with the basis for perceiving themselves as "person[s] of worth within the world of meaning"—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that Homo sapiens became aware of when they acquired a larger brain.[23][24]

The word is used in a general sense as the evolved ability to categorize and represent experiences with symbols and to act imaginatively and creatively.[25] This ability arose with the evolution of behavioral modernity in humans around 50,000 years ago and is often thought to be unique to humans.[26] However, some other species have demonstrated similar, though less complicated, abilities for social learning.[27] It is also used to denote the complex networks of practices and accumulated knowledge and ideas that are transmitted through social interaction and exist in specific human groups, or cultures, using the plural form.[28]

Change
Main article: Culture change

The Beatles exemplified changing cultural dynamics, not only in music, but fashion and lifestyle. Six decades after their emergence, they continue to have a worldwide cultural impact.
Raimon Panikkar identified 29 ways in which cultural change can be brought about, including growth, development, evolution, involution, renovation, reconception, reform, innovation, revivalism, revolution, mutation, progress, diffusion, osmosis, borrowing, eclecticism, syncretism, modernization, indigenization, and transformation.[29] In this context, modernization could be viewed as adopting Enlightenment-era beliefs and practices, such as science, rationalism, industry, commerce, democracy, and the notion of progress. Rein Raud, building on the work of Umberto Eco, Pierre Bourdieu and Jeffrey C. Alexander, has proposed a model of cultural change based on claims and bids, which are judged by their cognitive adequacy and endorsed or not endorsed by the symbolic authority of the cultural community in question.[30]


19th-century engraving shows Indigenous Australians opposing the arrival of James Cook in 1770
Cultural invention has come to mean any innovation that is new and found to be useful to a group of people and expressed in their behavior, but which does not exist as a physical object. Humanity is in a global "accelerating culture change period," driven by the expansion of international commerce, the mass media, and above all, the human population explosion, among other factors. Culture repositioning means the reconstruction of the cultural concept of a society.[31]

Cultures are internally affected by both forces encouraging change and forces resisting change. These forces are related to both social structures and natural events and are involved in perpetuating cultural ideas and practices within current structures, which themselves are subject to change.[32]

Social conflict and the development of technologies can produce changes within a society by altering social dynamics and promoting new cultural models and spurring or enabling generative action. These social shifts may accompany ideological shifts and other types of cultural change. For example, the feminist movement involved new practices that produced a shift in gender relations, altering both gender and economic structures. Environmental conditions may also enter as factors. For example, after tropical forests returned at the end of the last ice age, plants suitable for domestication were available, leading to the invention of agriculture, which in turn brought about many cultural innovations and shifts in social dynamics.[33]

Full-length profile portrait of a woman, standing on a carpet at the entrance to a yurt, dressed in traditional clothing and jewelry
Turkmen woman, on a carpet at the entrance to a yurt, in traditional clothing. Sense of time is dependent on culture. This is a 1913 photo, but it can be difficult to date for a viewer, due to the absence of cultural cues.
Cultures are externally affected via contact between societies, which may also produce—or inhibit—social shifts and changes in cultural practices. War or competition over resources may impact technological development or social dynamics. Additionally, cultural ideas may transfer from one society to another, through diffusion or acculturation. In diffusion, the form of something (though not necessarily its meaning) moves from one culture to another. For example, Western restaurant chains and culinary brands sparked curiosity and fascination to the Chinese as China opened its economy to international trade in the late 20th-century.[34] "Stimulus diffusion" (the sharing of ideas) refers to an element of one culture leading to an invention or propagation in another. "Direct borrowing", on the other hand, tends to refer to technological or tangible diffusion from one culture to another. Diffusion of innovations theory presents a research-based model of why and when individuals and cultures adopt new ideas, practices, and products.[35]

Acculturation has different meanings. Still, in this context, it refers to the replacement of traits of one culture with another, such as what happened to certain Native American tribes and many indigenous peoples across the globe during colonization. Related processes on an individual level include assimilation and transculturation. The transnational flow of culture has played a major role in merging different cultures and sharing thoughts, ideas, and beliefs.

Early modern discourses
German Romanticism

Johann Herder called attention to national cultures.
Immanuel Kant (1724–1804) formulated an individualist definition of "enlightenment" similar to the concept of bildung: "Enlightenment is man's emergence from his self-incurred immaturity."[36] He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: "Sapere Aude" ("Dare to be wise!"). In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of Bildung: "For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people."[37]


Adolf Bastian developed a universal model of culture.
In 1795, the Prussian linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a "Germany" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as "worldview" (Weltanschauung).[38] According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between "civilized" and "primitive" or "tribal" cultures.

In 1860, Adolf Bastian (1826–1905) argued for "the psychic unity of mankind".[39] He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of "elementary ideas" (Elementargedanken); different cultures, or different "folk ideas" (Völkergedanken), are local modifications of the elementary ideas.[40] This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.[41]

English Romanticism

British poet and critic Matthew Arnold viewed "culture" as the cultivation of the humanist ideal.
In the 19th century, humanists such as English poet and essayist Matthew Arnold (1822–1888) used the word "culture" to refer to an ideal of individual human refinement, of "the best that has been thought and said in the world".[42] This concept of culture is also comparable to the German concept of bildung: "...culture being a pursuit of our total perfection by means of getting to know, on all the matters which most concern us, the best which has been thought and said in the world".[42]
In practice, culture referred to an elite ideal and was associated with such activities as art, classical music, and haute cuisine.[43] As these forms were associated with urban life, "culture" was identified with "civilization" (from Latin: civitas, lit. 'city'). Another facet of the Romantic movement was an interest in folklore, which led to identifying a "culture" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling class, and low culture. In other words, the idea of "culture" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.[44]


British anthropologist Edward Tylor was one of the first English-speaking scholars to use the term culture in an inclusive and universal sense.
Matthew Arnold contrasted "culture" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted "culture" with "the state of nature". According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between "civilized" and "uncivilized".[45] According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures expresses the conflict between European elites and non-elites, other critics have argued that the distinction between civilized and uncivilized people is an expression of the conflict between European colonial powers and their colonial subjects.

Other 19th-century critics, following Rousseau, have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by "the folk," i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as "noble savages" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of Western culture.

In 1870 the anthropologist Edward Tylor (1832–1917) applied these ideas of higher versus lower culture to propose a theory of the evolution of religion. According to this theory, religion evolves from more polytheistic to more monotheistic forms.[46] In the process, he redefined culture as a diverse set of activities characteristic of all human societies. This view paved the way for the modern understanding of religion.

Anthropology
Main article: American anthropology

Petroglyphs in modern-day Gobustan, Azerbaijan, dating to 10,000 BCE and indicating a thriving culture
Although anthropologists worldwide refer to Tylor's definition of culture,[47] in the 20th century "culture" emerged as the central and unifying concept of American anthropology, where it most commonly refers to the universal human capacity to classify and encode human experiences symbolically, and to communicate symbolically encoded experiences socially.[48] American anthropology is organized into four fields, each of which plays an important role in research on culture: biological anthropology, linguistic anthropology, cultural anthropology, and in the United States and Canada, archaeology.[49][50][51][52] The term Kulturbrille, or 'culture glasses', coined by German American anthropologist Franz Boas, refers to the "lenses" through which a person sees their own culture. Martin Lindstrom asserts that Kulturbrille, which allow a person to make sense of the culture they inhabit, "can blind us to things outsiders pick up immediately".[53]

Sociology
Main article: Sociology of culture

An example of folkloric dancing in Colombia
The sociology of culture concerns culture as manifested in society. For sociologist Georg Simmel (1858–1918), culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history".[54] As such, culture in the sociological field can be defined as the ways of thinking, the ways of acting, and the material objects that together shape a people's way of life. Culture can be either of two types, non-material culture or material culture.[4] Non-material culture refers to the non-physical ideas that individuals have about their culture, including values, belief systems, rules, norms, morals, language, organizations, and institutions, while material culture is the physical evidence of a culture in the objects and architecture they make or have made. The term tends to be relevant only in archeological and anthropological studies, but it specifically means all material evidence which can be attributed to culture, past or present.

Cultural sociology first emerged in Weimar Germany (1918–1933), where sociologists such as Alfred Weber used the term Kultursoziologie ('cultural sociology'). Cultural sociology was then reinvented in the English-speaking world as a product of the cultural turn of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may be loosely regarded as an approach incorporating cultural analysis and critical theory. Cultural sociologists tend to reject scientific methods, instead hermeneutically focusing on words, artifacts and symbols. Culture has since become an important concept across many branches of sociology, including resolutely scientific fields like social stratification and social network analysis. As a result, there has been a recent influx of quantitative sociologists to the field. Thus, there is now a growing group of sociologists of culture who are, confusingly, not cultural sociologists. These scholars reject the abstracted postmodern aspects of cultural sociology, and instead, look for a theoretical backing in the more scientific vein of social psychology and cognitive science.[55]


Nowruz is a good sample of popular and folklore culture that is celebrated by people in more than 22 countries with different nations and religions, at the 1st day of spring. It has been celebrated by diverse communities for over 7,000 years.
Early researchers and development of cultural sociology
The sociology of culture grew from the intersection between sociology (as shaped by early theorists like Marx,[56] Durkheim, and Weber) with the growing discipline of anthropology, wherein researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field lingers in the methods (much of cultural, sociological research is qualitative), in the theories (a variety of critical approaches to sociology are central to current research communities), and in the substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.

Cultural studies
Main article: Cultural studies
In the United Kingdom, sociologists and other scholars influenced by Marxism such as Stuart Hall (1932–2014) and Raymond Williams (1921–88) developed cultural studies. Following nineteenth-century Romantics, they identified culture with consumption goods and leisure activities (such as art, music, film, food, sports, and clothing). They saw patterns of consumption and leisure as determined by relations of production, which led them to focus on class relations and the organization of production.[57][58]

In the UK, cultural studies focuses largely on the study of popular culture; that is, on the social meanings of mass-produced consumer and leisure goods. Richard Hoggart coined the term in 1964 when he founded the Centre for Contemporary Cultural Studies or CCCS.[59] Cultural studies in this sense, then, can be viewed as a limited concentration scoped on the intricacies of consumerism, which belongs to a wider culture sometimes referred to as Western civilization or globalism.


The Metropolitan Museum of Art in Manhattan. Visual art can be one expression of high culture.
From the 1970s onward, Stuart Hall's pioneering work, along with that of his colleagues Paul Willis, Dick Hebdige, Tony Jefferson, and Angela McRobbie, created an international intellectual movement. As the field developed, it began to combine political economy, communication, sociology, social theory, literary theory, media theory, film/video studies, cultural anthropology, philosophy, museum studies, and art history to study cultural phenomena or cultural texts. In this field researchers often concentrate on how particular phenomena relate to matters of ideology, nationality, ethnicity, social class, or gender.[60]

Cultural studies is concerned with the meaning and practices of everyday life. These practices comprise the ways people do particular things (such as watching television or eating out) in a given culture. It also studies the meanings and uses people attribute to various objects and practices. Specifically, culture involves those meanings and practices held independently of reason. Watching television to view a public perspective on a historical event should not be thought of as culture unless referring to the medium of television itself, which may have been selected culturally; however, schoolchildren watching television after school with their friends to "fit in" certainly qualifies since there is no grounded reason for one's participation in this practice.

In the context of cultural studies, a text includes not only written language, but also films, photographs, fashion, or hairstyles: the texts of cultural studies comprise all the meaningful artifacts of culture.[61] Similarly, the discipline widens the concept of culture. Culture, for a cultural-studies researcher, not only includes traditional high culture (the culture of the ruling social groups)[62] and popular culture, but also everyday meanings and practices. The last two, in fact, have become the main focus of cultural studies. A further and recent approach is comparative cultural studies, based on the disciplines of comparative literature and cultural studies.[63]

Scholars in the UK and the US developed different versions of cultural studies after the 1970s. The British version of cultural studies had originated in the 1950s and 60s, mainly under the influence of Richard Hoggart, E. P. Thompson, and Raymond Williams, and later that of Stuart Hall and others at the Centre for Contemporary Cultural Studies. This included overtly political, left-wing views, and criticisms of popular culture as "capitalist" mass culture; it absorbed some of the ideas of the Frankfurt School critique of the "culture industry" i.e. mass culture. This emerges in the writings of early British cultural-studies scholars and their influences: see the work of Raymond Williams, Stuart Hall, Paul Willis, and Paul Gilroy.

In the United States, Lindlof and Taylor write, "cultural studies [were] grounded in a pragmatic, liberal-pluralist tradition."[64] The American version of cultural studies initially concerned itself more with understanding the subjective and appropriative side of audience reactions to, and uses of, mass culture; for example, American cultural-studies advocates wrote about the liberatory aspects of fandom.[65][66]

Some researchers, especially in early British cultural studies, apply a Marxist model to the field. This strain of thinking has some influence from the Frankfurt School, but especially from the structuralist Marxism of Louis Althusser and others. The main focus of an orthodox Marxist approach concentrates on the production of meaning. This model assumes a mass production of culture and identifies power as residing with those producing cultural artifacts.

In a Marxist view, the mode and relations of production form the economic base of society, which constantly interacts and influences superstructures, such as culture.[67] Other approaches to cultural studies, such as feminist cultural studies and later American developments of the field, distance themselves from this view. They criticize the Marxist assumption of a single, dominant meaning, shared by all, for any cultural product. The non-Marxist approaches suggest that different ways of consuming cultural artifacts affect the meaning of the product.

This view comes through in the book Doing Cultural Studies: The Story of the Sony Walkman (by Paul du Gay et al.),[68] which seeks to challenge the notion that those who produce commodities control the meanings that people attribute to them. Feminist cultural analyst, theorist, and art historian Griselda Pollock contributed to cultural studies from viewpoints of art history and psychoanalysis. The writer Julia Kristeva is among influential voices at the turn of the century, contributing to cultural studies from the field of art and psychoanalytical French feminism.[69]

Petrakis and Kostis (2013) divide cultural background variables into two main groups:[70]

The first group covers the variables that represent the "efficiency orientation" of the societies: performance orientation, future orientation, assertiveness, power distance, and uncertainty avoidance.
The second covers the variables that represent the "social orientation" of societies, i.e., the attitudes and lifestyles of their members. These variables include gender egalitarianism, institutional collectivism, in-group collectivism, and human orientation.
In 2016, a new approach to culture was suggested by Rein Raud,[30] who defines culture as the sum of resources available to human beings for making sense of their world and proposes a two-tiered approach, combining the study of texts (all reified meanings in circulation) and cultural practices (all repeatable actions that involve the production, dissemination or transmission of purposes), thus making it possible to re-link anthropological and sociological study of culture with the tradition of textual theory.

Super culture
A super culture is a collection of cultures and subcultures, that interact with one another, share similar characteristics and collectively have a degree of sense of unity.[citation needed] In other words, Super-culture is a culture encompassing several subcultures with common elements.[71] Examples include: List of Super-cultures:

Rave - In modern society, rave is described as a culture closely defined as a super culture.[72][73]
Steampunk - it is fast becoming a super-culture rather than a mere subculture.[74]
Foodtruck collectives & Pop-up Restaurants + Shops.
Some ancient cultures that are also considered (termed) "Super-culture":

Megalithic Super-culture in Prehistoric Europe
Asian Super-culture (See Korean nationalist historiography)
Psychology
See also: Social psychology, Cultural psychology, and Cross-cultural psychology

The NYC Pride March is the world's largest LGBT event. Regional variation exists with respect to tolerance in different parts of the world.

Cognitive tools suggest a way for people from certain culture to deal with real-life problems, like Suanpan for mathematical calculation.
Starting in the 1990s,[75]: 31  psychological research on culture influence began to grow and challenge the universality assumed in general psychology.[76]: 158–168 [77] Culture psychologists began to try to explore the relationship between emotions and culture, and answer whether the human mind is independent from culture. For example, people from collectivistic cultures, such as the Japanese, suppress their positive emotions more than their American counterparts.[78] Culture may affect the way that people experience and express emotions. On the other hand, some researchers try to look for differences between people's personalities across cultures.[79][80] As different cultures dictate distinctive norms, culture shock is also studied to understand how people react when they are confronted with other cultures. LGBT culture is displayed with significantly different levels of tolerance within different cultures and nations. Cognitive tools may not be accessible or they may function differently cross culture.[75]: 19  For example, people who are raised in a culture with an abacus are trained with distinctive reasoning style.[81] Cultural lenses may also make people view the same outcome of events differently. Westerners are more motivated by their successes than their failures, while East Asians are better motivated by the avoidance of failure.[82]

Culture is important for psychologists to consider when understanding the human mental operation. The notion of the anxious, unstable, and rebellious adolescent has been criticized by experts, such as Robert Epstein, who state that an undeveloped brain is not the main cause of teenagers' turmoils.[83][84] Some have criticized this understanding of adolescence, classifying it as a relatively recent phenomenon in human history created by modern society,[85][86][87][88] and have been highly critical of what they view as the infantilization of young adults in American society.[89] According to Robert Epstein and Jennifer, "American-style teen turmoil is absent in more than 100 cultures around the world, suggesting that such mayhem is not biologically inevitable. Second, the brain itself changes in response to experiences, raising the question of whether adolescent brain characteristics are the cause of teen tumult or rather the result of lifestyle and experiences."[90] David Moshman has also stated in regards to adolescence that brain research "is crucial for a full picture, but it does not provide an ultimate explanation".[91]

Protection of culture

Restoration of an ancient Egyptian monument
There are a number of international agreements and national laws relating to the protection of cultural heritage and cultural diversity. UNESCO and its partner organizations such as Blue Shield International coordinate international protection and local implementation.[92][93] The Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict and the UNESCO Convention on the Protection and Promotion of the Diversity of Cultural Expressions deal with the protection of culture. Article 27 of the Universal Declaration of Human Rights deals with cultural heritage in two ways: it gives people the right to participate in cultural life on the one hand and the right to the protection of their contributions to cultural life on the other.[94]


Anarchist poster reading "No Culture, No Future!", 5 December 2024
In the 21st century, the protection of culture has been the focus of increasing activity by national and international organizations. The United Nations and UNESCO promote cultural preservation and cultural diversity through declarations and legally-binding conventions or treaties. The aim is not to protect a person's property, but rather to preserve the cultural heritage of humanity, especially in the event of war and armed conflict. According to Karl von Habsburg, President of Blue Shield International, the destruction of cultural assets is also part of psychological warfare. The target of the attack is the identity of the opponent, which is why symbolic cultural assets become a main target. It is also intended to affect the particularly sensitive cultural memory, the growing cultural diversity and the economic basis (such as tourism) of a state, region or municipality.[95][96][97]

Tourism is having an increasing impact on the various forms of culture. On the one hand, this can be physical impact on individual objects or the destruction caused by increasing environmental pollution and, on the other hand, socio-cultural effects on society.[98][99][100]
The formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.[50]

In the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics.[51] The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.[17]

By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (langue), from language as a concrete manifestation of this system (parole).[52]

Modern linguistics

Noam Chomsky is one of the most important linguistic theorists of the 20th century.
In the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.[53]

In opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out.[22][note 2] The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.[55]

Physiological and neural architecture of language and speech
Speaking is the default modality for language in all cultures.[citation needed] The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language.[56] The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.[57]

The brain
Main articles: Neurolinguistics and Language processing in the brain

Language Areas of the brain.
  Angular gyrus
  Supramarginal gyrus
  Broca's area
  Wernicke's area
  Primary auditory cortex
The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.[58]

Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out.[59] They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with word repetition. The condition affects both spoken and written language. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.[60][61]

With technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.[58]

Anatomy of speech
Main articles: Speech production, Phonetics, and Articulatory phonetics

The human vocal tract

Spectrogram of American English vowels [i, u, ɑ] showing the formants f1 and f2
Duration: 15 seconds.0:15
Real time MRI scan of a person speaking in Mandarin Chinese
Spoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.[62]

The sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between them. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.[63]

Consonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave. Formants are the amplitude peaks in the frequency spectrum of a specific sound.[63][64]

Vowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity.[63] Vowels are called close when the lips are relatively closed, as in the pronunciation of the vowel [i] (English "ee"), or open when the lips are relatively open, as in the vowel [a] (English "ah"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as [u] (English "oo"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between [i] (unrounded front vowel such as English "ee") and [y] (rounded front vowel such as German "ü").[65]

Consonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called occlusive or stop, or different degrees of aperture creating fricatives and approximants. Consonants can also be either voiced or unvoiced, depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English [s] in bus (unvoiced sibilant) from [z] in buzz (voiced sibilant).[66]

Some speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called nasals or nasalized sounds. Other sounds are defined by the way the tongue moves within the mouth such as the l-sounds (called laterals, because the air flows along both sides of the tongue), and the r-sounds (called rhotics).[64]

By using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.[67]

Modality
Human languages display considerable plasticity[1] in their deployment of two fundamental modes: oral (speech and mouthing) and manual (sign and gesture).[note 3] For example, it is common for oral language to be accompanied by gesture, and for sign language to be accompanied by mouthing. In addition, some language communities use both modes to convey lexical or grammatical meaning, each mode complementing the other. Such bimodal use of language is especially common in genres such as story-telling (with Plains Indian Sign Language and Australian Aboriginal sign languages used alongside oral language, for example), but also occurs in mundane conversation. For instance, many Australian languages have a rich set of case suffixes that provide details about the instrument used to perform an action. Others lack such grammatical precision in the oral mode, but supplement it with gesture to convey that information in the sign mode. In Iwaidja, for example, 'he went out for fish using a torch' is spoken as simply "he-hunted fish torch", but the word for 'torch' is accompanied by a gesture indicating that it was held. In another example, the ritual language Damin had a heavily reduced oral vocabulary of only a few hundred words, each of which was very general in meaning, but which were supplemented by gesture for greater precision (e.g., the single word for fish, l*i, was accompanied by a gesture to indicate the kind of fish).[68]
India, officially the Republic of India,[j][20] is a country in South Asia. It is the seventh-largest country by area; the most populous country since 2023;[21] and, since its independence in 1947, the world's most populous democracy.[22][23][24] Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Myanmar, Thailand, and Indonesia.

Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.[26][27][28] Their long occupation, predominantly in isolation as hunter-gatherers, has made the region highly diverse.[29] Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[30] By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest.[31][32] Its hymns recorded the early dawnings of Hinduism in India.[33] India's pre-existing Dravidian languages were supplanted in the northern regions.[34] By 400 BCE, caste had emerged within Hinduism,[35] and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.[36] Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires.[37] Widespread creativity suffused this era,[38] but the status of women declined,[39] and untouchability became an organised belief.[l][40] In South India, the Middle kingdoms exported Dravidian language scripts and religious cultures to the kingdoms of Southeast Asia.[41]

In the 1st millennium, Islam, Christianity, Judaism, and Zoroastrianism became established on India's southern and western coasts.[42] In the early centuries of the 2nd millennium Muslim armies from Central Asia intermittently overran India's northern plains.[43] The resulting Delhi Sultanate drew northern India into the cosmopolitan networks of medieval Islam.[44] In south India, the Vijayanagara Empire created a long-lasting composite Hindu culture.[45] In the Punjab, Sikhism emerged, rejecting institutionalised religion.[46] The Mughal Empire ushered in two centuries of economic expansion and relative peace,[47] and left a a rich architectural legacy.[48][49] Gradually expanding rule of the British East India Company turned India into a colonial economy but consolidated its sovereignty.[50] British Crown rule began in 1858. The rights promised to Indians were granted slowly,[51][52] but technological changes were introduced, and modern ideas of education and the public life took root.[53] A nationalist movement emerged in India, the first in the non-European British Empire and an influence on other nationalist movements.[54][55] Noted for nonviolent resistance after 1920,[56] it became the primary factor in ending British rule.[57] In 1947, the British Indian Empire was partitioned into two independent dominions,[58][59][60][61] a Hindu-majority dominion of India and a Muslim-majority dominion of Pakistan. A large-scale loss of life and an unprecedented migration accompanied the partition.[62]

India has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to over 1.4 billion in 2023.[63] During this time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. A comparatively destitute country in 1951,[64] India has become a fast-growing major economy and a hub for information technology services, with an expanding middle class.[65] India has reduced its poverty rate, though at the cost of increasing economic inequality.[66] It is a nuclear-weapon state that ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century.[67] Among the socio-economic challenges India faces are gender inequality, child malnutrition,[68] and rising levels of air pollution.[69] India's land is megadiverse with four biodiversity hotspots.[70] India's wildlife, which has traditionally been viewed with tolerance in its culture,[71] is supported in protected habitats.

Etymology
Main article: Names for India
According to the Oxford English Dictionary, the English proper noun "India" derives most immediately from the Classical Latin India, a reference to a loosely-defined historical region of Asia stretching from South Asia to the borders of China. Further etymons are: Hellenistic Greek India (Ἰνδία); Ancient Greek Indos (Ἰνδός), or the River Indus; Achaemenian Old Persian Hindu (an eastern province of the Achaemenid Empire); and Sanskrit Sindhu, or "river," but specifically the Indus river, and by extension its well-settled basin.[72] The Ancient Greeks referred to South Asians as Indoi, 'the people of the Indus'.[73]

The term Bharat (Bhārat; pronounced [ˈbʱaːɾət] ⓘ), mentioned in both Indian epic poetry and the Constitution of India,[74][75] is used in its variations by many Indian languages. A modern rendering of the historical name Bharatavarsha, which applied originally to North India,[76][77] Bharat gained increased currency from the mid-19th century as a native name for India.[74][78]

Hindustan ([ɦɪndʊˈstaːn] ⓘ) is a Middle Persian name for India that became popular by the 13th century,[79] and was used widely since the era of the Mughal Empire. The meaning of Hindustan has varied, referring to a region encompassing the northern Indian subcontinent (present-day northern India and Pakistan) or to India in its near entirety.[74][78][80]

History
Main article: History of India
Ancient India

Manuscript illustration, c. 1650, of the Sanskrit epic Ramayana, composed in story-telling fashion c. 400 BCE – c. 300 CE[81]
Based on coalescence of Mitochondrial DNA and Y Chromosome data, it is thought that the earliest extant lineages of anatomically modern humans or Homo sapiens on the Indian subcontinent had reached there from Africa between 80,000 and 50,000 years ago, and with high likelihood by 55,000 years ago.[82][26][27][28] However, the earliest known modern human fossils in South Asia date to about 30,000 years ago.[27] Evidence for domestication of food crops and animals, construction of permanent structures, and storage of agricultural surplus appeared in Mehrgarh and other sites in Balochistan, Pakistan after 6500 BCE.[83] These gradually developed into the Indus Valley Civilisation,[84][83] the first urban culture in South Asia,[85] which flourished during 2500–1900 BCE in Pakistan and western India.[86] Centred around cities such as Mohenjo-daro, Harappa, Dholavira, and Kalibangan, and relying on varied forms of subsistence, the civilisation engaged robustly in crafts production and wide-ranging trade.[85]

During the period 2000–500 BCE, many regions of the subcontinent transitioned from the Chalcolithic cultures to the Iron Age ones.[87] The Vedas, the oldest scriptures associated with Hinduism,[88] were composed during this period,[89] and historians have analysed these to posit a Vedic culture in the Punjab region and the upper Gangetic Plain.[87] Most historians also consider this period to have encompassed several waves of Indo-Aryan migration into the subcontinent from the north-west.[88] The caste system, which created a hierarchy of priests, warriors, and free peasants, but which excluded indigenous peoples by labelling their occupations impure, arose during this period.[90] On the Deccan Plateau, archaeological evidence from this period suggests the existence of a chiefdom stage of political organisation.[87] In South India, a progression to sedentary life is indicated by the large number of megalithic monuments dating from this period,[91] as well as by nearby traces of agriculture, irrigation tanks, and craft traditions.[91]


Cave 26 of the rock-cut Ajanta Caves
In the late Vedic period, around the 6th century BCE, the small states and chiefdoms of the Ganges Plain and the north-western regions had consolidated into 16 major oligarchies and monarchies that were known as the mahajanapadas.[92][93] The emerging urbanisation gave rise to non-Vedic religious movements, two of which became independent religions. Jainism came into prominence during the life of its exemplar, Mahavira.[94] Buddhism, based on the teachings of Gautama Buddha, attracted followers from all social classes excepting the middle class; chronicling the life of the Buddha was central to the beginnings of recorded history in India.[95][96][97]

In an age of increasing urban wealth, both religions held up renunciation as an ideal,[98] and both established long-lasting monastic traditions. Politically, by the 3rd century BCE, the kingdom of Magadha had annexed or reduced other states to emerge as the Maurya Empire.[99] The empire was once thought to have controlled most of the subcontinent except the far south, but its core regions are now thought to have been separated by large autonomous areas.[100][101] The Mauryan kings are known as much for their empire-building and determined management of public life as for Ashoka's renunciation of militarism and far-flung advocacy of the Buddhist dhamma.[102][103]

The Sangam literature of the Tamil language reveals that, between 200 BCE and 200 CE, the southern peninsula was ruled by the Cheras, the Cholas, and the Pandyas, dynasties that traded extensively with the Roman Empire and with West and Southeast Asia.[104][105] In North India, Hinduism asserted patriarchal control within the family, leading to increased subordination of women.[106][99] By the 4th and 5th centuries, the Gupta Empire had created a complex system of administration and taxation in the greater Ganges Plain; this system became a model for later Indian kingdoms.[107][108] Under the Guptas, a renewed Hinduism based on devotion, rather than the management of ritual, began to assert itself.[109] This renewal was reflected in a flowering of sculpture and architecture, which found patrons among an urban elite.[108] Classical Sanskrit literature flowered as well, and Indian science, astronomy, medicine, and mathematics made significant advances.[108]

Medieval India
Main article: Medieval India

Brihadeshwara temple, Thanjavur, completed in 1010 CE

The Qutub Minar, 73 m (240 ft) tall, completed by the Sultan of Delhi, Iltutmish
The Indian early medieval age, from 600 to 1200 CE, is defined by regional kingdoms and cultural diversity.[110] When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647 CE, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan.[111] When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal.[111] When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south.[111] No ruler of this period was able to create an empire and consistently control lands much beyond their core region.[110] During this time, pastoral peoples, whose land had been cleared to make way for the growing agricultural economy, were accommodated within caste society, as were new non-traditional ruling classes.[112] The caste system consequently began to show regional differences.[112]

In the 6th and 7th centuries, the first devotional hymns were created in the Tamil language.[113] They were imitated all over India and led to both the resurgence of Hinduism and the development of all modern languages of the subcontinent.[113] Indian royalty, big and small, and the temples they patronised drew citizens in great numbers to the capital cities, which became economic hubs as well.[114] Temple towns of various sizes began to appear everywhere as India underwent another urbanisation.[114] By the 8th and 9th centuries, the effects were felt in Southeast Asia, as South Indian culture and political systems were exported to lands that became part of modern-day Myanmar, Thailand, Laos, Brunei, Cambodia, Vietnam, Philippines, Malaysia, and Indonesia.[115] Indian merchants, scholars, and sometimes armies were involved in this transmission; Southeast Asians took the initiative as well, with many sojourning in Indian seminaries and translating Buddhist and Hindu texts into their languages.[115]

After the 10th century, Muslim Central Asian nomadic clans, using swift-horse cavalry and raising vast armies united by ethnicity and religion, repeatedly overran South Asia's north-western plains, leading eventually to the establishment of the Islamic Delhi Sultanate in 1206.[116] The sultanate was to control much of North India and to make many forays into South India. Although at first disruptive for the Indian elites, the sultanate largely left its vast non-Muslim subject population to its own laws and customs.[117][118]

By repeatedly repulsing Mongol raiders in the 13th century, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north.[119][120] The sultanate's raiding and weakening of the regional kingdoms of South India paved the way for the indigenous Vijayanagara Empire.[121] Embracing a strong Shaivite tradition and building upon the military technology of the sultanate, the empire came to control much of peninsular India,[122] and was to influence South Indian society for long afterwards.[121]

Early modern India

A distant view of the Taj Mahal from the Agra Fort

A two-mohur Company gold coin, issued in 1835, the obverse inscribed "William IIII, King"
In the early 16th century, northern India, then under mainly Muslim rulers,[123] fell again to the superior mobility and firepower of a new generation of Central Asian warriors.[124] The resulting Mughal Empire did not stamp out the local societies it came to rule. Instead, it balanced and pacified them through new administrative practices[125][126] and diverse and inclusive ruling elites,[127] leading to more systematic, centralised, and uniform rule.[128] Eschewing tribal bonds and Islamic identity, especially under Akbar, the Mughals united their far-flung realms through loyalty, expressed through a Persianised culture, to an emperor who had near-divine status.[127]

The Mughal state's economic policies, deriving most revenues from agriculture[129] and mandating that taxes be paid in the well-regulated silver currency,[130] caused peasants and artisans to enter larger markets.[128] The relative peace maintained by the empire during much of the 17th century was a factor in India's economic expansion,[128] resulting in greater patronage of painting, literary forms, textiles, and architecture.[131] Newly coherent social groups in northern and western India, such as the Marathas, the Rajputs, and the Sikhs, gained military and governing ambitions during Mughal rule, which, through collaboration or adversity, gave them both recognition and military experience.[132] Expanding commerce during Mughal rule gave rise to new Indian commercial and political elites along the coasts of southern and eastern India.[132] As the empire disintegrated, many among these elites were able to seek and control their own affairs.[133]

By the early 18th century, with the lines between commercial and political dominance being increasingly blurred, a number of European trading companies, including the English East India Company, had established coastal outposts.[134][135] The East India Company's control of the seas, greater resources, and more advanced military training and technology led it to increasingly assert its military strength and caused it to become attractive to a portion of the Indian elite; these factors were crucial in allowing the company to gain control over the Bengal region by 1765 and sideline the other European companies.[136][134][137][138] Its further access to the riches of Bengal and the subsequent increased strength and size of its army enabled it to annexe or subdue most of India by the 1820s.[139] India was then no longer exporting manufactured goods as it long had, but was instead supplying the British Empire with raw materials. Many historians consider this to be the onset of India's colonial period.[134] By this time, with its economic power severely curtailed by the British parliament and having effectively been made an arm of British administration, the East India Company began more consciously to enter non-economic arenas, including education, social reform, and culture.[140]
Historians consider India's modern age to have begun sometime between 1848 and 1885. The appointment in 1848 of Lord Dalhousie as Governor General of the East India Company set the stage for changes essential to a modern state. These included the consolidation and demarcation of sovereignty, the surveillance of the population, and the education of citizens. Technological changes—among them, railways, canals, and the telegraph—were introduced not long after their introduction in Europe.[141][142][143][144] Disaffection with the company also grew during this time and set off the Indian Rebellion of 1857. Fed by diverse resentments and perceptions, including invasive British-style social reforms, harsh land taxes, and summary treatment of some rich landowners and princes, the rebellion rocked many regions of northern and central India and shook the foundations of Company rule.[145][146]

Although the rebellion was suppressed by 1858, it led to the dissolution of the East India Company and the direct administration of India by the British government. Proclaiming a unitary state and a gradual but limited British-style parliamentary system, the new rulers also protected princes and landed gentry as a feudal safeguard against future unrest.[147][148] In the decades following, public life gradually emerged all over India, leading eventually to the founding of the Indian National Congress (generally referred to as the Congress) in 1885.[149][150][151][152]


Jawaharlal Nehru sharing a light moment with Mahatma Gandhi, Mumbai, 6 July 1946
The rush of technology and the commercialisation of agriculture in the second half of the 19th century was marked by economic setbacks, and many small farmers became dependent on the whims of far-away markets.[153] There was an increase in the number of large-scale famines,[154] and, despite the risks of infrastructure development borne by Indian taxpayers, little industrial employment was generated for Indians.[155] There were also salutary effects: commercial cropping, especially in the newly canalled Punjab, led to increased food production for internal consumption.[156] The railway network provided critical famine relief,[157] notably reduced the cost of moving goods,[157] and helped nascent Indian-owned industry.[156]

After World War I, in which approximately one million Indians served,[158] a new period began. It was marked by British reforms but also repressive legislation, by more strident Indian calls for self-rule, and by the beginnings of a nonviolent movement of non-co-operation, of which Mahatma Gandhi would become the leader and enduring symbol.[159] During the 1930s, slow legislative reform was enacted by the British; the Indian National Congress won victories in the resulting elections.[160] The next decade was beset with crises: Indian participation in World War II, the Congress's final push for non-co-operation, and an upsurge of Muslim nationalism. All were capped by the advent of independence in 1947, but tempered by the partition of India into two states: India and Pakistan.[161]

Vital to India's self-image as an independent nation was its constitution, completed in 1950, which put in place a secular and democratic republic.[162] Economic liberalisation, which began in the 1980s and with the collaboration with Soviet Union for technical knowledge,[163] has created a large urban middle class, transformed India into one of the world's fastest-growing economies,[164] and increased its geopolitical influence. Yet, India is also shaped by persistent poverty, both rural and urban;[165] by religious and caste-related violence;[166] by Maoist-inspired Naxalite insurgencies;[167] and by separatism in Jammu and Kashmir and in Northeast India.[168] It has unresolved territorial disputes with China and with Pakistan.[169] India's sustained democratic freedoms are unique among the world's newer nations; however, in spite of its recent economic successes, freedom from want for its disadvantaged population remains a goal yet to be achieved.[170]

Geography
Main articles: Geography of India and Himalayas

A panaromic view of the Garhwal- and Kumaon Himalayas. Rising above their surroundings in the view are among others, Trisul, Nanda Devi, and Nanda Kot. Nanda Devi, the highest peak entirely within India's borders, is at the centre of the Nanda Devi National Park, a UNESCO World Heritage Natural Site.

The Tungabhadra, with rocky outcrops, flows into the peninsular Krishna River.[171]
India accounts for the bulk of the Indian subcontinent, lying atop the Indian tectonic plate, and a part of the Indo-Australian Plate.[172] India's defining geologic processes began approximately 70 million years ago, when the Indian Plate, then part of the southern supercontinent Gondwana, began a north-eastward drift caused by seafloor spreading to its south-west, and later, south and south-east.[172] Simultaneously, the vast Tethyan oceanic crust, to its northeast, began to subduct under the Eurasian Plate.[172] The Indian continental crust, however, was obstructed and sheared horizontally. Its lower crust and mantle slid under, but the upper layer piled up in sheets (or nappes) ahead of the subduction zone.[173] This created the orogeny, or process of mountain building, of the Himalayas.[174] The middle and stiffer layer continued to push into Tibet, causing crustal thickening and giving rise to the Tibetan Plateau.[175] Immediately south of the emerging Himalayas, plate movement created a vast crescent-shaped trough that rapidly filled with river-borne sediment[176] and now constitutes the Indo-Gangetic Plain.[177] The original Indian plate makes its first appearance above the sediment in the ancient Aravalli range, which extends from the Delhi Ridge in a southwesterly direction. To the west lies the Thar Desert, the eastern spread of which is checked by the Aravallis.[178][179][180]

The remaining Indian Plate survives as peninsular India, the oldest and geologically most stable part of India. It extends as far north as the Satpura and Vindhya ranges in central India. These parallel chains run from the Arabian Sea coast in Gujarat in the west to the coal-rich Chota Nagpur Plateau in Jharkhand in the east.[181] To the south, the remaining peninsular landmass, the Deccan Plateau, is flanked on the west and east by coastal ranges known as the Western and Eastern Ghats;[182] the plateau contains the country's oldest rock formations, some over one billion years old. Constituted in such fashion, India lies to the north of the equator between 6° 44′ and 35° 30′ north latitude[m] and 68° 7′ and 97° 25′ east longitude.[183]

Major Himalayan-origin rivers that substantially flow through India include the Ganges and the Brahmaputra, both of which drain into the Bay of Bengal.[184] Important tributaries of the Ganges include the Yamuna and the Kosi. The Kosi's extremely low gradient, caused by long-term silt deposition, leads to severe floods and course changes.[185][186] Major peninsular rivers, whose steeper gradients prevent their waters from flooding, include the Godavari, the Mahanadi, the Kaveri, and the Krishna, which also drain into the Bay of Bengal;[187] and the Narmada and the Tapti, which drain into the Arabian Sea.[188]

India's coastline measures 7,517 kilometres (4,700 mi) in length; of this distance, 5,423 kilometres (3,400 mi) belong to peninsular India and 2,094 kilometres (1,300 mi) to the Andaman, Nicobar, and Lakshadweep island chains.[189] According to the Indian naval hydrographic charts, the mainland coastline consists of the following: 43% sandy beaches; 11% rocky shores, including cliffs; and 46% mudflats or marshy shores.[189]Coastal features include the marshy Rann of Kutch of western India and the alluvial Sundarbans delta of eastern India; the latter is shared with Bangladesh.[190] India has two archipelagos: the Lakshadweep, coral atolls off India's south-western coast; and the Andaman and Nicobar Islands, a volcanic chain in the Andaman Sea.[191]

Climate
Main article: Climate of India

Fishing boats are moored and lashed together during an approaching monsoon storm whose dark clouds can be seen overhead. The scene is a tidal creek in Anjarle, a coastal village in Maharashtra.
The Indian climate is strongly influenced by the Himalayas and the Thar Desert, both of which drive the economically and culturally pivotal summer and winter monsoons.[192] The Himalayas prevent cold Central Asian katabatic winds from blowing in, keeping the bulk of the Indian subcontinent warmer than most locations at similar latitudes.[193][194] The Thar Desert plays a crucial role in attracting the moisture-laden south-west summer monsoon winds that, between June and October, provide the majority of India's rainfall.[192]

Four major climatic groupings predominate in India: tropical wet, tropical dry, subtropical humid, and montane.[195] Temperatures in India have risen by 0.7 °C (1.3 °F) between 1901 and 2018.[196] Climate change in India is often thought to be the cause. The retreat of Himalayan glaciers has adversely affected the flow rate of the major Himalayan rivers, including the Ganges and the Brahmaputra.[197] According to some current projections, the number and severity of droughts in India will have markedly increased by the end of the present century.[198]

Biodiversity
Main articles: Forestry in India and Wildlife of India

India has the majority of the world's wild tigers, approximately 3,170 in 2022.[199]

A chital (Axis axis) stag in the Nagarhole National Park in a region covered by a moderately dense[n] forest

Three of the last Asiatic cheetahs in India were shot dead in 1948 in Surguja district, Madhya Pradesh, Central India, by Maharajah Ramanuj Pratap Singh Deo. The young male cheetahs, all from the same litter, were sitting together when they were shot at night.
India is a megadiverse country, a term employed for 17 countries that display high biological diversity and contain many species exclusively indigenous, or endemic, to them.[200] India is the habitat for 8.6% of all mammals, 13.7% of bird species, 7.9% of reptile species, 6% of amphibian species, 12.2% of fish species, and 6.0% of all flowering plant species.[201][202] Fully a third of Indian plant species are endemic.[203] India also contains four of the world's 34 biodiversity hotspots,[70] or regions that display significant habitat loss in the presence of high endemism.[o][204]

India's most dense forests, such as the tropical moist forest of the Andaman Islands, the Western Ghats, and Northeast India, occupy approximately 3% of its land area.[205][206] Moderately dense forest, whose canopy density is between 40% and 70%, occupies 9.39% of India's land area.[205][206] It predominates in the temperate coniferous forest of the Himalayas, the moist deciduous sal forest of eastern India, and the dry deciduous teak forest of central and southern India.[207] India has two natural zones of thorn forest, one in the Deccan Plateau, immediately east of the Western Ghats, and the other in the western part of the Indo-Gangetic plain, now turned into rich agricultural land by irrigation, its features no longer visible.[208] Among the Indian subcontinent's notable indigenous trees are the astringent Azadirachta indica, or neem, which is widely used in rural Indian herbal medicine,[209] and the luxuriant Ficus religiosa, or peepul,[210] which is displayed on the ancient seals of Mohenjo-daro,[211] and under which the Buddha is recorded in the Pali canon to have sought enlightenment.[212]

Many Indian species have descended from those of Gondwana, the southern supercontinent from which India separated more than 100 million years ago.[213] India's subsequent collision with Eurasia set off a mass exchange of species. However, volcanism and climatic changes later caused the extinction of many endemic Indian forms.[214] Still later, mammals entered India from Asia through two zoogeographic passes flanking the Himalayas.[215] This lowered endemism among India's mammals, which stands at 12.6%, contrasting with 45.8% among reptiles and 55.8% among amphibians.[202] Among endemics are the vulnerable[216] hooded leaf monkey[217] and the threatened Beddome's toad[218][219] of the Western Ghats.

India contains 172 IUCN-designated threatened animal species, or 2.9% of endangered forms.[220] These include the endangered Bengal tiger and the Ganges river dolphin. Critically endangered species include the gharial, a crocodilian; the great Indian bustard; and the Indian white-rumped vulture, which has become nearly extinct by having ingested the carrion of diclofenac-treated cattle.[221] Before they were extensively used for agriculture and cleared for human settlement, the thorn forests of Punjab were mingled at intervals with open grasslands that were grazed by large herds of blackbuck preyed on by the Asiatic cheetah; the blackbuck, no longer extant in Punjab, is now severely endangered in India, and the cheetah is extinct.[222] The pervasive and ecologically devastating human encroachment of recent decades has critically endangered Indian wildlife. In response, the system of national parks and protected areas, first established in 1935, was expanded substantially. In 1972, India enacted the Wildlife Protection Act[223] and Project Tiger to safeguard crucial wilderness; the Forest Conservation Act was enacted in 1980 and amendments added in 1988.[224] India hosts more than five hundred wildlife sanctuaries and eighteen biosphere reserves,[225] four of which are part of the World Network of Biosphere Reserves; its eighty-nine wetlands are registered under the Ramsar Convention.[226]

Government and politics
Politics
Main article: Politics of India
See also: Democracy in India

As part of Janadesh 2007, 25,000 pro–land reform landless people in Madhya Pradesh listen to Rajagopal P. V.[227]

US president Barack Obama addresses the members of the Parliament of India in New Delhi in November 2010.
India is a parliamentary republic with a multi-party system.[228] It has six recognised national parties, including the Indian National Congress (generally referred to as the Congress) and the Bharatiya Janata Party (BJP), and over 50 regional parties.[229] The Congress is considered the ideological centre in Indian political culture,[230] whereas the BJP is right-wing to far-right.[231][232][233] From 1950 to the late 1980s, Congress held a majority in India's parliament. Afterwards, it increasingly shared power with the BJP,[234] as well as with powerful regional parties, which forced multi-party coalition governments at the centre.[235]

In the general elections in 1951, 1957, and 1962, the Congress, led by Jawaharlal Nehru, won easy victories. On Nehru's death in 1964, Lal Bahadur Shastri briefly became prime minister; he was succeeded in 1966, by Nehru's daughter Indira Gandhi, who led the Congress to election victories in 1967 and 1971. Following public discontent with the state of emergency Indira Gandhi had declared in 1975, the Congress was voted out of power in 1977; Janata Party, which had opposed the emergency, was voted in. Its government lasted two years; Morarji Desai and Charan Singh served as prime ministers. After the Congress was returned to power in 1980, Indira Gandhi was assassinated and succeeded by Rajiv Gandhi, who won comfortably in the elections later that year. A National Front coalition led by the Janata Dal in alliance with the Left Front won the 1989 elections, with the subsequent government lasting just under two years, and V.P. Singh and Chandra Shekhar serving as prime ministers.[236] In the 1991 Indian general election, the Congress, as the largest single party, formed a minority government led by P. V. Narasimha Rao.[237]

After the 1996 Indian general election, the BJP formed a government briefly; it was followed by United Front coalitions, which depended on external political support. Two prime ministers served during this period: H.D. Deve Gowda and I.K. Gujral. In 1998, the BJP formed a coalition—the National Democratic Alliance (NDA). Led by Atal Bihari Vajpayee, the NDA became the first non-Congress, coalition government to complete a five-year term.[238] In the 2004 Indian general elections, no party won an absolute majority. Still, the Congress emerged as the largest single party, forming another successful coalition: the United Progressive Alliance (UPA). It had the support of left-leaning parties and MPs who opposed the BJP. The UPA returned to power in the 2009 general election with increased numbers, and it no longer required external support from India's communist parties.[239] Manmohan Singh became the first prime minister since Jawaharlal Nehru in 1957 and 1962 to be re-elected to a consecutive five-year term.[240] In the 2014 general election, the BJP became the first political party since 1984 to win an absolute majority.[241] In the 2019 general election, the BJP regained an absolute majority. In the 2024 general election, a BJP-led NDA coalition formed the government. Narendra Modi, a former chief minister of Gujarat, is in his third term as the prime minister of India and has served in the position since 26 May 2014.[242]

Government
Main article: Government of India
See also: Constitution of India

Rashtrapati Bhavan, the official residence of the President of India, was designed by British architects Edwin Lutyens and Herbert Baker for the Viceroy of India, and constructed between 1911 and 1931 during the British Raj.[243]
India is a federation with a parliamentary system governed under the Constitution of India. Federalism in India defines the power distribution between the union and the states. India's form of government, traditionally described as "quasi-federal" with a strong centre and weak states,[244] has grown increasingly federal since the late 1990s as a result of political, economic, and social changes.[245][246]

The Government of India comprises three branches: the Executive, Legislature, and Judiciary.[247] The President of India is the ceremonial head of state,[248] who is elected indirectly for a five-year term by an electoral college comprising members of national and state legislatures.[249][250] The Prime Minister of India is the head of government and exercises most executive power.[251] Appointed by the president,[252] the prime minister is supported by the party or political alliance with a majority of seats in the lower house of parliament.[251] The executive of the Indian government consists of the president, the vice-president, and the Union Council of Ministers—with the cabinet being its executive committee—headed by the prime minister. Any minister holding a portfolio must be a member of one of the houses of parliament.[248] In the Indian parliamentary system, the executive is subordinate to the legislature; the prime minister and their council are directly responsible to the lower house of the parliament. Civil servants act as permanent executives and all decisions of the executive are implemented by them.[253]

The legislature of India is the bicameral parliament. Operating under a Westminster-style parliamentary system, it comprises an upper house called the Rajya Sabha (Council of States) and a lower house called the Lok Sabha (House of the People).[254] The Rajya Sabha is a permanent body of 245 members who serve staggered six-year terms with elections every 2 years.[255] Most are elected indirectly by the state and union territorial legislatures in numbers proportional to their state's share of the national population.[252] The Lok Sabha's 543 members are elected directly by popular vote among citizens aged at least 18;[256] they represent single-member constituencies for five-year terms.[257] Several seats from each state are reserved for candidates from Scheduled Castes and Scheduled Tribes in proportion to their population within that state.[256]

India has a three-tier unitary independent judiciary[258] comprising the supreme court, headed by the Chief Justice of India, 25 high courts, and a large number of trial courts.[258] The supreme court has original jurisdiction over cases involving fundamental rights and over disputes between states and the centre and has appellate jurisdiction over the high courts.[259] It has the power to both strike down union or state laws which contravene the constitution[260] and invalidate any government action it deems unconstitutional.[261]

Administrative divisions
Main article: Administrative divisions of India
See also: Political integration of India

A clickable map of the 28 states and 8 union territories of India
India is a federal union comprising 28 states and 8 union territories.[12] All states, as well as the union territories of Jammu and Kashmir, Puducherry and the National Capital Territory of Delhi, have elected legislatures and governments following the Westminster system. The remaining five union territories are directly ruled by the central government through appointed administrators. In 1956, under the States Reorganisation Act, states were reorganised on a linguistic basis.[262] There are over a quarter of a million local government bodies at city, town, block, district and village levels.[263]

States
Andhra Pradesh
Arunachal Pradesh
Assam
Bihar
Chhattisgarh
Goa
Gujarat
Haryana
Himachal Pradesh
Jharkhand
Karnataka
Kerala
Madhya Pradesh
Maharashtra
Manipur
Meghalaya
Mizoram
Nagaland
Odisha
Punjab
Rajasthan
Sikkim
Tamil Nadu
Telangana
Tripura
Uttar Pradesh
Uttarakhand
West Bengal
Union territories
Andaman and Nicobar Islands
Chandigarh
Dadra and Nagar Haveli and Daman and Diu
Jammu and Kashmir
Ladakh
Lakshadweep
National Capital Territory of Delhi
Puducherry
Foreign relations
Main article: Foreign relations of India

In the 1950s and 60s, India played a pivotal role in the Non-Aligned Movement.[264] From left to right: Gamal Abdel Nasser of United Arab Republic (now Egypt), Josip Broz Tito of Yugoslavia and Jawaharlal Nehru in Belgrade, September 1961.

The Indian Air Force contingent marching at the 221st Bastille Day military parade in Paris, July 2009. The parade at which India was the foreign guest was led by India's oldest regiment, the Maratha Light Infantry, founded in 1768.[265]
India became a republic in 1950, remaining a member of the Commonwealth of Nations.[266][267] India strongly supported decolonisation in Africa and Asia in the 1950s; it played a leading role in the Non-Aligned Movement.[268] After initially cordial relations, India suffered a humiliating military defeat to China in a 1962 war.[269] Another military conflict followed in 1967 in which India successfully repelled a Chinese attack.[270]

India has had uneasy relations with its western neighbour, Pakistan. The two countries went to war in 1947, 1965, 1971, and 1999. Three of these wars were fought over the disputed territory of Kashmir. In contrast, the 1971 war followed India's support for the independence of Bangladesh.[271] After the 1965 war with Pakistan, India began to pursue close military and economic ties with the Soviet Union. By the late 1960s, the Soviet Union was its largest arms supplier.[272] India has played a key role in the South Asian Association for Regional Cooperation and the World Trade Organization.[citation needed] The nation has supplied over 290,000 military and police personnel in over 50 UN peacekeeping operations.[273]

China's nuclear test of 1964 and threats to intervene in support of Pakistan in the 1965 war caused India to produce nuclear weapons.[274] India conducted its first nuclear weapons test in 1974 and carried out additional underground testing in 1998. India has signed neither the Comprehensive Nuclear-Test-Ban Treaty nor the Nuclear Non-Proliferation Treaty, considering both to be flawed and discriminatory.[275] India maintains a "no first use" nuclear policy and is developing a nuclear triad capability as a part of its "Minimum Credible Deterrence" doctrine.[276][277]

Since the end of the Cold War, India has increased its economic, strategic, and military cooperation with the United States and the European Union.[278] In 2008, a civilian nuclear agreement was signed between India and the United States. Although India possessed nuclear weapons at the time and was not a party to the Nuclear Non-Proliferation Treaty, it received waivers from the International Atomic Energy Agency and the Nuclear Suppliers Group, ending earlier restrictions on India's nuclear technology and commerce; India subsequently signed co-operation agreements involving civilian nuclear energy with Russia,[279] France,[280] the United Kingdom,[281] and Canada.[282]

The President of India is the supreme commander of the nation's armed forces. With 1.45 million active troops, they are the world's second-largest military. It comprises the Indian Army, the Indian Navy, the Indian Air Force, and the Indian Coast Guard.[283] The official Indian defence budget for 2011 was US$36.03 billion, or 1.83% of GDP.[284] Defence expenditure was pegged at US$70.12 billion for fiscal year 2022–23 and, increased 9.8% on the previous fiscal year.[285][286] India is the world's second-largest arms importer; between 2016 and 2020, it accounted for 9.5% of the total global arms imports.[287] Much of the military expenditure was focused on defence against Pakistan and countering growing Chinese influence in the Indian Ocean.[288]

Economy
Main article: Economy of India

In 2019, 43% of India's total workforce was employed in agriculture.[289]

India is the world's largest producer of milk, with the largest population of cattle. In 2018, nearly 80% of India's milk was sourced from small farms with herd size between one and two, the milk harvested by hand milking.[291]

55% of India's female workforce was employed in agriculture in 2019.[290]
According to the International Monetary Fund (IMF), the Indian economy in 2024 was nominally worth $3.94 trillion; it is the fifth-largest economy by market exchange rates and is, at around $15.0 trillion, the third-largest by purchasing power parity (PPP).[16] With its average annual GDP growth rate of 5.8% over the past two decades, and reaching 6.1% during 2011–2012,[292] India is one of the world's fastest-growing economies.[293] However, due to its low GDP per capita—which ranks 136th in the world in nominal per capita income and 125th in per capita income adjusted for purchasing power parity (PPP)—the vast majority of Indians fall into the low-income group.[294][295]

Until 1991, all Indian governments followed protectionist policies that were influenced by socialist economics. Widespread state intervention and regulation largely walled the economy off from the outside world. An acute balance of payments crisis in 1991 forced the nation to liberalise its economy;[296] since then, it has moved increasingly towards a free-market system[297][298] by emphasising both foreign trade and direct investment inflows.[299] India has been a member of World Trade Organization since 1 January 1995.[300]

The 522-million-worker Indian labour force is the world's second largest, as of 2017.[283] The service sector makes up 55.6% of GDP, the industrial sector 26.3% and the agricultural sector 18.1%. India's foreign exchange remittances of US$100 billion in 2022,[301] highest in the world, were contributed to its economy by 32 million Indians working in foreign countries.[302] In 2006, the share of external trade in India's GDP stood at 24%, up from 6% in 1985.[297] In 2008, India's share of world trade was 1.7%;[303] In 2021, India was the world's ninth-largest importer and the sixteenth-largest exporter.[304] Between 2001 and 2011, the contribution of petrochemical and engineering goods to total exports grew from 14% to 42%.[305] India was the world's second-largest textile exporter after China in the 2013 calendar year.[306]

Averaging an economic growth rate of 7.5% for several years before 2007,[297] India has more than doubled its hourly wage rates during the first decade of the 21st century.[307] Some 431 million Indians have left poverty since 1985; India's middle classes are projected to number around 580 million by 2030.[308] In 2024, India's consumer market was the world's third largest.[309] India's nominal GDP per capita increased steadily from US$308 in 1991, when economic liberalisation began, to US$1,380 in 2010, to an estimated US$2,731 in 2024. It is expected to grow to US$3,264 by 2026.[16]

Industries

A tea garden in Sikkim. India, the world's second-largest producer of tea, is a nation of one billion tea drinkers, who consume 70% of India's tea output.
The Indian automotive industry, the world's second-fastest growing, increased domestic sales by 26% during 2009–2010,[310] and exports by 36% during 2008–2009.[311] In 2022, India became the world's third-largest vehicle market after China and the United States, surpassing Japan.[312] At the end of 2011, the Indian IT industry employed 2.8 million professionals, generated revenues close to US$100 billion equalling 7.5% of Indian GDP, and contributed 26% of India's merchandise exports.[313]

The pharmaceutical industry in India includes 3,000 pharmaceutical companies and 10,500 manufacturing units; India is the world's third-largest pharmaceutical producer, largest producer of generic medicines and supply up to 50–60% of global vaccines demand, these all contribute up to US$24.44 billions in exports and India's local pharmaceutical market is estimated up to US$42 billion.[314][315] India is among the top 12 biotech destinations in the world.[316][317] The Indian biotech industry grew by 15.1% in 2012–2013, increasing its revenues from ₹204.4 billion (Indian rupees) to ₹235.24 billion (US$3.94 billion at June 2013 exchange rates).[318]

Energy
Main article: Energy in India
See also: Energy policy of India
India's capacity to generate electrical power is 300 gigawatts, of which 42 gigawatts is renewable.[319] The country's usage of coal is a major cause of India's greenhouse gas emissions, but its renewable energy is competing strongly.[320][better source needed] India emits about 7% of global greenhouse gas emissions. This equates to about 2.5 tons of carbon dioxide per person per year, which is half the world average.[321][322] Increasing access to electricity and clean cooking with liquefied petroleum gas have been priorities for energy in India.[323]

Socio-economic challenges
Main articles: Poverty in India, Income inequality in India, and Debt bondage in India

Health workers about to begin another day of immunisation against infectious diseases in 2006. Eight years later, and three years after India's last case of polio, the World Health Organization declared India to be polio-free.[324]
Despite economic growth during recent decades, India continues to face socio-economic challenges. In 2006, India contained the largest number of people living below the World Bank's international poverty line of US$1.25 per day.[325] The proportion decreased from 60% in 1981 to 42% in 2005.[326] Under the World Bank's later revised poverty line, it was 21%-22.5 in 2011.[p][328][329] In 2019, the estimates had gone down to 10.2%.[329] In 2014, 30.7% of India's children under the age of five were underweight.[330] According to a Food and Agriculture Organization report in 2015, 15% of the population was undernourished.[331][332] The Midday Meal Scheme attempts to lower these rates.[333]

A 2018 Walk Free Foundation report estimated that nearly 8 million people in India were living in different forms of modern slavery, such as bonded labour, child labour, human trafficking, and forced begging.[334] According to the 2011 census, there were 10.1 million child labourers in the country, a decline of 2.6 million from 12.6 million in 2001.[335]

Since 1991, economic inequality between India's states has consistently grown: the per-capita net state domestic product of the richest states in 2007 was 3.2 times that of the poorest.[336] Corruption in India is perceived to have decreased. According to the Corruption Perceptions Index, India ranked 78th out of 180 countries in 2018, an improvement from 85th in 2014.[337][338]
With an estimated 1,428,627,663 residents in 2023, India is the world's most populous country.[13] 1,210,193,422 residents were reported in the 2011 provisional census report.[339] Its population grew by 17.64% from 2001 to 2011,[340] compared to 21.54% growth in the previous decade (1991–2001).[340] The human sex ratio, according to the 2011 census, is 940 females per 1,000 males.[339] The median age was 28.7 in 2020.[283]

The first post-colonial census, conducted in 1951, counted 361 million people.[341] Medical advances made in the last 50 years as well as increased agricultural productivity brought about by the "Green Revolution" have caused India's population to grow rapidly.[342] The life expectancy in India is 70 years to 71.5 years for women, and 68.7 years for men.[283] There are around 93 physicians per 100,000 people.[343]

Urbanisation
Main article: Urbanisation in India
Migration from rural to urban areas has been an important dynamic in India's recent history. The number of people living in urban areas grew by 31.2% between 1991 and 2001.[344] In 2001, over 70% lived in rural areas.[345][346] The level of urbanisation increased further from 27.81% in the 2001 census to 31.16% in the 2011 census. The slowing down of the overall population growth rate was due to the sharp decline in the growth rate in rural areas since 1991.[347] In the 2011 census, there were 53 million-plus urban agglomerations in India. Among them Mumbai, Delhi, Kolkata, Chennai, Bengaluru, Hyderabad and Ahmedabad, in decreasing order by population.[348]

According to several air quality reports, 83 out of the top 100 most polluted cities in the world are located in India.[349][350][351][352][353]

Languages
Main article: Languages of India
Among speakers of the Indian languages, 74% speak Indo-Aryan languages (the easternmost branch of the Indo-European languages), 24% speak Dravidian languages (indigenous to South Asia and spoken widely before the spread of Indo-Aryan languages), and 2% speak Austroasiatic languages or the Sino-Tibetan languages. India has no national language.[354] Hindi, with the largest number of speakers, is the official language of the government.[355][356] English is used extensively in business and administration and has the status of a "subsidiary official language";[6] it is important in education, especially as a medium of higher education. Each state and union territory has one or more official languages, and the constitution recognises in particular 22 "scheduled languages".

Religion
Main article: Religion in India
The 2011 census reported the religion in India with the largest number of followers was Hinduism (79.80% of the population), followed by Islam (14.23%); the remaining were Christianity (2.30%), Sikhism (1.72%), Buddhism (0.70%), Jainism (0.36%) and others[q] (0.9%).[11] India has the third-largest Muslim population—the largest for a non-Muslim majority country.[357][358]

Education
Main article: Education in India
See also: Literacy in India and History of education in the Indian subcontinent

Children awaiting school lunch in Rayka (also Raika), a village in rural Gujarat. The salutation Jai Bhim written on the blackboard honours the jurist, social reformer, and Dalit leader B. R. Ambedkar.
The literacy rate in 2011 was 74.04%: 65.46% among females and 82.14% among males.[359] The rural-urban literacy gap, which was 21.2 percentage points in 2001, dropped to 16.1 percentage points in 2011. The improvement in the rural literacy rate is twice that of urban areas.[347] Kerala is the most literate state with 93.91% literacy; while Bihar the least with 63.82%.[359] In the 2011 census, about 73% of the population was literate, with 81% for men and 65% for women. This compares to 1981 when the respective rates were 41%, 53% and 29%. In 1951, the rates were 18%, 27% and 9%. In 1921, the rates 7%, 12% and 2%. In 1891, they were 5%, 9% and 1%,[360][361] According to Latika Chaudhary, in 1911 there were under three primary schools for every ten villages. Statistically, more caste and religious diversity reduced private spending. Primary schools taught literacy, so local diversity limited its growth.[362]

The education system of India is the world's second-largest.[363] India has over 900 universities, 40,000 colleges[364] and 1.5 million schools.[365] In India's higher education system, a significant number of seats are reserved under affirmative action policies for the historically disadvantaged. In recent decades India's improved education system is often cited as one of the main contributors to its economic development.[366][367]

Health
Main article: Health in India
The life expectancy at birth has increased from 49.7 years in 1970–1975 to 72.0 years in 2023.[368][369] The under-five mortality rate for the country was 113 per 1,000 live births in 1994 whereas in 2018 it reduced to 41.1 per 1,000 live births.[368]

India bears a disproportionately large burden of the world's tuberculosis rates, with World Health Organization (WHO) statistics for 2022 estimating 2.8 million new infections annually, accounting for 26% of the global total.[370] It is estimated that approximately 40% of the population of India carry tuberculosis infection.[371]

In 2018 chronic obstructive pulmonary disease was the leading cause of death after heart disease. The 10 most polluted cities in the world are all in India with more than 140 million people breathing air 10 times or more over the WHO safe limit. In 2017, air pollution killed 1.24 million Indians.[372]

Culture
Main article: Culture of India
Society
Main articles: Caste system in India and Gender inequality in India

Muslims offer namaz at a mosque in Srinagar, Jammu and Kashmir.
The Indian caste system embodies much of the social stratification and many of the social restrictions found on the Indian subcontinent. Social classes are defined by thousands of endogamous hereditary groups, often termed as jātis, or "castes".[373] India abolished untouchability in 1950 with the adoption of the constitution and has since enacted other anti-discriminatory laws and social welfare initiatives.[r] However, the system continues to be dominant in India, and caste-based inequality, discrimination, segregation, and violence persist.[375][376]

Multi-generational patrilineal joint families have been the norm in India, though nuclear families are becoming common in urban areas.[377] An overwhelming majority of Indians have their marriages arranged by their parents or other family elders.[378] Marriage is thought to be for life,[378] and the divorce rate is extremely low,[379] with less than one in a thousand marriages ending in divorce.[380] Child marriages are common, especially in rural areas; many women wed before reaching 18, which is their legal marriageable age.[381] Female infanticide in India, and lately female foeticide, have created skewed gender ratios; the number of missing women in the country quadrupled from 15 million to 63 million in the 50 years ending in 2014, faster than the population growth during the same period.[382] According to an Indian government study, an additional 21 million girls are unwanted and do not receive adequate care.[383] Despite a government ban on sex-selective foeticide, the practice remains commonplace in India, the result of a preference for boys in a patriarchal society.[384] The payment of dowry, although illegal, remains widespread across class lines.[385] Deaths resulting from dowry, mostly from bride burning, are on the rise, despite stringent anti-dowry laws.[386]

Visual art
Main article: Indian art
India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and Southeast Asia, the last also greatly influenced by Hindu art.[387] Thousands of seals from the Indus Valley civilisation of the third millennium BCE have been found, usually carved with animals, but also some with human figures. The Pashupati seal, excavated in Mohenjo-daro, Pakistan, in 1928–29, is the best known.[388][389] After this there is a long period with virtually nothing surviving.[389][390] Almost all surviving ancient Indian art thereafter is in various forms of religious sculpture in durable materials, or coins. There was probably originally far more in wood, which is lost. In north India Mauryan art is the first imperial movement.[391][392][393]

In the first millennium CE, Buddhist art spread with Indian religions to Central, East and Southeast Asia, the last also greatly influenced by Hindu art.[394] Over the following centuries a distinctly Indian style of sculpting the human figure developed, with less interest in articulating precise anatomy than ancient Greek sculpture but showing smoothly flowing forms expressing prana ("breath" or life-force).[395][396] This is often complicated by the need to give figures multiple arms or heads, or represent different genders on the left and right of figures, as with the Ardhanarishvara form of Shiva and Parvati.[397][398]

Most of the earliest large sculpture is Buddhist, either excavated from Buddhist stupas such as Sanchi, Sarnath and Amaravati,[399] or is rock cut reliefs at sites such as Ajanta, Karla and Ellora. Hindu and Jain sites appear rather later.[400][401] In spite of this complex mixture of religious traditions, generally, the prevailing artistic style at any time and place has been shared by the major religious groups, and sculptors probably usually served all communities.[402] Gupta art, at its peak c. 300 CE – c. 500 CE, is often regarded as a classical period whose influence lingered for many centuries after; it saw a new dominance of Hindu sculpture, as at the Elephanta Caves.[403][404] Across the north, this became rather stiff and formulaic after c. 800 CE, though rich with finely carved detail in the surrounds of statues.[405] But in the South, under the Pallava and Chola dynasties, sculpture in both stone and bronze had a sustained period of great achievement; the large bronzes with Shiva as Nataraja have become an iconic symbol of India.[406][407]

Ancient paintings have only survived at a few sites, of which the crowded scenes of court life in the Ajanta Caves are some of the most important.[408][409] Painted manuscripts of religious texts survive from Eastern India from 10th century onwards, most of the earliest being Buddhist and later Jain. These significantly influenced later artistic styles.[410] The Persian-derived Deccan painting, starting just before the Mughal miniature, between them give the first large body of secular painting, with an emphasis on portraits, and the recording of princely pleasures and wars.[411][412] The style spread to Hindu courts, especially among the Rajputs, and developed a variety of styles, with the smaller courts often the most innovative, with figures such as Nihâl Chand and Nainsukh.[413][414] As a market developed among European residents, it was supplied by Company painting by Indian artists with considerable Western influence.[415][416] In the 19th century, cheap Kalighat paintings of gods and everyday life, done on paper, were urban folk art from Calcutta, which later saw the Bengal School of Art, reflecting the art colleges founded by the British, the first movement in modern Indian painting.[417][418]

Bhutesvara Yakshis, Buddhist reliefs from Mathura, 2nd century CE
Bhutesvara Yakshis, Buddhist reliefs from Mathura, 2nd century CE
 
Gupta terracotta relief, Krishna Killing the Horse Demon Keshi, 5th century
Gupta terracotta relief, Krishna Killing the Horse Demon Keshi, 5th century
 
Elephanta Caves, triple-bust (trimurti) of Shiva, 18 feet (5.5 m) tall, c. 550
Elephanta Caves, triple-bust (trimurti) of Shiva, 18 feet (5.5 m) tall, c. 550
 
Chola bronze of Shiva as Nataraja ("Lord of Dance"), Tamil Nadu, 10th or 11th century
Chola bronze of Shiva as Nataraja ("Lord of Dance"), Tamil Nadu, 10th or 11th century
 
Jahangir Receives Prince Khurram at Ajmer on His Return from the Mewar Campaign, Balchand, c. 1635
Jahangir Receives Prince Khurram at Ajmer on His Return from the Mewar Campaign, Balchand, c. 1635
 
Krishna Fluting to the Milkmaids, Kangra painting, 1775–1785
Krishna Fluting to the Milkmaids, Kangra painting, 1775–1785
Music
Main article: Music of India
page is in the middle of an expansion or major revamping
This article or section is undergoing significant expansion or restructuring. You are welcome to assist in its construction by editing it as well. If this article or section has not been edited in several days, please remove this template.
If you are actively editing this article or section, you can replace this template with {{in use|5 minutes}}. This article was last edited by Fowler&fowler (talk | contribs) 0 seconds ago. (Update timer)
India contains a wide array of musical practices. Indian classical music has Vedic origins, and split in the 13th century into the two main traditions of Hindustani and Carnatic music. Hindustani is associated with North India and more improvisational, featuring instruments such as the sitar and tabla, and Carnatic is South Indian and more focused on written compositions such as the kriti, while both styles contain common elements such as the raga melodic framework.[419]
For other uses, see Theatre (disambiguation). "Theatrical" redirects here. For the racehorse, see Theatrical (horse).




Clockwise, from left to right:
Sarah Bernhardt in 1899 as Hamlet in Shakespeare's eponymous tragedy
The character Sun Wukong at the Peking opera from Journey to the West
Eduardo De Filippo as Pulcinella, a character from the Commedia dell'arte
Koothu, an ancient Indian form of performing art that originated in early Tamilakam
Part of a series on
Performing arts

AcrobaticsBalletCircus skillsClownDanceGymnasticsMagicMimeMusicOperaProfessional wrestlingPuppetrySpeechStand-up comedyStreet performanceTheatreVentriloquism
vte
Theatre or theater[a] is a collaborative form of performing art that uses live performers, usually actors, to present experiences of a real or imagined event before a live audience in a specific place, often a stage. The performers may communicate this experience to the audience through combinations of gesture, speech, song, music, and dance. It is the oldest form of drama, though live theatre has now been joined by modern recorded forms. Elements of art, such as painted scenery and stagecraft such as lighting are used to enhance the physicality, presence and immediacy of the experience.[1] Places, normally buildings, where performances regularly take place are also called "theatres" (or "theaters"), as derived from the Ancient Greek θέατρον (théatron, "a place for viewing"), itself from θεάομαι (theáomai, "to see", "to watch", "to observe").

Modern Western theatre comes, in large measure, from the theatre of ancient Greece, from which it borrows technical terminology, classification into genres, and many of its themes, stock characters, and plot elements. Theatre artist Patrice Pavis defines theatricality, theatrical language, stage writing and the specificity of theatre as synonymous expressions that differentiate theatre from the other performing arts, literature and the arts in general.[2][b]

A theatre company is an organisation that produces theatrical performances,[3] as distinct from a theatre troupe (or acting company), which is a group of theatrical performers working together.[4][5]

Modern theatre includes performances of plays and musical theatre. The art forms of ballet and opera are also theatre and use many conventions such as acting, costumes and staging. They were influential in the development of musical theatre.

History of theatre
Main article: History of theatre
Classical, Hellenistic Greece and Magna Graecia
Main article: Theatre of ancient Greece

The best-preserved example of a classical Greek theatre, the Ancient Theatre of Epidaurus, has a circular orchêstra and probably gives the best idea of the original shape of the Athenian theatre, though it dates from the 4th century BC.[6]

Greek Theater of Taormina, Sicily, Magna Graecia, present-day Italy
The city-state of Athens is where Western theatre originated.[7][8][9][c] It was part of a broader culture of theatricality and performance in classical Greece that included festivals, religious rituals, politics, law, athletics and gymnastics, music, poetry, weddings, funerals, and symposia.[10][9][11][12][d]

Participation in the city-state's many festivals—and mandatory attendance at the City Dionysia as an audience member (or even as a participant in the theatrical productions) in particular—was an important part of citizenship.[14] Civic participation also involved the evaluation of the rhetoric of orators evidenced in performances in the law-court or political assembly, both of which were understood as analogous to the theatre and increasingly came to absorb its dramatic vocabulary.[15][16] The Greeks also developed the concepts of dramatic criticism and theatre architecture.[17][18][19][failed verification] Actors were either amateur or at best semi-professional.[20] The theatre of ancient Greece consisted of three types of drama: tragedy, comedy, and the satyr play.[21]

The origins of theatre in ancient Greece, according to Aristotle (384–322 BCE), the first theoretician of theatre, are to be found in the festivals that honoured Dionysus. The performances were given in semi-circular auditoria cut into hillsides, capable of seating 10,000–20,000 people. The stage consisted of a dancing floor (orchestra), dressing room and scene-building area (skene). Since the words were the most important part, good acoustics and clear delivery were paramount. The actors (always men) wore masks appropriate to the characters they represented, and each might play several parts.[22]

Athenian tragedy—the oldest surviving form of tragedy—is a type of dance-drama that formed an important part of the theatrical culture of the city-state.[7][8][9][23][24][e] Having emerged sometime during the 6th century BCE, it flowered during the 5th century BCE (from the end of which it began to spread throughout the Greek world), and continued to be popular until the beginning of the Hellenistic period.[26][27][8][f]

No tragedies from the 6th century BCE and only 32 of the more than a thousand that were performed in during the 5th century BCE have survived.[29][30][g] We have complete texts extant by Aeschylus, Sophocles, and Euripides.[31][h] The origins of tragedy remain obscure, though by the 5th century BCE it was institutionalized in competitions (agon) held as part of festivities celebrating Dionysus (the god of wine and fertility).[32][33] As contestants in the City Dionysia's competition (the most prestigious of the festivals to stage drama) playwrights were required to present a tetralogy of plays (though the individual works were not necessarily connected by story or theme), which usually consisted of three tragedies and one satyr play.[34][35][i] The performance of tragedies at the City Dionysia may have begun as early as 534 BCE; official records (didaskaliai) begin from 501 BCE, when the satyr play was introduced.[36][34][j]

Most Athenian tragedies dramatize events from Greek mythology, though The Persians—which stages the Persian response to news of their military defeat at the Battle of Salamis in 480 BCE—is the notable exception in the surviving drama.[34][k] When Aeschylus won first prize for it at the City Dionysia in 472 BCE, he had been writing tragedies for more than 25 years, yet its tragic treatment of recent history is the earliest example of drama to survive.[34][38] More than 130 years later, the philosopher Aristotle analysed 5th-century Athenian tragedy in the oldest surviving work of dramatic theory—his Poetics (c. 335 BCE).

Athenian comedy is conventionally divided into three periods, "Old Comedy", "Middle Comedy", and "New Comedy". Old Comedy survives today largely in the form of the eleven surviving plays of Aristophanes, while Middle Comedy is largely lost (preserved only in relatively short fragments in authors such as Athenaeus of Naucratis). New Comedy is known primarily from the substantial papyrus fragments of Menander. Aristotle defined comedy as a representation of laughable people that involves some kind of blunder or ugliness that does not cause pain or disaster.[l]

In addition to the categories of comedy and tragedy at the City Dionysia, the festival also included the Satyr Play. Finding its origins in rural, agricultural rituals dedicated to Dionysus, the satyr play eventually found its way to Athens in its most well-known form. Satyr's themselves were tied to the god Dionysus as his loyal woodland companions, often engaging in drunken revelry and mischief at his side. The satyr play itself was classified as tragicomedy, erring on the side of the more modern burlesque traditions of the early twentieth century. The plotlines of the plays were typically concerned with the dealings of the pantheon of Gods and their involvement in human affairs, backed by the chorus of Satyrs. However, according to Webster, satyr actors did not always perform typical satyr actions and would break from the acting traditions assigned to the character type of a mythical forest creature.[39]

The Greek colonists in Southern Italy, the so-called Magna Graecia, brought theatrical art from their motherland.[40] The Greek Theatre of Syracuse, the Greek Theatre of Segesta [it], the Greek Theatre of Tindari [it], the Greek Theatre of Hippana [it], the Greek Theatre of Akrai [it], the Greek Theatre of Monte Jato [it], the Greek Theatre of Morgantina [it] and the most famous Greek Theater of Taormina, amply demonstrate this. Only fragments of original dramaturgical works are left, but the tragedies of the three great giants Aeschylus, Sophocles and Euripides and the comedies of Aristophanes are known.[41] Some famous playwrights in the Greek language came directly from Magna Graecia. Others, such as Aeschylus and Epicharmus, worked for a long time in Sicily. Epicharmus can be considered Syracusan in all respects, having worked all his life with the tyrants of Syracuse. His comedy preceded that of the more famous Aristophanes by staging the gods for the first time in comedy. While Aeschylus, after a long stay in the Sicilian colonies, died in Sicily in the colony of Gela in 456 BC. Epicarmus and Phormis, both of 6th century BC, are the basis, for Aristotle, of the invention of the Greek comedy, as he says in his book on Poetics:[42]

As for the composition of the stories (Epicharmus and Phormis) it came in the beginning from Sicily

— Aristotle, Poetics
Other native dramatic authors of Magna Graecia, in addition to the Syracusan Formides mentioned, are Achaeus of Syracuse, Apollodorus of Gela, Philemon of Syracuse and his son Philemon the younger. From Calabria, precisely from the colony of Thurii, came the playwright Alexis. While Rhinthon, although Sicilian from Syracuse, worked almost exclusively for the colony of Taranto in Apulia.[43]

Roman theatre
Main article: Theatre of ancient Rome

Roman theatre of Benevento

Roman mosaic depicting actors and an aulos player (House of the Tragic Poet, Pompeii).
Western theatre developed and expanded considerably under the Romans. The Roman historian Livy wrote that the Romans first experienced theatre in the 4th century BC, with a performance by Etruscan actors.[44] Beacham argues that Romans had been familiar with "pre-theatrical practices" for some time before that recorded contact.[45] The theatre of ancient Rome was a thriving and diverse art form, ranging from festival performances of street theatre, nude dancing, and acrobatics, to the staging of Plautus's broadly appealing situation comedies, to the high-style, verbally elaborate tragedies of Seneca. Although Rome had a native tradition of performance, the Hellenization of Roman culture in the 3rd century BC had a profound and energizing effect on Roman theatre and encouraged the development of Latin literature of the highest quality for the stage.

Following the expansion of the Roman Republic (509–27 BC) into several Greek territories between 270 and 240 BC, Rome encountered Greek drama.[46] From the later years of the republic and by means of the Roman Empire (27 BC-476 AD), theatre spread west across Europe, around the Mediterranean and reached England; Roman theatre was more varied, extensive and sophisticated than that of any culture before it.[47] While Greek drama continued to be performed throughout the Roman period, the year 240 BC marks the beginning of regular Roman drama.[46][m] From the beginning of the empire, however, interest in full-length drama declined in favour of a broader variety of theatrical entertainments.[48]

The first important works of Roman literature were the tragedies and comedies that Livius Andronicus wrote from 240 BC.[49] Five years later, Gnaeus Naevius also began to write drama.[49] No plays from either writer have survived. While both dramatists composed in both genres, Andronicus was most appreciated for his tragedies and Naevius for his comedies; their successors tended to specialise in one or the other, which led to a separation of the subsequent development of each type of drama.[49] By the beginning of the 2nd century BC, drama was firmly established in Rome and a guild of writers (collegium poetarum) had been formed.[50]

The Roman comedies that have survived are all fabula palliata (comedies based on Greek subjects) and come from two dramatists: Titus Maccius Plautus (Plautus) and Publius Terentius Afer (Terence).[51] In re-working the Greek originals, the Roman comic dramatists abolished the role of the chorus in dividing the drama into episodes and introduced musical accompaniment to its dialogue (between one-third of the dialogue in the comedies of Plautus and two-thirds in those of Terence).[52] The action of all scenes is set in the exterior location of a street and its complications often follow from eavesdropping.[52] Plautus, the more popular of the two, wrote between 205 and 184 BC and twenty of his comedies survive, of which his farces are best known; he was admired for the wit of his dialogue and his use of a variety of poetic meters.[53] All of the six comedies that Terence wrote between 166 and 160 BC have survived; the complexity of his plots, in which he often combined several Greek originals, was sometimes denounced, but his double-plots enabled a sophisticated presentation of contrasting human behaviour.[53]

No early Roman tragedy survives, though it was highly regarded in its day; historians know of three early tragedians—Quintus Ennius, Marcus Pacuvius and Lucius Accius.[52] From the time of the empire, the work of two tragedians survives—one is an unknown author, while the other is the Stoic philosopher Seneca.[54] Nine of Seneca's tragedies survive, all of which are fabula crepidata (tragedies adapted from Greek originals); his Phaedra, for example, was based on Euripides' Hippolytus.[55] Historians do not know who wrote the only extant example of the fabula praetexta (tragedies based on Roman subjects), Octavia, but in former times it was mistakenly attributed to Seneca due to his appearance as a character in the tragedy.[54]

In contrast to Ancient Greek theatre, the theatre in Ancient Rome did allow female performers. While the majority were employed for dancing and singing, a minority of actresses are known to have performed speaking roles, and there were actresses who achieved wealth, fame and recognition for their art, such as Eucharis, Dionysia, Galeria Copiola and Fabia Arete: they also formed their own acting guild, the Sociae Mimae, which was evidently quite wealthy.[56]
A film[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and some times using other sensory stimulations.[1]

Films are produced by recording actual people and objects with cameras or by creating them using animation techniques and special effects. They comprise a series of individual frames, but when these images are shown rapidly in succession, the illusion of motion is given to the viewer. Flickering between frames is not seen due to an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion; a psychological effect identified as beta movement.

Films are considered by many to be an important art form; films entertain, educate, enlighten and inspire audiences. The visual elements of cinema need no translation, giving the motion picture a universal power of communication. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the dialogue. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them.

History
Main articles: History of film technology and History of film
Precursors
Main article: Precursors of film
The art of film has drawn on several earlier traditions in fields such as oral storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving or projected images such as shadowgraphy, camera obscura, shadow puppetry and magic lantern.[2][3][4]

1830s–1880s: Before celluloid

Animated GIF of Prof. Stampfer's Stroboscopische Scheibe No. X (Trentsensky & Vieweg 1833)
The stroboscopic animation principle was introduced in 1833 with the stroboscopic disc (better known as the phénakisticope) and later applied in the zoetrope (since 1866), the flip book (since 1868), and the praxinoscope (since 1877), before it became the basic principle for cinematography.[citation needed]

Experiments with early phénakisticope-based animation projectors were made at least as early as 1843 and publicly screened in 1847. Jules Duboscq marketed phénakisticope projection systems in France from c. 1853 until the 1890s.[citation needed]

Photography was introduced in 1839, but initially photographic emulsions needed such long exposures that the recording of moving subjects seemed impossible. At least as early as 1844, photographic series of subjects posed in different positions were created to either suggest a motion sequence or document a range of different viewing angles. The advent of stereoscopic photography, with early experiments in the 1840s and commercial success since the early 1850s, raised interest in completing the photographic medium with the addition of means to capture color and motion. In 1849, Joseph Plateau published about the idea to combine his invention of the phénakisticope with the stereoscope, as suggested to him by stereoscope inventor Charles Wheatstone, and to use photographs of plaster sculptures in different positions to be animated in the combined device. In 1852, Jules Duboscq patented such an instrument as the "Stéréoscope-fantascope, ou Bïoscope", but he only marketed it very briefly, without success. One Bïoscope disc with stereoscopic photographs of a machine is in the Plateau collection of Ghent University, but no instruments or other discs have yet been found.[citation needed]


An animation of the retouched Sallie Garner card from The Horse in Motion series (1878–1879) by Muybridge
By the late 1850s, the first examples of instantaneous photography came about and provided hope that motion photography would soon be possible, but it took a few decades before it was successfully combined with a method to record series of sequential images in real-time. In 1878, Eadweard Muybridge eventually managed to take a series of photographs of a running horse with a battery of cameras in a line along the track and published the results as The Horse in Motion on cabinet cards. Muybridge, as well as Étienne-Jules Marey, Ottomar Anschütz and many others, would create many more chronophotography studies. Muybridge had the contours of dozens of his chronophotographic series traced onto glass discs and projected them with his zoopraxiscope in his lectures from 1880 to 1895.[citation needed]


An Anschütz electrotachyscope American Scientific, 16/11/1889, p. 303
Anschütz made his first instantaneous photographs in 1881. He developed a portable camera that allowed shutter speeds as short as 1/1000 of a second in 1882. The quality of his pictures was generally regarded as much higher than that of the chronophotography works of Muybridge and Étienne-Jules Marey.[5] In 1886, Anschütz developed the Electrotachyscope, an early device that displayed short motion picture loops with 24 glass plate photographs on a 1.5 meter wide rotating wheel that was hand-cranked to a speed of circa 30 frames per second. Different versions were shown at many international exhibitions, fairs, conventions, and arcades from 1887 until at least 1894. Starting in 1891, some 152 examples of a coin-operated peep-box Electrotachyscope model were manufactured by Siemens & Halske in Berlin and sold internationally.[5][6] Nearly 34,000 people paid to see it at the Berlin Exhibition Park in the summer of 1892. Others saw it in London or at the 1893 Chicago World's Fair. On 25 November 1894, Anschütz introduced a Electrotachyscope projector with a 6x8 meter screening in Berlin. Between 22 February and 30 March 1895, a total of circa 7,000 paying customers came to view a 1.5-hour show of some 40 scenes at a 300-seat hall in the old Reichstag building in Berlin.[7]

Duration: 4 minutes and 59 seconds.4:59Subtitles available.CC
Pauvre Pierrot (1892) repainted clip
Émile Reynaud already mentioned the possibility of projecting images of the Praxinoscope in his 1877 patent application. He presented a praxinoscope projection device at the Société française de photographie on 4 June 1880, but did not market his praxinoscope a projection before 1882. He then further developed the device into the Théâtre Optique which could project longer sequences with separate backgrounds, patented in 1888. He created several movies for the machine by painting images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Musée Grévin in Paris.[citation needed]

1880s–1890s: First motion pictures

A frame from Roundhay Garden Scene, the world's earliest surviving film produced using a motion picture camera, by Louis Le Prince, 1888

A frame from L'Arrivée d'un train en gare de La Ciotat, one of earliest films released in theaters, 1896
By the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph a rapid sequence of images using only one lens, allowed action to be captured and stored on a single compact reel of film.[citation needed]

Movies were initially shown publicly to one person at a time through "peep show" devices such as the Electrotachyscope, Kinetoscope and the Mutoscope. Not much later, exhibitors managed to project films on large screens for theatre audiences.[citation needed]

The first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons (using films produced by their Eidoloscope company),[8] by the Skladanowsky brothers, and by French brothers Auguste and Louis Lumière, best known for L'Arrivée d'un train en gare de La Ciotat (1896),[9] with ten of their own productions.[10] Private screenings had preceded these by several months, with Latham's slightly predating the others'.[citation needed]

1910s: Early evolution
The earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Typical films showed employees leaving a factory gate, people walking in the street, and the view from the front of a trolley as it traveled a city's Main Street. According to legend, when a film showed a locomotive at high speed approaching the audience, the audience panicked and ran from the theater. Around the turn of the 20th century, films started stringing several scenes together to tell a story. (The filmmakers who first put several shots or scenes discovered that, when one shot follows another, that act establishes a relationship between the content in the separate shots in the minds of the viewer. It is this relationship that makes all film storytelling possible. In a simple example, if a person is shown looking out a window, whatever the next shot shows, it will be regarded as the view the person was seeing.) Each scene was a single stationary shot with the action occurring before it. The scenes were later broken up into multiple shots photographed from different distances and angles.[citation needed]

Duration: 1 minute and 48 seconds.1:48
A clip from the Charlie Chaplin silent film The Bond (1918)
Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions.[citation needed]

The rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium.[citation needed]

1920s–1960s: Evolution in sound
In the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen.[citation needed] The resulting sound films were initially distinguished from the usual silent "moving pictures" or "movies" by calling them "talking pictures" or "talkies."[11] The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as "the old medium."[citation needed]

Duration: 1 hour, 28 minutes and 57 seconds.1:28:57
Sound in cinema started gaining acceptance with movies like The Jazz Singer (1927)
The evolution of sound in cinema began with the idea of combining moving images with existing phonograph sound technology. Early sound-film systems, such as Thomas Edison's Kinetoscope and the Vitaphone used by Warner Bros., laid the groundwork for synchronized sound in film. The Vitaphone system, produced alongside Bell Telephone Company and Western Electric, faced initial resistance due to expensive equipping costs, but sound in cinema gained acceptance with movies like Don Juan (1926) and The Jazz Singer (1927).[12][13]

American film studios, while Europe standardized on Tobis-Klangfilm and Tri-Ergon systems. This new technology allowed for greater fluidity in film, giving rise to more complex and epic movies like King Kong (1933).[14]

As the television threat emerged in the 1940s and 1950s, the film industry needed to innovate to attract audiences. In terms of sound technology, this meant the development of surround sound and more sophisticated audio systems, such as Cinerama's seven-channel system. However, these advances required a large number of personnel to operate the equipment and maintain the sound experience in theaters.[14]

In 1966, Dolby Laboratories introduced the Dolby A noise reduction system, which became a standard in the recording industry and eliminated the hissing sound associated with earlier standardization efforts. Dolby Stereo, a revolutionary surround sound system, followed and allowed cinema designers to take acoustics into consideration when designing theaters. This innovation enabled audiences in smaller venues to enjoy comparable audio experiences to those in larger city theaters.[15]

Today, the future of sound in film remains uncertain, with potential influences from artificial intelligence, remastered audio, and personal viewing experiences shaping its development.[16][17] However, it is clear that the evolution of sound in cinema has been marked by continuous innovation and a desire to create more immersive and engaging experiences for audiences.[citation needed]

1930s: Evolution in color
A significant technological advancement in film was the introduction of "natural color," where color was captured directly from nature through photography, as opposed to being manually added to black-and-white prints using techniques like hand-coloring or stencil-coloring.[18][19] Early color processes often produced colors that appeared far from "natural".[20] Unlike the rapid transition from silent films to sound films, color's replacement of black-and-white happened more gradually.[21]

The crucial innovation was the three-strip version of the Technicolor process, first used in animated cartoons in 1932.[22][23] The process was later applied to live-action short films, specific sequences in feature films, and finally, for an entire feature film, Becky Sharp, in 1935.[24] Although the process was expensive, the positive public response, as evidenced by increased box office revenue, generally justified the additional cost.[18] Consequently, the number of films made in color gradually increased year after year.[25][26] One of the first mainstream films to use color was The Wizard of Oz (1939).[27][28]

1950s: growing influence of television
Duration: 1 hour, 35 minutes and 53 seconds.1:35:53Subtitles available.CC
Night of the Living Dead (1968) has been listed as one of the most influential horror film ever made.[29]
In the early 1950s, black-and-white television started receiving criticism with many believing that television failed to reach the lofty intellectual and cultural expectations that accompanied its introduction.[30] In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color.[31]

During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of "star" filmmakers such as Peter Bogdanovich, Martin Scorsese and Alfred Hitchcock with his film Psycho (1960).[32]

1960s–present: Modern cinema

Salah Zulfikar, one of the most popular actors in the golden age of Egyptian Cinema.[citation needed]
The decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, New German Cinema wave, Indian New Wave, Japanese New Wave, New Hollywood, and Egyptian New Wave) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century.[citation needed]

Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and that became briefly popular in the early 2010s with films like Avatar (2009).[33] Large-screen cinemas systems using 35mm and 70mm film were developed in the late 2010s, with companies like the IMAX corporation.[34]

Film theory
16 mm spring-wound Bolex H16 Reflex camera
This 16 mm spring-wound Bolex "H16" Reflex camera is a popular entry level camera used in film schools.
"Film theory" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's manifest The Birth of the Sixth Art. The Moscow Film School, the oldest film school in the world, was founded in 1919, in order to teach about and research film theory. Formalist film theory, led by Rudolf Arnheim, Béla Balázs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. André Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life.[citation needed]

Language
Film is considered to have its own language.[citation needed] James Monaco wrote a classic text on film theory, titled "How to Read a Film," that addresses this.[35] Director Ingmar Bergman famously said, "Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream."[36] An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation.[citation needed] This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing.[citation needed] The "Hollywood style" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition.[citation needed]

Montage
Main article: Montage (filmmaking)
Montage is a film editing technique in which separate pieces of film are selected, edited, and assembled to create a new section or sequence within a film. This technique can be used to convey a narrative or to create an emotional or intellectual effect by juxtaposing different shots, often for the purpose of condensing time, space, or information. Montage can involve flashbacks, parallel action, or the interplay of various visual elements to enhance the storytelling or create symbolic meaning.[37]

The concept of montage emerged in the 1920s, with pioneering Soviet filmmakers such as Sergei Eisenstein and Lev Kuleshov developing the theory of montage. Eisenstein's film Battleship Potemkin (1925) is a prime example of the innovative use of montage, where he employed complex juxtapositions of images to create a visceral impact on the audience.[38]

As the art of montage evolved, filmmakers began incorporating musical and visual counterpoint to create a more dynamic and engaging experience for the viewer. The development of scene construction through mise-en-scène, editing, and special effects led to more sophisticated techniques that can be compared to those utilized in opera and ballet.[39]

The French New Wave movement of the late 1950s and 1960s also embraced the montage technique, with filmmakers such as Jean-Luc Godard and François Truffaut using montage to create distinctive and innovative films. This approach continues to be influential in contemporary cinema, with directors employing montage to create memorable sequences in their films.[40]

In contemporary cinema, montage continues to play an essential role in shaping narratives and creating emotional resonance. Filmmakers have adapted the traditional montage technique to suit the evolving aesthetics and storytelling styles of modern cinema.
istory is the systematic study of the past, focusing primarily on the human past. As an academic discipline, it analyses and interprets evidence to construct narratives about what happened and explain why it happened. Some theorists categorize history as a social science, while others see it as part of the humanities or consider it a hybrid discipline. Similar debates surround the purpose of history—for example, whether its main aim is theoretical, to uncover the truth, or practical, to learn lessons from the past. In a more general sense, the term history refers not to an academic field but to the past itself, times in the past, or to individual texts about the past.

Historical research relies on primary and secondary sources to reconstruct past events and validate interpretations. Source criticism is used to evaluate these sources, assessing their authenticity, content, and reliability. Historians strive to integrate the perspectives of several sources to develop a coherent narrative. Different schools of thought, such as positivism, the Annales school, Marxism, and postmodernism, have distinct methodological approaches.

History is a broad discipline encompassing many branches. Some focus on specific time periods, such as ancient history, while others concentrate on particular geographic regions, such as the history of Africa. Thematic categorizations include political history, military history, social history, and economic history. Branches associated with specific research methods and sources include quantitative history, comparative history, and oral history.

History emerged as a field of inquiry in antiquity to replace myth-infused narratives, with influential early traditions originating in Greece, China, and later in the Islamic world. Historical writing evolved throughout the ages and became increasingly professional, particularly during the 19th century, when a rigorous methodology and various academic institutions were established. History is related to many fields, including historiography, philosophy, education, and politics.

Definition
As an academic discipline, history is the study of the past with the main focus on the human past.[1] It conceptualizes and describes what happened by collecting and analysing evidence to construct narratives. These narratives cover not only how events developed over time but also why they happened and in which contexts, providing an explanation of relevant background conditions and causal mechanisms. History further examines the meaning of historical events and the underlying human motives driving them.[2]

In a slightly different sense, history refers to the past events themselves. Under this interpretation, history is what happened rather than the academic field studying what happened. When used as a countable noun, a history is a representation of the past in the form of a history text. History texts are cultural products involving active interpretation and reconstruction. The narratives presented in them can change as historians discover new evidence or reinterpret already-known sources. The past itself, by contrast, is static and unchangeable.[3] Some historians focus on the interpretative and explanatory aspects to distinguish histories from chronicles, arguing that chronicles only catalogue events in chronological order, whereas histories aim at a comprehensive understanding of their causes, contexts, and consequences.[4][a]

History has been primarily concerned with written documents. It focused on recorded history since the invention of writing, leaving prehistory[b] to other fields, such as archaeology.[7] Its scope broadened in the 20th century as historians became interested in the human past before the invention of writing.[8][c]

Historians debate whether history is a social science or forms part of the humanities. Like social scientists, historians formulate hypotheses, gather objective evidence, and present arguments based on this evidence. At the same time, history aligns closely with the humanities because of its reliance on subjective aspects associated with interpretation, storytelling, human experience, and cultural heritage.[10] Some historians strongly support one or the other classification while others characterize history as a hybrid discipline that does not belong to one category at the exclusion of the other.[11] History contrasts with pseudohistory, a label used to describe practices that deviate from historiographical standards by relying on disputed historical evidence, selectively ignoring genuine evidence, or using other means to distort the historical record. Often motivated by specific ideological agendas, pseudohistorical practices mimic historical methodology to promote biased, misleading narratives that lack rigorous analysis and scholarly consensus.[12]

Purpose
Various suggestions about the purpose or value of history have been made. Some historians propose that its primary function is the pure discovery of truth about the past. This view emphasizes that the disinterested pursuit of truth is an end in itself, while external purposes, associated with ideology or politics, threaten to undermine the accuracy of historical research by distorting the past. In this role, history also challenges traditional myths lacking factual support.[13][d]

A different perspective suggests that the main value of history lies in the lessons it teaches for the present. This view is based on the idea that an understanding of the past can guide decision-making, for example, to avoid repeating previous mistakes.[15] A related perspective focuses on a general understanding of the human condition, making people aware of the diversity of human behaviour across different contexts—similar to what one can learn by visiting foreign countries.[16] History can also foster social cohesion by providing people with a collective identity through a shared past, helping to preserve and cultivate cultural heritage and values across generations.[17] For some scholars, including Whig historians and the Marxist scholar E. H. Carr, history is a key to understanding the present[18] and, in Carr's case, shaping the future.[19]

History has sometimes been used for political or ideological purposes, for instance, to justify the status quo by emphasising the respectability of certain traditions or to promote change by highlighting past injustices.[20] In extreme forms, evidence is intentionally ignored or misinterpreted to construct misleading narratives, which can result in pseudohistory or historical denialism.[12][e] Influential examples are Holocaust denial, Armenian genocide denial, Nanjing Massacre denial, and Holodomor denial.[22]

Etymology
Photo of a damaged text written in black ink
Fragment of the Histories by Herodotus, an Ancient Greek historical text[23]
The word history comes from the Ancient Greek term ἵστωρ (histōr), meaning 'learned, wise man'. It gave rise to the Ancient Greek word ἱστορία (historiā), which had a wide meaning associated with inquiry in general and giving testimony. The term was later adopted into Classical Latin as historia. In Hellenistic and Roman times, the meaning of the term shifted, placing more emphasis on narrative aspects and the art of presentation rather than focusing on investigation and testimony.[24]

The word entered Middle English in the 14th century via the Old French term histoire.[25] At this time, it meant 'story, tale', encompassing both factual and fictional narratives. In the 15th century, its meaning shifted to cover the branch of knowledge studying the past in addition to narratives about the past.[26] In the 18th and 19th centuries, the word history became more closely associated with factual accounts and evidence-based inquiry, coinciding with the professionalization of historical inquiry, a meaning still dominant in contemporary usage.[27] The dual meaning, referring to both mere stories and factual accounts of the past, is present in the terms for history in many other European languages. They include the French histoire, the Italian storia, and the German Geschichte.[28]

Methods
Main article: Historical method
The historical method is a set of techniques historians use to research and interpret the past, covering the processes of collecting, evaluating, and synthesizing evidence.[f] It seeks to ensure scholarly rigour, accuracy, and reliability in how historical evidence is chosen, analysed, and interpreted.[30] Historical research often starts with a research question to define the scope of the inquiry. Some research questions focus on a simple description of what happened. Others aim to explain why a particular event occurred, refute an existing theory, or confirm a new hypothesis.[31]

Sources and source criticism
To answer research questions, historians rely on various types of evidence to reconstruct the past and support their conclusions. Historical evidence is usually divided into primary and secondary sources.[32] A primary source is a source that originated during the period that is studied. Primary sources can take various forms, such as official documents, letters, diaries, eyewitness accounts, photographs, and audio or video recordings. They also include historical remains examined in archaeology, geology, and the medical sciences, such as artefacts and fossils unearthed from excavations. Primary sources offer the most direct evidence of historical events.[33]

Photo of archive storage area; on the left, the hand cranks to operate shelving units; on the right, the shelves of one unit containing storage boxes
Archives preserve large quantities of original sources for researchers to access.[34]
A secondary source is a source that analyses or interprets information found in other sources.[35] Whether a document is a primary or a secondary source depends not only on the document itself but also on the purpose for which it is used. For example, if a historian writes a text about slavery based on an analysis of historical documents, then the text is a secondary source on slavery and a primary source on the historian's opinion.[36][g] Consistency with available sources is one of the main standards of historical works. For instance, the discovery of new sources may lead historians to revise or dismiss previously accepted narratives.[38] To find and access primary and secondary sources, historians consult archives, libraries, and museums. Archives play a central role by preserving countless original sources and making them available to researchers in a systematic and accessible manner. Thanks to technological advances, historians increasingly rely on online resources, which offer vast digital databases with methods to search and access specific documents.[39]

Source criticism is the process of analysing and evaluating the information a source provides.[h] Typically, this process begins with external criticism, which evaluates the authenticity of a source. It addresses the questions of when and where the source was created and seeks to identify the author, understand their reasons for producing the source, and determine if it has undergone some type of modification since its creation. Additionally, the process involves distinguishing between original works, copies, and deceptive forgeries.[41]

Internal criticism evaluates the content of a source, typically beginning with the clarification of the meaning within the source. This involves disambiguating individual terms that could be misunderstood but may also require a general translation if the source is written in an unfamiliar language.[i] Once the information content of a source is understood, internal criticism is specifically interested in determining accuracy. Critics ask whether the information is reliable or misrepresents the topic and further question whether the source is comprehensive or omits important details. One way to make these assessments is to evaluate whether the author was able, in principle, to provide a faithful presentation of the studied event. Other approaches include the assessment of the influences of the author's intentions and prejudices, and cross-referencing information with other credible sources. Being aware of the inadequacies of a source helps historians decide whether and which aspects of it to trust, and how to use it to construct a narrative.[43]

Synthesis and schools of thought
The selection, analysis, and criticism of sources result in the validation of a large collection of mostly isolated statements about the past. As a next step, sometimes termed historical synthesis, historians examine how the individual pieces of evidence fit together to form part of a larger story.[j] Constructing this broader perspective is crucial for a comprehensive understanding of the topic as a whole. It is a creative aspect[k] of historical writing that reconstructs, interprets, and explains what happened by showing how different events are connected.[46] In this way, historians address not only which events occurred but also why they occurred and what consequences they had.[47] While there are no universally accepted techniques for this synthesis, historians rely on various interpretative tools and approaches in this process.[48]

Drawing of a seated man in formal dark clothes
Auguste Comte articulated positivism, advocating a science-based approach to history.[49]
One tool to provide an accessible overview of complex developments is the use of periodization, which divides a timeframe into different periods, each organized around central themes or developments that shaped the period. For example, the three-age system is traditionally used to divide early human history into Stone Age, Bronze Age, and Iron Age based on the predominant materials and technologies during these periods.[50] Another methodological tool is the examination of silences, gaps or omissions in the historical record of events that occurred but did not leave significant evidential traces. Silences can happen when contemporaries find information too obvious to document but may also occur if there are specific reasons to withhold or destroy information.[51][l] Conversely, when large datasets are available, quantitative approaches can be used. For instance, economic and social historians commonly employ statistical analysis to identify patterns and trends associated with large groups.[54]

Different schools of thought often come with their own methodological implications for how to write history.[55] Positivists emphasize the scientific nature of historical inquiry, focusing on empirical evidence to discover objective truths.[56] In contrast, postmodernists reject grand narratives that claim to offer a single, objective truth. Instead, they highlight the subjective nature of historical interpretation, which leads to a multiplicity of divergent perspectives.[57] Marxists interpret historical developments as expressions of economic forces and class struggles.[58] The Annales school highlights long-term social and economic trends while relying on quantitative and interdisciplinary methods.[59] Feminist historians study the role of gender in history, with a particular interest in analysing the experiences of women to challenge patriarchal perspectives.[60]
Science is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][page needed][2] Modern science is typically divided into two – or three – major branches:[3] the natural sciences, which study the physical world, and the social sciences, which study individuals and societies.[4][5] While referred to as the formal sciences, the study of logic, mathematics, and theoretical computer science are typically regarded as separate because they rely on deductive reasoning instead of the scientific method as their main methodology.[6][7][8][9] Meanwhile, applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12]

The history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity and later medieval scholarship, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes; while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India and Islamic Golden Age.[13]: 12 [14][15][16][13]: 163–192  The recovery and assimilation of Greek works and Islamic inquiries into Western Europe during the Renaissance revived natural philosophy,[13]: 193–224, 225–253 [17] which was later transformed by the Scientific Revolution that began in the 16th century[18] as new ideas and discoveries departed from previous Greek conceptions and traditions.[13]: 357–368 [19] The scientific method soon played a greater role in the acquisition of knowledge, and in the 19th century, many of the institutional and professional features of science began to take shape,[20][21] along with the changing of "natural philosophy" to "natural science".[22]

New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[23][24] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[25] government agencies,[13]: 163–192  and companies.[26] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.

Etymology
The word science has been used in Middle English since the 14th century in the sense of "the state of knowing". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning "knowledge, awareness, understanding", a noun derivative of sciens meaning "knowing", itself the present active participle of sciō, "to know".[27]

There are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning "to know", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io meaning "to incise". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning "to not know, be unfamiliar with", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2- from *sḱʰeh2(i)-meaning "to cut".[28]

In the past, science was a synonym for "knowledge" or "study", in keeping with its Latin origin. A person who conducted scientific research was called a "natural philosopher" or "man of science".[29] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[30] crediting it to "some ingenious gentleman" (possibly himself).[31]

History
Main article: History of science
Early history
Main article: Science in the ancient world
Clay tablet with markings, three columns for numbers and one for ordinals
The Plimpton 322 tablet by the Babylonians records Pythagorean triples, written c. 1800 BCE
Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[32][33] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[34] as did religious rituals.[35] Some scholars use the term "protoscience" to label activities in the past that resemble modern science in some but not all features;[36][37][38] however, this label has also been criticised as denigrating,[39] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[40]

Direct evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science.[13]: 12–15 [14] Although the words and concepts of "science" and "nature" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[41][13]: 12  From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system,[42] solved practical problems using geometry,[43] and developed a calendar.[44] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]: 9 

The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[45] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[46] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[45][47] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[45]

Classical antiquity
Main article: Science in classical antiquity
Framed mosaic of philosophers gathering around and conversing
Plato's Academy mosaic, made between 100 BCE and 79 CE, shows many Greek philosophers and scholars.
In classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[48] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural "way" in which a plant grows,[49] and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish "nature" and "convention".[50]

The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[51] The Pythagoreans developed a complex number philosophy[52]: 467–468  and contributed significantly to the development of mathematical science.[52]: 465  The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[53][54] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a "canon" (ruler, standard) which established physical criteria or standards of scientific truth.[55] The Greek doctor Hippocrates established the tradition of systematic medical science[56][57] and is known as "The Father of Medicine".[58]

A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly held truths that shape beliefs and scrutinises them for consistency.[59] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism.[60]

In the 4th century BCE, Aristotle created a systematic programme of teleological philosophy.[61] In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it.[62] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[62] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[63][64] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[65] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History.[66][67][68]

Positional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide.[69]

Middle Ages
Main article: History of science § Middle Ages
Picture of a peacock on very old paper
The first page of Vienna Dioscurides depicts a peacock, made in the 6th century.
Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe.[13]: 194  Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge.[70] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[13]: 159  John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus.[13]: 307, 311, 363, 402  His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[13]: 307–308 [71]

During late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[72] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were made by Christians, mainly Nestorians and Miaphysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists.[73] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world.[74]

Islamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq[75] and the flourished[76] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study.[a][78][79] Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[80]

By the 11th century most of Europe had become Christian,[13]: 204  and in 1088, the University of Bologna emerged as the first university in Europe.[81] As such, demand for Latin translation of ancient and scientific texts grew,[13]: 204  a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[82] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi.[83]

Renaissance
Main articles: Scientific Revolution and Science in the Renaissance

Drawing of the heliocentric model as proposed by the Copernicus's De revolutionibus orbium coelestiumalt=Drawing of planets' orbit around the Sun
New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[77]: Book I  A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[84]

In the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.[85]

Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[84][86] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[87] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[88]

The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[89] Francis Bacon and René Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[90] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature.[91]
Age of Enlightenment
Main article: Science in the Age of Enlightenment

Title page of the 1687 first edition of Philosophiæ Naturalis Principia Mathematica by Isaac Newton
At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica greatly influencing future physicists.[92] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[93]

During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime or pleasing [speculation]".[94]

Science during the Enlightenment was dominated by scientific societies and academies,[95] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population.[96] Enlightenment philosophers turned to a few of their scientific predecessors – Galileo, Kepler, Boyle, and Newton principally – as the guides to every physical and social field of the day.[97][98]

The 18th century saw significant advancements in the practice of medicine[99] and physics;[100] the development of biological taxonomy by Carl Linnaeus;[101] a new understanding of magnetism and electricity;[102] and the maturation of chemistry as a discipline.[103] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[104] Modern sociology largely originated from this movement.[105] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[106]

19th century
Main article: 19th century in science
Sketch of a map with captions
The first diagram of an evolutionary tree made by Charles Darwin in 1837
During the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as "biologist", "physicist", and "scientist"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals.[107] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[108]

During the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859.[109] Separately, Gregor Mendel presented his paper, "Experiments on Plant Hybridisation" in 1865,[110] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[111]

Early in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[112] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[113] This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[b]

The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[116] Marie Curie then became the first person to win two Nobel Prizes.[117] In the next year came the discovery of the first subatomic particle, the electron.[118]

20th century
Main article: 20th century in science
Graph showing lower ozone concentration at the South Pole
A computer graph of the ozone hole made in 1987 using data from a space telescope
In the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally.[119][120] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies.[121]

During this period scientific experimentation became increasingly larger in scale and funding.[122] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race.[123][124] Substantial international collaborations were also made, despite armed conflicts.[125]

In the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[126] The discovery of the cosmic microwave background in 1964[127] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lemaître.[128]

The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics.[129] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[130][131] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling.[132]

21st century
Main article: 21st century § Science and technology
The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[133] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body.[134] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[135] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[136][137] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc.[138]

Branches
Main article: Branches of science
Modern science is commonly divided into three major branches: natural science, social science, and formal science.[3] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[139] Both natural and social sciences are empirical sciences,[140] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[141]

Natural
Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[142] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings.[143] Today, "natural history" suggests observational descriptions aimed at popular audiences.[144]

Social
Two curve crossing over at a point, forming a X shape
Supply and demand curve in economics, crossing over at the optimal equilibrium
Social science is the study of human behaviour and the functioning of societies.[4][5] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[4] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology.[4] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[4]

Formal
Formal science is an area of study that generates knowledge using formal systems.[145][146][147] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[148] It includes mathematics,[149][150] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[8][151][141] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[6][152] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[153] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[154] chemistry,[155] biology,[156] finance,[157] and economics.[158]

Applied
Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[159][12] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[160] Science may contribute to the development of new technologies.[161] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[162][163]

Basic
The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[164][165]

Blue skies
Blue skies research, also called blue sky science, is scientific research in domains where "real-world" applications are not immediately apparent. It has been defined as "research without a clear goal"[166] and "curiosity-driven science". Proponents of this mode of science argue that unanticipated scientific breakthroughs are sometimes more valuable than the outcomes of agenda-driven research, heralding advances in genetics and stem cell biology as examples of unforeseen benefits of research that was originally seen as purely theoretical in scope. Because of the inherently uncertain return on investment, blue-sky projects are sometimes politically and commercially unpopular and tend to lose funding to research perceived as being more reliably profitable or practical.[167]
Research is creative and systematic work undertaken to increase the stock of knowledge.[1] It involves the collection, organization, and analysis of evidence to increase understanding of a topic, characterized by a particular attentiveness to controlling sources of bias and error. These activities are characterized by accounting and controlling for biases. A research project may be an expansion of past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole.

The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, and the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life, technological, etc. The scientific study of research practices is known as meta-research.

A researcher is a person who conducts research, especially in order to discover new information or to reach a new understanding.[2] In order to be a social researcher or a social scientist, one should have enormous knowledge of subjects related to social science that they are specialized in. Similarly, in order to be a natural science researcher, the person should have knowledge of fields related to natural science (physics, chemistry, biology, astronomy, zoology and so on). Professional associations provide one pathway to mature in the research profession.[3]

Etymology

Aristotle, (384–322 BC), an Ancient Greek philosopher and pioneer in developing the scientific method[4]
The word research is derived from the Middle French "recherche", which means "to go about seeking", the term itself being derived from the Old French term "recerchier", a compound word from "re-" + "cerchier", or "sercher", meaning 'search'.[5] The earliest recorded use of the term was in 1577.[5]

Definitions
Research has been defined in a number of different ways, and while there are similarities, there does not appear to be a single, all-encompassing definition that is embraced by all who engage in it.

Research, in its simplest terms, is searching for knowledge and searching for truth. In a formal sense, it is a systematic study of a problem attacked by a deliberately chosen strategy, which starts with choosing an approach to preparing a blueprint (design) and acting upon it in terms of designing research hypotheses, choosing methods and techniques, selecting or developing data collection tools, processing the data, interpretation, and ending with presenting solution(s) of the problem.[6]

Another definition of research is given by John W. Creswell, who states that "research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue". It consists of three steps: pose a question, collect data to answer the question, and present an answer to the question.[7][page needed]

The Merriam-Webster Online Dictionary defines research more generally to also include studying already existing knowledge: "studious inquiry or examination; especially: investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws".[5]

Forms of research
Original research
"Original research" redirects here. For the Wikipedia prohibition against user-generated, unpublished research, see Wikipedia:No original research.
Original research, also called primary research, is research that is not exclusively based on a summary, review, or synthesis of earlier publications on the subject of research. This material is of a primary-source character. The purpose of the original research is to produce new knowledge rather than present the existing knowledge in a new form (e.g., summarized or classified).[8][9] Original research can take various forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.[10]

The degree of originality of the research is among the major criteria for articles to be published in academic journals and usually established by means of peer review.[11] Graduate students are commonly required to perform original research as part of a dissertation.[12]

Scientific research
Main article: Scientific method

This section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these messages)
This section needs additional citations for verification. (July 2025)
This section may require cleanup to meet Wikipedia's quality standards. The specific problem is: unsourced and vague. (July 2025)

Primary scientific research being carried out at the Microscopy Laboratory at the Idaho National Laboratory

Scientific research equipment at MIT

The German maritime research vessel Sonne
Scientific research is a systematic way of gathering data and harnessing curiosity.[citation needed] This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research may be funded by public authorities, charitable organizations, and private organizations. Scientific research can be subdivided by discipline.

Generally, research is understood to follow a certain structural process. Though the order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:

Observations and formation of the topic: Consists of the subject area of one's interest and following that subject area to conduct subject-related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.
Hypothesis: A testable prediction which designates the relationship between two or more variables.
Conceptual definition: Description of a concept by relating it to other concepts.
Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study.
Gathering of data: Consists of identifying a population and selecting samples, gathering information from or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.
Analysis of data: Involves breaking down the individual pieces of data to draw conclusions about it.
Data Interpretation: This can be represented through tables, figures, and pictures, and then described in words.
Test, revising of hypothesis
Conclusion, reiteration if necessary
A common misconception is that a hypothesis will be proven (see, rather, null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.

A useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which states no relationship or difference between the independent or dependent variables.

Research in the humanities
Research in the humanities involves different methods such as for example hermeneutics and semiotics. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead, explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past. Other studies aim to merely examine the occurrence of behaviours in societies and communities, without particularly looking for reasons or motivations to explain these. These studies may be qualitative or quantitative, and can use a variety of approaches, such as queer theory or feminist theory.[13]

Artistic research
Artistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.

The controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines.[14] One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.[15]

Artistic research has been defined by the School of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner – "Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods, and criticality. Through presented documentation, the insights gained shall be placed in a context."[16] Artistic research aims to enhance knowledge and understanding with presentation of the arts.[17] A simpler understanding by Julian Klein defines artistic research as any kind of research employing the artistic mode of perception.[18] For a survey of the central problematics of today's artistic research, see Giaco Schiesser.[19]

According to artist Hakan Topal, in artistic research, "perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities".[20] Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.[21]

The Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR),[22][23] an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC),[24][25][26] a searchable, documentary database of artistic research, to which anyone can contribute.

Patricia Leavy addresses eight arts-based research (ABR) genres: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.[27]

In 2016, the European League of Institutes of the Arts launched The Florence Principles' on the Doctorate in the Arts.[28] The Florence Principles relating to the Salzburg Principles and the Salzburg Recommendations of the European University Association name seven points of attention to specify the Doctorate / PhD in the Arts compared to a scientific doctorate / PhD. The Florence Principles have been endorsed and are supported also by AEC, CILECT, CUMULUS and SAR.

Historical research
Main article: Historical method

Leopold von Ranke (1795–1886), a German historian and a founder of modern source-based history
The historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:[29]

Identification of origin date
Evidence of localization
Recognition of authorship
Analysis of data
Identification of integrity
Attribution of credibility
Documentary research
Main article: Documentary research
Steps in conducting research

Research design and evidence

Research cycle
Research is often conducted using the hourglass model structure of research.[30] The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:[31]

Identification of research problem
Literature review
Specifying the purpose of research
Determining specific research questions
Specification of a conceptual framework, sometimes including a set of hypotheses[32]
Choice of a methodology (for data collection)
Data collection
Verifying data
Analyzing and interpreting the data
Reporting and evaluating research
Communicating the research findings and, possibly, recommendations
The steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps.[33] Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study.[34] The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in rejecting or failing to reject the null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the reverse approach: starting with articulating findings and discussion of them, moving "up" to identification of a research problem that emerges in the findings and literature review. The reverse approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings have fully emerged and been interpreted.

Rudolph Rummel says, "... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results."[35]

Plato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrased in the following way, "If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!"[36]

Research methods

The research room at the New York Public Library, an example of secondary research in progress

Maurice Hilleman, a 20th-century vaccinologist credited with saving more lives than any other scientist of his era[37]
The goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):

Exploratory research, which helps to identify and define a problem or question.
Constructive research, which tests theories and proposes solutions to a problem or question.
Empirical research, which tests the feasibility of a solution using empirical evidence.
There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:

Qualitative research Qualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations.[38] In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement.[39][40]

Quantitative research Quantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).[7] Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism.

The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies.[41] Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest.[41]

If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).[citation needed] If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.[42]

In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data.[41] Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.[43]

Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common.[44] This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.[45]

Big data has brought big impacts on research methods so that now many researchers do not put much effort into data collection; furthermore, methods to analyze easily available huge amounts of data have also been developed.

Non-empirical research Non-empirical (theoretical) research is an approach that involves the development of theory as opposed to using observation and experimentation. As such, non-empirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool of existing and established knowledge. Non-empirical research is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose in science. Typically empirical research produces observations that need to be explained; then theoretical research tries to explain them, and in so doing generates empirically testable hypotheses; these hypotheses are then tested empirically, giving more observations that may need further explanation; and so on. See Scientific method.
In physics, a quantum (pl.: quanta) is the minimum amount of any physical entity (physical property) involved in an interaction. The fundamental notion that a property can be "quantized" is referred to as "the hypothesis of quantization".[1] This means that the magnitude of the physical property can take on only discrete values consisting of integer multiples of one quantum. For example, a photon is a single quantum of light of a specific frequency (or of any other form of electromagnetic radiation). Similarly, the energy of an electron bound within an atom is quantized and can exist only in certain discrete values.[2] Atoms and matter in general are stable because electrons can exist only at discrete energy levels within an atom. Quantization is one of the foundations of the much broader physics of quantum mechanics. Quantization of energy and its influence on how energy and matter interact (quantum electrodynamics) is part of the fundamental framework for understanding and describing nature.

Origin

German physicist and 1918 Nobel Prize for Physics recipient Max Planck (1858–1947)
The modern concept of the quantum in physics originates from December 14, 1900, when Max Planck reported his findings to the German Physical Society. He showed that modelling harmonic oscillators with discrete energy levels resolved a longstanding problem in the theory of blackbody radiation.[3]: 15 [4] In his report, Planck did not use the term quantum in the modern sense. Instead, he used the term Elementarquantum to refer to the "quantum of electricity", now known as the elementary charge. For the smallest unit of energy, he employed the term Energieelement, "energy element", rather than calling it a quantum.[5]

Shortly afterwards, in a paper published in Annalen der Physik,[6] Planck introduced the constant h, which he termed the "quantum of action" (elementares Wirkungsquantum) in 1906.[5] In this paper, Planck also reported more precise values for the elementary charge and the Avogadro–Loschmidt number, the number of molecules in one mole of substance.[7] The constant h is now known as the Planck constant. After his theory was validated, Planck was awarded the Nobel Prize in Physics for his discovery in 1918.[8]


In 1905 Albert Einstein suggested that electromagnetic radiation exists in spatially localized packets which he called "quanta of light" (Lichtquanta).[5][9] Einstein was able to use this hypothesis to recast Planck's treatment of the blackbody problem in a form that also explained the voltages observed in Philipp Lenard's experiments on the photoelectric effect.[3]: 23  Shortly thereafter, the term "energy quantum" was introduced for the quantity hν.[10]

Quantization
Main article: Quantization (physics)
While quantization was first discovered in electromagnetic radiation, it describes a fundamental aspect of energy not just restricted to photons.[11] In the attempt to bring theory into agreement with experiment, Max Planck postulated that electromagnetic energy is absorbed or emitted in discrete packets, or quanta.[12]
Mathematics is a field of study that discovers and organizes methods, theories, and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics).

Mathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms. Mathematics uses pure reason to prove the properties of objects through proofs, which consist of a succession of applications of deductive rules to already established results. These results, called theorems, include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.[1]

Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications.[2][3]

Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[4] Since its beginning, mathematics was primarily divided into geometry and arithmetics (the manipulation of natural numbers and fractions) until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both.[5] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[6] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics.[7][8]

Areas of mathematics
Before the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes.[9] Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.[10]

During the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus[11]—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics.[12] The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the 17th century.[13]

At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[14][6] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[8] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have "geometry" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[7]

Number theory
Main article: Number theory

This is the Ulam spiral, which illustrates the distribution of prime numbers. The dark diagonal lines in the spiral hint at the hypothesized approximate independence between being prime and being a value of a quadratic polynomial, a conjecture now known as Hardy and Littlewood's Conjecture F.
Number theory began with the manipulation of numbers, that is, natural numbers 
(
N
)
,
{\displaystyle (\mathbb {N} ),} and later expanded to integers 
(
Z
)
{\displaystyle (\mathbb {Z} )} and rational numbers 
(
Q
)
.
{\displaystyle (\mathbb {Q} ).} Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations.[15] Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria.[16] The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.[17]

Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra.[18] Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.[19]

Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), Diophantine analysis, and transcendence theory (problem oriented).[7]

Geometry
Main article: Geometry

On the surface of a sphere, Euclidean geometry only applies as a local approximation. For larger scales the sum of the angles of a triangle is not equal to 180°.
Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.[20]

A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements.[21][22]

The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space.[b][20]

Euclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically.[23]

Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions.[20]

In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem.[24][6] In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space.[25]

Today's subareas of geometry include:[7]

Projective geometry, introduced in the 16th century by Girard Desargues, extends Euclidean geometry by adding points at infinity at which parallel lines intersect. This simplifies many aspects of classical geometry by unifying the treatments for intersecting and parallel lines.
Affine geometry, the study of properties relative to parallelism and independent from the concept of length.
Differential geometry, the study of curves, surfaces, and their generalizations, which are defined using differentiable functions.
Manifold theory, the study of shapes that are not necessarily embedded in a larger space.
Riemannian geometry, the study of distance properties in curved spaces.
Algebraic geometry, the study of curves, surfaces, and their generalizations, which are defined using polynomials.
Topology, the study of properties that are kept under continuous deformations.
Algebraic topology, the use in topology of algebraic methods, mainly homological algebra.
Discrete geometry, the study of finite configurations in geometry.
Convex geometry, the study of convex sets, which takes its importance from its applications in optimization.
Complex geometry, the geometry obtained by replacing real numbers with complex numbers.
Algebra
Main article: Algebra
refer to caption
The quadratic formula, which concisely expresses the solutions of all quadratic equations
A shuffled 3x3 rubik's cube
The Rubik's Cube group is a concrete application of group theory.[26]
Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra.[27][28] Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution.[29] Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side.[30] The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise.[31][32]

Algebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers.[33] Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas.[34]

Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid.[35] The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether,[36] and popularized by Van der Waerden's book Moderne Algebra.

Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include:[7]

group theory
field theory
vector spaces, whose study is essentially the same as linear algebra
ring theory
commutative algebra, which is the study of commutative rings, includes the study of polynomials, and is a foundational part of algebraic geometry
homological algebra
Lie algebra and Lie group theory
Boolean algebra, which is widely used for the study of the logical structure of computers
The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory.[37] The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.[38]

Calculus and analysis
Main articles: Calculus and Mathematical analysis

A Cauchy sequence consists of elements such that all subsequent terms of a term become arbitrarily close to each other as the sequence progresses (from left to right).
Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz.[39] It is fundamentally the study of the relationship between variables that depend continuously on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results.[40] Presently, "calculus" refers mainly to the elementary part of this theory, and "analysis" is commonly used for advanced parts.[41]

Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include:[7]

Multivariable calculus
Functional analysis, where variables represent varying functions
Integration, measure theory and potential theory, all strongly related with probability theory on a continuum
Ordinary differential equations
Partial differential equations
Numerical analysis, mainly devoted to the computation on computers of solutions of ordinary and partial differential equations that arise in many applications
Discrete mathematics
Main article: Discrete mathematics

A diagram representing a two-state Markov chain. The states are represented by 'A' and 'E'. The numbers are the probability of flipping the state.
Discrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers.[42] Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply.[c] Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics.[43]

The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century.[44] The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems.[45]

Discrete mathematics includes:[7]

Combinatorics, the art of enumerating mathematical objects that satisfy some given constraints. Originally, these objects were elements or subsets of a given set; this has been extended to various objects, which establishes a strong link between combinatorics and other parts of discrete mathematics. For example, discrete geometry includes counting configurations of geometric shapes.
Graph theory and hypergraphs
Coding theory, including error correcting codes and a part of cryptography
Matroid theory
Discrete geometry
Discrete probability distributions
Game theory (although continuous games are also studied, most common games, such as chess and poker are discrete)
Discrete optimization, including combinatorial optimization, integer programming, constraint programming
Mathematical logic and set theory
Main articles: Mathematical logic and Set theory
A blue and pink circle and their intersection labeled
The Venn diagram is a commonly used method to illustrate the relations between sets.
The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century.[46][47] Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.[48]

Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets[49] but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory.[50] In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour.[51]

This became the foundational crisis of mathematics.[52] It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have.[14] For example, in Peano arithmetic, the natural numbers are defined by "zero is a number", "each number has a unique successor", "each number but zero has a unique predecessor", and some rules of reasoning.[53] This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910.[54]

The "nature" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called "intuition"—to guide their study and proofs. The approach allows considering "logics" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system.[55] This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic (which explicitly lacks the law of excluded middle).[56][57]

These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory.[7] Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, formal verification, program analysis, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.[58]

Statistics and other decision sciences
Main articles: Statistics and Probability theory

Whatever the form of a random population distribution (μ), the sampling mean (x̄) tends to a Gaussian distribution and its variance (σ) is given by the central limit theorem of probability theory.[59]
The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods such as, and especially, probability theory. Statisticians generate data with random sampling or randomized experiments.[60]

Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[61] Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics.[62]

Computational mathematics
Main article: Computational mathematics
Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity.[63][64] Part of computational mathematics involves numerical analysis, which is the study of methods for problems in analysis using functional analysis and approximation theory. Numerical analysis broadly includes the study of approximation and discretization, with special focus on rounding errors.[65] Numerical analysis and, more broadly, scientific computing, also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.[7]

History
Main article: History of mathematics
Etymology
The word mathematics comes from the Ancient Greek word máthēma (μάθημα), meaning 'something learned, knowledge, mathematics', and the derived expression mathēmatikḗ tékhnē (μαθηματικὴ τέχνη), meaning 'mathematical science'. It entered the English language during the Late Middle English period through French and Latin.[66]

Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant "learners" rather than "mathematicians" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established.[67]

In Latin and English, until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning "astrologers", is sometimes mistranslated as a condemnation of mathematicians.[68]

The apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly "all things mathematical", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek.[69] In English, the noun mathematics takes a singular verb. It is often shortened to maths[70] or, in North America, math.[71]

Ancient

The Babylonian mathematical tablet Plimpton 322, dated to 1800 BC
In addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years.[72][73] Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[74] The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC.[75] Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time.[76]

In the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right.[77] Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof.[78] His book, Elements, is widely considered the most successful and influential textbook of all time.[79] The greatest mathematician of antiquity is often held to be Archimedes (c. 287 – c. 212 BC) of Syracuse.[80] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[81] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[82] trigonometry (Hipparchus of Nicaea, 2nd century BC),[83] and the beginnings of algebra (Diophantus, 3rd century AD).[84]


The numerals used in the Bakhshali manuscript, dated between the 2nd century BC and the 2nd century AD
The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics.[85] Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.[86][87]
During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system.[88] Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.[89] The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe.[90]

During the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of logarithms by John Napier in 1614, which greatly simplified numerical calculations, especially for astronomy and marine navigation, the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1643–1727) and Gottfried Leibniz (1646–1716). Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems.[91]


Carl Friedrich Gauss
Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics.[92] In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved.[55]

Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[93]

Symbolic notation and terminology
Main articles: Mathematical notation, Language of mathematics, and Glossary of mathematics

An explanation of the sigma (Σ) summation notation
Mathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas.[94] More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs,[95] such as + (plus), × (multiplication), 
∫
{\textstyle \int } (integral), = (equal), and < (less than).[96] All these symbols are generally grouped according to specific rules to form expressions and formulas.[97] Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses.

Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary.[98]

Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism.[99] Other technical terms are words of the common language that are used in an accurate meaning that may differ slightly from their common meaning. For example, in mathematics, "or" means "one, the other or both", while, in common language, it is either ambiguous or means "one or the other but not both" (in mathematics, the latter is called "exclusive or"). Finally, many mathematical terms are common words that are used with a completely different meaning.[100] This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, "every free module is flat" and "a field is always a ring".

Relationship with sciences
Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws.[101] The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model.[102] Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used.[103] For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.[104]

There is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation.[105] In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied "durch planmässiges Tattonieren" (through systematic experimentation).[106] However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence.[107][108][109][110]

Pure and applied mathematics
Main articles: Applied mathematics and Pure mathematics
Isaac Newton
Gottfried Wilhelm von Leibniz
Isaac Newton (left) and Gottfried Wilhelm Leibniz developed infinitesimal calculus.
Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics.[111] For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture, and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians.[112] However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece.[113] The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.[114]

In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics.[111][115] This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred.[116]

The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere.[117][118] Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the "pure theory".[119][120]

An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis.[121] An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high.[122] For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry.[123]

In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas.[124][125] The Mathematics Subject Classification has a section for "general applied mathematics" but does not mention "pure mathematics".[7] However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge.

Unreasonable effectiveness
The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner.[3] It is the fact that many mathematical theories (even the "purest") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced.[126] Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.

A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem.[127] A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It was almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.[128]

In the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four.[129][130]

A striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon 
Ω
−
.
{\displaystyle \Omega ^{-}.} In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments.[131][132][133]

Specific sciences
Physics
Main article: Relationship between mathematics and physics

Diagram of a pendulum
Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly,[134] and is also considered to be the motivation of major mathematical developments.[135]

Computing
Further information: Theoretical computer science and Computational mathematics
Computing is closely related to mathematics in several ways.[136] Theoretical computer science is considered to be mathematical in nature.[137] Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, and graph theory.[138] In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer.[139]

Biology and chemistry
Main articles: Mathematical and theoretical biology and Mathematical chemistry

The skin of this giant pufferfish exhibits a Turing pattern, which can be modeled by reaction–diffusion systems.
Biology uses probability extensively in fields such as ecology or neurobiology.[140] Most discussion of probability centers on the concept of evolutionary fitness.[140] Ecology heavily uses modeling to simulate population dynamics,[140][141] study ecosystems such as the predator-prey model, measure pollution diffusion,[142] or to assess climate change.[143] The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations.[144]

Statistical hypothesis testing, is run on data from clinical trials to determine whether a new treatment works.[145] Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions.[146]

Earth sciences
Main article: Geomathematics
Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes.[147] Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models.[148][149][150]

Social sciences
Further information: Mathematical economics and Historical dynamics
Areas of mathematics used in the social sciences include probability/statistics and differential equations. These are used in linguistics, economics, sociology,[151] and psychology.[152]


Supply and demand curves, like this one, are a staple of mathematical economics.
Often the fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man').[153] In this model, the individual seeks to maximize their self-interest,[153] and always makes optimal choices using perfect information.[154] This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms. Some reject or criticise the concept of Homo economicus. Economists note that real people have limited information, make poor choices, and care about fairness and altruism, not just personal gain.[155]

Without mathematical modeling, it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions that can be tested against real-world data.[156]

At the start of the 20th century, there was a development to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis.[157] Towards the end of the 19th century, mathematicians extended their analysis into geopolitics.[158] Peter Turchin developed cliodynamics in the 1990s.[159]

Mathematization of the social sciences is not without risk. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences.[160] The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models, can be subject to controversy.[161][162]
Culture (/ˈkʌltʃər/ KUL-chər) is a concept that encompasses the social behavior, institutions, and norms found in human societies, as well as the knowledge, beliefs, arts, laws, customs, capabilities, attitudes, and habits of the individuals in these groups.[1] Culture often originates from or is attributed to a specific region or location.

Humans acquire culture through the learning processes of enculturation and socialization, which is shown by the diversity of cultures across societies. A cultural norm codifies acceptable conduct in society; it serves as a guideline for behavior, dress, language, and demeanor in a situation, which serves as a template for expectations in a social group. Accepting only a monoculture in a social group can bear risks, just as a single species can wither in the face of environmental change, for lack of functional responses to the change.[2] Thus in military culture, valor is counted as a typical behavior for an individual, and duty, honor, and loyalty to the social group are counted as virtues or functional responses in the continuum of conflict. In religion, analogous attributes can be identified in a social group.

Cultural change, or repositioning, is the reconstruction of a cultural concept of a society. Cultures are internally affected by both forces encouraging change and forces resisting change. Cultures are externally affected via contact between societies. Organizations like UNESCO attempt to preserve culture and cultural heritage.

Description

Pygmy music has been polyphonic well before their discovery by non-African explorers of the Baka, Aka, Efe, and other foragers of the Central African forests, in the 1200s, which is at least 200 years before polyphony developed in Europe. Note the multiple lines of singers and dancers. The motifs are independent, with theme and variation interweaving.[3] This type of music is thought to be the first expression of polyphony in world music.
Culture is considered a central concept in anthropology, encompassing the range of phenomena that are transmitted through social learning in human societies. Cultural universals are found in all human societies. These include expressive forms like art, music, dance, ritual, religion, and technologies like tool usage, cooking, shelter, and clothing. The concept of material culture covers the physical expressions of culture, such as technology, architecture and art, whereas the immaterial aspects of culture such as principles of social organization (including practices of political organization and social institutions), mythology, philosophy, literature (both written and oral), and science comprise the intangible cultural heritage of a society.[4]

In the humanities, one sense of culture as an attribute of the individual has been the degree to which they have cultivated a particular level of sophistication in the arts, sciences, education, or manners.[5] The level of cultural sophistication has also sometimes been used to distinguish civilizations from less complex societies.[6] Such hierarchical perspectives on culture are also found in class-based distinctions between a high culture of the social elite and a low culture, popular culture, or folk culture of the lower classes, distinguished by stratified access to cultural capital.[7] In common parlance, culture is often used to refer specifically to the symbolic markers used by ethnic groups to distinguish themselves visibly from each other, such as body modification, clothing or jewelry.[8] Mass culture refers to the mass-produced and mass-mediated forms of consumer culture that emerged in the 20th century.[9] Some schools of philosophy, such as Marxism and critical theory, have argued that culture is often used politically as a tool of the elites to manipulate the proletariat and create a false consciousness.[10] Such perspectives are common in the discipline of cultural studies.[11] In the wider social sciences, the theoretical perspective of cultural materialism holds that human symbolic culture arises from the material conditions of human life, and that the basis of culture is found in evolved biological dispositions.[12]

When used as a count noun, a "culture" is the set of customs, traditions, and values of a society or community, such as an ethnic group or nation, and the knowledge acquired over time.[13] In this sense, multiculturalism values the peaceful coexistence and mutual respect between different cultures inhabiting the same planet.[14] Sometimes "culture" is also used to describe specific practices within a subgroup of a society, a subculture (e.g., "bro culture"), or a counterculture.[15] Within cultural anthropology, the ideology and analytical stance of cultural relativism hold that cultures cannot easily be objectively ranked or evaluated because any evaluation is necessarily situated within the value system of a given culture.[16]

Etymology
The modern term culture is based on a term used by the ancient Roman orator Cicero in his Tusculanae Disputationes, where he wrote of a cultivation of the soul or cultura animi,[17] using an agricultural metaphor for the development of a philosophical soul, understood teleologically as the highest possible ideal for human development. Samuel von Pufendorf took over this metaphor in a modern context, meaning something similar, but no longer assuming philosophy was humanity's natural perfection. This use, and that of many writers, "refers to all the ways in which human beings overcome their original barbarism, and through artifice, become fully human".[18]

Edward S. Casey wrote, "The very word culture meant 'place tilled' in Middle English, and the same word goes back to Latin colere, 'to inhabit, care for, till, worship' and cultus, 'A cult, especially a religious one.' To be cultural, to have a culture, is to inhabit a place sufficiently intensely to cultivate it—to be responsible for it, to respond to it, to attend to it caringly."[19]

Culture described by Richard Velkley:[18]

... originally meant the cultivation of the soul or mind, acquires most of its later modern meaning in the writings of the 18th-century German thinkers, who were on various levels developing Rousseau's criticism of "modern liberalism and Enlightenment". Thus a contrast between "culture" and "civilization" is usually implied in these authors, even when not expressed as such.

In the words of anthropologist E. B. Tylor, it is "that complex whole which includes knowledge, belief, art, morals, law, custom and any other capabilities and habits acquired by man as a member of society".[20] Alternatively, in a contemporary variant, "Culture is defined as a social domain that emphasizes the practices, discourses and material expressions, which, over time, express the continuities and discontinuities of social meaning of a life held in common.[21]

The Cambridge English Dictionary states that culture is "the way of life, especially the general customs and beliefs, of a particular group of people at a particular time."[22] Terror management theory posits that culture is a series of activities and worldviews that provide humans with the basis for perceiving themselves as "person[s] of worth within the world of meaning"—raising themselves above the merely physical aspects of existence, in order to deny the animal insignificance and death that Homo sapiens became aware of when they acquired a larger brain.[23][24]

The word is used in a general sense as the evolved ability to categorize and represent experiences with symbols and to act imaginatively and creatively.[25] This ability arose with the evolution of behavioral modernity in humans around 50,000 years ago and is often thought to be unique to humans.[26] However, some other species have demonstrated similar, though less complicated, abilities for social learning.[27] It is also used to denote the complex networks of practices and accumulated knowledge and ideas that are transmitted through social interaction and exist in specific human groups, or cultures, using the plural form.[28]

Change
Main article: Culture change

The Beatles exemplified changing cultural dynamics, not only in music, but fashion and lifestyle. Six decades after their emergence, they continue to have a worldwide cultural impact.
Raimon Panikkar identified 29 ways in which cultural change can be brought about, including growth, development, evolution, involution, renovation, reconception, reform, innovation, revivalism, revolution, mutation, progress, diffusion, osmosis, borrowing, eclecticism, syncretism, modernization, indigenization, and transformation.[29] In this context, modernization could be viewed as adopting Enlightenment-era beliefs and practices, such as science, rationalism, industry, commerce, democracy, and the notion of progress. Rein Raud, building on the work of Umberto Eco, Pierre Bourdieu and Jeffrey C. Alexander, has proposed a model of cultural change based on claims and bids, which are judged by their cognitive adequacy and endorsed or not endorsed by the symbolic authority of the cultural community in question.[30]


19th-century engraving shows Indigenous Australians opposing the arrival of James Cook in 1770
Cultural invention has come to mean any innovation that is new and found to be useful to a group of people and expressed in their behavior, but which does not exist as a physical object. Humanity is in a global "accelerating culture change period," driven by the expansion of international commerce, the mass media, and above all, the human population explosion, among other factors. Culture repositioning means the reconstruction of the cultural concept of a society.[31]

Cultures are internally affected by both forces encouraging change and forces resisting change. These forces are related to both social structures and natural events and are involved in perpetuating cultural ideas and practices within current structures, which themselves are subject to change.[32]

Social conflict and the development of technologies can produce changes within a society by altering social dynamics and promoting new cultural models and spurring or enabling generative action. These social shifts may accompany ideological shifts and other types of cultural change. For example, the feminist movement involved new practices that produced a shift in gender relations, altering both gender and economic structures. Environmental conditions may also enter as factors. For example, after tropical forests returned at the end of the last ice age, plants suitable for domestication were available, leading to the invention of agriculture, which in turn brought about many cultural innovations and shifts in social dynamics.[33]

Full-length profile portrait of a woman, standing on a carpet at the entrance to a yurt, dressed in traditional clothing and jewelry
Turkmen woman, on a carpet at the entrance to a yurt, in traditional clothing. Sense of time is dependent on culture. This is a 1913 photo, but it can be difficult to date for a viewer, due to the absence of cultural cues.
Cultures are externally affected via contact between societies, which may also produce—or inhibit—social shifts and changes in cultural practices. War or competition over resources may impact technological development or social dynamics. Additionally, cultural ideas may transfer from one society to another, through diffusion or acculturation. In diffusion, the form of something (though not necessarily its meaning) moves from one culture to another. For example, Western restaurant chains and culinary brands sparked curiosity and fascination to the Chinese as China opened its economy to international trade in the late 20th-century.[34] "Stimulus diffusion" (the sharing of ideas) refers to an element of one culture leading to an invention or propagation in another. "Direct borrowing", on the other hand, tends to refer to technological or tangible diffusion from one culture to another. Diffusion of innovations theory presents a research-based model of why and when individuals and cultures adopt new ideas, practices, and products.[35]

Acculturation has different meanings. Still, in this context, it refers to the replacement of traits of one culture with another, such as what happened to certain Native American tribes and many indigenous peoples across the globe during colonization. Related processes on an individual level include assimilation and transculturation. The transnational flow of culture has played a major role in merging different cultures and sharing thoughts, ideas, and beliefs.

Early modern discourses
German Romanticism

Johann Herder called attention to national cultures.
Immanuel Kant (1724–1804) formulated an individualist definition of "enlightenment" similar to the concept of bildung: "Enlightenment is man's emergence from his self-incurred immaturity."[36] He argued that this immaturity comes not from a lack of understanding, but from a lack of courage to think independently. Against this intellectual cowardice, Kant urged: "Sapere Aude" ("Dare to be wise!"). In reaction to Kant, German scholars such as Johann Gottfried Herder (1744–1803) argued that human creativity, which necessarily takes unpredictable and highly diverse forms, is as important as human rationality. Moreover, Herder proposed a collective form of Bildung: "For Herder, Bildung was the totality of experiences that provide a coherent identity, and sense of common destiny, to a people."[37]


Adolf Bastian developed a universal model of culture.
In 1795, the Prussian linguist and philosopher Wilhelm von Humboldt (1767–1835) called for an anthropology that would synthesize Kant's and Herder's interests. During the Romantic era, scholars in Germany, especially those concerned with nationalist movements—such as the nationalist struggle to create a "Germany" out of diverse principalities, and the nationalist struggles by ethnic minorities against the Austro-Hungarian Empire—developed a more inclusive notion of culture as "worldview" (Weltanschauung).[38] According to this school of thought, each ethnic group has a distinct worldview that is incommensurable with the worldviews of other groups. Although more inclusive than earlier views, this approach to culture still allowed for distinctions between "civilized" and "primitive" or "tribal" cultures.

In 1860, Adolf Bastian (1826–1905) argued for "the psychic unity of mankind".[39] He proposed that a scientific comparison of all human societies would reveal that distinct worldviews consisted of the same basic elements. According to Bastian, all human societies share a set of "elementary ideas" (Elementargedanken); different cultures, or different "folk ideas" (Völkergedanken), are local modifications of the elementary ideas.[40] This view paved the way for the modern understanding of culture. Franz Boas (1858–1942) was trained in this tradition, and he brought it with him when he left Germany for the United States.[41]

English Romanticism

British poet and critic Matthew Arnold viewed "culture" as the cultivation of the humanist ideal.
In the 19th century, humanists such as English poet and essayist Matthew Arnold (1822–1888) used the word "culture" to refer to an ideal of individual human refinement, of "the best that has been thought and said in the world".[42] This concept of culture is also comparable to the German concept of bildung: "...culture being a pursuit of our total perfection by means of getting to know, on all the matters which most concern us, the best which has been thought and said in the world".[42]
In practice, culture referred to an elite ideal and was associated with such activities as art, classical music, and haute cuisine.[43] As these forms were associated with urban life, "culture" was identified with "civilization" (from Latin: civitas, lit. 'city'). Another facet of the Romantic movement was an interest in folklore, which led to identifying a "culture" among non-elites. This distinction is often characterized as that between high culture, namely that of the ruling class, and low culture. In other words, the idea of "culture" that developed in Europe during the 18th and early 19th centuries reflected inequalities within European societies.[44]


British anthropologist Edward Tylor was one of the first English-speaking scholars to use the term culture in an inclusive and universal sense.
Matthew Arnold contrasted "culture" with anarchy; other Europeans, following philosophers Thomas Hobbes and Jean-Jacques Rousseau, contrasted "culture" with "the state of nature". According to Hobbes and Rousseau, the Native Americans who were being conquered by Europeans from the 16th centuries on were living in a state of nature; this opposition was expressed through the contrast between "civilized" and "uncivilized".[45] According to this way of thinking, one could classify some countries and nations as more civilized than others and some people as more cultured than others. This contrast led to Herbert Spencer's theory of Social Darwinism and Lewis Henry Morgan's theory of cultural evolution. Just as some critics have argued that the distinction between high and low cultures expresses the conflict between European elites and non-elites, other critics have argued that the distinction between civilized and uncivilized people is an expression of the conflict between European colonial powers and their colonial subjects.

Other 19th-century critics, following Rousseau, have accepted this differentiation between higher and lower culture, but have seen the refinement and sophistication of high culture as corrupting and unnatural developments that obscure and distort people's essential nature. These critics considered folk music (as produced by "the folk," i.e., rural, illiterate, peasants) to honestly express a natural way of life, while classical music seemed superficial and decadent. Equally, this view often portrayed indigenous peoples as "noble savages" living authentic and unblemished lives, uncomplicated and uncorrupted by the highly stratified capitalist systems of Western culture.

In 1870 the anthropologist Edward Tylor (1832–1917) applied these ideas of higher versus lower culture to propose a theory of the evolution of religion. According to this theory, religion evolves from more polytheistic to more monotheistic forms.[46] In the process, he redefined culture as a diverse set of activities characteristic of all human societies. This view paved the way for the modern understanding of religion.

Anthropology
Main article: American anthropology

Petroglyphs in modern-day Gobustan, Azerbaijan, dating to 10,000 BCE and indicating a thriving culture
Although anthropologists worldwide refer to Tylor's definition of culture,[47] in the 20th century "culture" emerged as the central and unifying concept of American anthropology, where it most commonly refers to the universal human capacity to classify and encode human experiences symbolically, and to communicate symbolically encoded experiences socially.[48] American anthropology is organized into four fields, each of which plays an important role in research on culture: biological anthropology, linguistic anthropology, cultural anthropology, and in the United States and Canada, archaeology.[49][50][51][52] The term Kulturbrille, or 'culture glasses', coined by German American anthropologist Franz Boas, refers to the "lenses" through which a person sees their own culture. Martin Lindstrom asserts that Kulturbrille, which allow a person to make sense of the culture they inhabit, "can blind us to things outsiders pick up immediately".[53]

Sociology
Main article: Sociology of culture

An example of folkloric dancing in Colombia
The sociology of culture concerns culture as manifested in society. For sociologist Georg Simmel (1858–1918), culture referred to "the cultivation of individuals through the agency of external forms which have been objectified in the course of history".[54] As such, culture in the sociological field can be defined as the ways of thinking, the ways of acting, and the material objects that together shape a people's way of life. Culture can be either of two types, non-material culture or material culture.[4] Non-material culture refers to the non-physical ideas that individuals have about their culture, including values, belief systems, rules, norms, morals, language, organizations, and institutions, while material culture is the physical evidence of a culture in the objects and architecture they make or have made. The term tends to be relevant only in archeological and anthropological studies, but it specifically means all material evidence which can be attributed to culture, past or present.

Cultural sociology first emerged in Weimar Germany (1918–1933), where sociologists such as Alfred Weber used the term Kultursoziologie ('cultural sociology'). Cultural sociology was then reinvented in the English-speaking world as a product of the cultural turn of the 1960s, which ushered in structuralist and postmodern approaches to social science. This type of cultural sociology may be loosely regarded as an approach incorporating cultural analysis and critical theory. Cultural sociologists tend to reject scientific methods, instead hermeneutically focusing on words, artifacts and symbols. Culture has since become an important concept across many branches of sociology, including resolutely scientific fields like social stratification and social network analysis. As a result, there has been a recent influx of quantitative sociologists to the field. Thus, there is now a growing group of sociologists of culture who are, confusingly, not cultural sociologists. These scholars reject the abstracted postmodern aspects of cultural sociology, and instead, look for a theoretical backing in the more scientific vein of social psychology and cognitive science.[55]


Nowruz is a good sample of popular and folklore culture that is celebrated by people in more than 22 countries with different nations and religions, at the 1st day of spring. It has been celebrated by diverse communities for over 7,000 years.
Early researchers and development of cultural sociology
The sociology of culture grew from the intersection between sociology (as shaped by early theorists like Marx,[56] Durkheim, and Weber) with the growing discipline of anthropology, wherein researchers pioneered ethnographic strategies for describing and analyzing a variety of cultures around the world. Part of the legacy of the early development of the field lingers in the methods (much of cultural, sociological research is qualitative), in the theories (a variety of critical approaches to sociology are central to current research communities), and in the substantive focus of the field. For instance, relationships between popular culture, political control, and social class were early and lasting concerns in the field.

Cultural studies
Main article: Cultural studies
In the United Kingdom, sociologists and other scholars influenced by Marxism such as Stuart Hall (1932–2014) and Raymond Williams (1921–88) developed cultural studies. Following nineteenth-century Romantics, they identified culture with consumption goods and leisure activities (such as art, music, film, food, sports, and clothing). They saw patterns of consumption and leisure as determined by relations of production, which led them to focus on class relations and the organization of production.[57][58]

In the UK, cultural studies focuses largely on the study of popular culture; that is, on the social meanings of mass-produced consumer and leisure goods. Richard Hoggart coined the term in 1964 when he founded the Centre for Contemporary Cultural Studies or CCCS.[59] Cultural studies in this sense, then, can be viewed as a limited concentration scoped on the intricacies of consumerism, which belongs to a wider culture sometimes referred to as Western civilization or globalism.


The Metropolitan Museum of Art in Manhattan. Visual art can be one expression of high culture.
From the 1970s onward, Stuart Hall's pioneering work, along with that of his colleagues Paul Willis, Dick Hebdige, Tony Jefferson, and Angela McRobbie, created an international intellectual movement. As the field developed, it began to combine political economy, communication, sociology, social theory, literary theory, media theory, film/video studies, cultural anthropology, philosophy, museum studies, and art history to study cultural phenomena or cultural texts. In this field researchers often concentrate on how particular phenomena relate to matters of ideology, nationality, ethnicity, social class, or gender.[60]

Cultural studies is concerned with the meaning and practices of everyday life. These practices comprise the ways people do particular things (such as watching television or eating out) in a given culture. It also studies the meanings and uses people attribute to various objects and practices. Specifically, culture involves those meanings and practices held independently of reason. Watching television to view a public perspective on a historical event should not be thought of as culture unless referring to the medium of television itself, which may have been selected culturally; however, schoolchildren watching television after school with their friends to "fit in" certainly qualifies since there is no grounded reason for one's participation in this practice.

In the context of cultural studies, a text includes not only written language, but also films, photographs, fashion, or hairstyles: the texts of cultural studies comprise all the meaningful artifacts of culture.[61] Similarly, the discipline widens the concept of culture. Culture, for a cultural-studies researcher, not only includes traditional high culture (the culture of the ruling social groups)[62] and popular culture, but also everyday meanings and practices. The last two, in fact, have become the main focus of cultural studies. A further and recent approach is comparative cultural studies, based on the disciplines of comparative literature and cultural studies.[63]

Scholars in the UK and the US developed different versions of cultural studies after the 1970s. The British version of cultural studies had originated in the 1950s and 60s, mainly under the influence of Richard Hoggart, E. P. Thompson, and Raymond Williams, and later that of Stuart Hall and others at the Centre for Contemporary Cultural Studies. This included overtly political, left-wing views, and criticisms of popular culture as "capitalist" mass culture; it absorbed some of the ideas of the Frankfurt School critique of the "culture industry" i.e. mass culture. This emerges in the writings of early British cultural-studies scholars and their influences: see the work of Raymond Williams, Stuart Hall, Paul Willis, and Paul Gilroy.

In the United States, Lindlof and Taylor write, "cultural studies [were] grounded in a pragmatic, liberal-pluralist tradition."[64] The American version of cultural studies initially concerned itself more with understanding the subjective and appropriative side of audience reactions to, and uses of, mass culture; for example, American cultural-studies advocates wrote about the liberatory aspects of fandom.[65][66]

Some researchers, especially in early British cultural studies, apply a Marxist model to the field. This strain of thinking has some influence from the Frankfurt School, but especially from the structuralist Marxism of Louis Althusser and others. The main focus of an orthodox Marxist approach concentrates on the production of meaning. This model assumes a mass production of culture and identifies power as residing with those producing cultural artifacts.

In a Marxist view, the mode and relations of production form the economic base of society, which constantly interacts and influences superstructures, such as culture.[67] Other approaches to cultural studies, such as feminist cultural studies and later American developments of the field, distance themselves from this view. They criticize the Marxist assumption of a single, dominant meaning, shared by all, for any cultural product. The non-Marxist approaches suggest that different ways of consuming cultural artifacts affect the meaning of the product.

This view comes through in the book Doing Cultural Studies: The Story of the Sony Walkman (by Paul du Gay et al.),[68] which seeks to challenge the notion that those who produce commodities control the meanings that people attribute to them. Feminist cultural analyst, theorist, and art historian Griselda Pollock contributed to cultural studies from viewpoints of art history and psychoanalysis. The writer Julia Kristeva is among influential voices at the turn of the century, contributing to cultural studies from the field of art and psychoanalytical French feminism.[69]

Petrakis and Kostis (2013) divide cultural background variables into two main groups:[70]

The first group covers the variables that represent the "efficiency orientation" of the societies: performance orientation, future orientation, assertiveness, power distance, and uncertainty avoidance.
The second covers the variables that represent the "social orientation" of societies, i.e., the attitudes and lifestyles of their members. These variables include gender egalitarianism, institutional collectivism, in-group collectivism, and human orientation.
In 2016, a new approach to culture was suggested by Rein Raud,[30] who defines culture as the sum of resources available to human beings for making sense of their world and proposes a two-tiered approach, combining the study of texts (all reified meanings in circulation) and cultural practices (all repeatable actions that involve the production, dissemination or transmission of purposes), thus making it possible to re-link anthropological and sociological study of culture with the tradition of textual theory.

Super culture
A super culture is a collection of cultures and subcultures, that interact with one another, share similar characteristics and collectively have a degree of sense of unity.[citation needed] In other words, Super-culture is a culture encompassing several subcultures with common elements.[71] Examples include: List of Super-cultures:

Rave - In modern society, rave is described as a culture closely defined as a super culture.[72][73]
Steampunk - it is fast becoming a super-culture rather than a mere subculture.[74]
Foodtruck collectives & Pop-up Restaurants + Shops.
Some ancient cultures that are also considered (termed) "Super-culture":

Megalithic Super-culture in Prehistoric Europe
Asian Super-culture (See Korean nationalist historiography)
Psychology
See also: Social psychology, Cultural psychology, and Cross-cultural psychology

The NYC Pride March is the world's largest LGBT event. Regional variation exists with respect to tolerance in different parts of the world.

Cognitive tools suggest a way for people from certain culture to deal with real-life problems, like Suanpan for mathematical calculation.
Starting in the 1990s,[75]: 31  psychological research on culture influence began to grow and challenge the universality assumed in general psychology.[76]: 158–168 [77] Culture psychologists began to try to explore the relationship between emotions and culture, and answer whether the human mind is independent from culture. For example, people from collectivistic cultures, such as the Japanese, suppress their positive emotions more than their American counterparts.[78] Culture may affect the way that people experience and express emotions. On the other hand, some researchers try to look for differences between people's personalities across cultures.[79][80] As different cultures dictate distinctive norms, culture shock is also studied to understand how people react when they are confronted with other cultures. LGBT culture is displayed with significantly different levels of tolerance within different cultures and nations. Cognitive tools may not be accessible or they may function differently cross culture.[75]: 19  For example, people who are raised in a culture with an abacus are trained with distinctive reasoning style.[81] Cultural lenses may also make people view the same outcome of events differently. Westerners are more motivated by their successes than their failures, while East Asians are better motivated by the avoidance of failure.[82]

Culture is important for psychologists to consider when understanding the human mental operation. The notion of the anxious, unstable, and rebellious adolescent has been criticized by experts, such as Robert Epstein, who state that an undeveloped brain is not the main cause of teenagers' turmoils.[83][84] Some have criticized this understanding of adolescence, classifying it as a relatively recent phenomenon in human history created by modern society,[85][86][87][88] and have been highly critical of what they view as the infantilization of young adults in American society.[89] According to Robert Epstein and Jennifer, "American-style teen turmoil is absent in more than 100 cultures around the world, suggesting that such mayhem is not biologically inevitable. Second, the brain itself changes in response to experiences, raising the question of whether adolescent brain characteristics are the cause of teen tumult or rather the result of lifestyle and experiences."[90] David Moshman has also stated in regards to adolescence that brain research "is crucial for a full picture, but it does not provide an ultimate explanation".[91]

Protection of culture

Restoration of an ancient Egyptian monument
There are a number of international agreements and national laws relating to the protection of cultural heritage and cultural diversity. UNESCO and its partner organizations such as Blue Shield International coordinate international protection and local implementation.[92][93] The Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict and the UNESCO Convention on the Protection and Promotion of the Diversity of Cultural Expressions deal with the protection of culture. Article 27 of the Universal Declaration of Human Rights deals with cultural heritage in two ways: it gives people the right to participate in cultural life on the one hand and the right to the protection of their contributions to cultural life on the other.[94]


Anarchist poster reading "No Culture, No Future!", 5 December 2024
In the 21st century, the protection of culture has been the focus of increasing activity by national and international organizations. The United Nations and UNESCO promote cultural preservation and cultural diversity through declarations and legally-binding conventions or treaties. The aim is not to protect a person's property, but rather to preserve the cultural heritage of humanity, especially in the event of war and armed conflict. According to Karl von Habsburg, President of Blue Shield International, the destruction of cultural assets is also part of psychological warfare. The target of the attack is the identity of the opponent, which is why symbolic cultural assets become a main target. It is also intended to affect the particularly sensitive cultural memory, the growing cultural diversity and the economic basis (such as tourism) of a state, region or municipality.[95][96][97]

Tourism is having an increasing impact on the various forms of culture. On the one hand, this can be physical impact on individual objects or the destruction caused by increasing environmental pollution and, on the other hand, socio-cultural effects on society.[98][99][100]
The formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.[50]

In the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics.[51] The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.[17]

By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (langue), from language as a concrete manifestation of this system (parole).[52]

Modern linguistics

Noam Chomsky is one of the most important linguistic theorists of the 20th century.
In the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.[53]

In opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out.[22][note 2] The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.[55]

Physiological and neural architecture of language and speech
Speaking is the default modality for language in all cultures.[citation needed] The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language.[56] The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.[57]

The brain
Main articles: Neurolinguistics and Language processing in the brain

Language Areas of the brain.
  Angular gyrus
  Supramarginal gyrus
  Broca's area
  Wernicke's area
  Primary auditory cortex
The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.[58]

Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out.[59] They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with word repetition. The condition affects both spoken and written language. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.[60][61]

With technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.[58]

Anatomy of speech
Main articles: Speech production, Phonetics, and Articulatory phonetics

The human vocal tract

Spectrogram of American English vowels [i, u, ɑ] showing the formants f1 and f2
Duration: 15 seconds.0:15
Real time MRI scan of a person speaking in Mandarin Chinese
Spoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.[62]

The sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between them. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.[63]

Consonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave. Formants are the amplitude peaks in the frequency spectrum of a specific sound.[63][64]

Vowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity.[63] Vowels are called close when the lips are relatively closed, as in the pronunciation of the vowel [i] (English "ee"), or open when the lips are relatively open, as in the vowel [a] (English "ah"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as [u] (English "oo"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between [i] (unrounded front vowel such as English "ee") and [y] (rounded front vowel such as German "ü").[65]

Consonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called occlusive or stop, or different degrees of aperture creating fricatives and approximants. Consonants can also be either voiced or unvoiced, depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English [s] in bus (unvoiced sibilant) from [z] in buzz (voiced sibilant).[66]

Some speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called nasals or nasalized sounds. Other sounds are defined by the way the tongue moves within the mouth such as the l-sounds (called laterals, because the air flows along both sides of the tongue), and the r-sounds (called rhotics).[64]

By using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.[67]

Modality
Human languages display considerable plasticity[1] in their deployment of two fundamental modes: oral (speech and mouthing) and manual (sign and gesture).[note 3] For example, it is common for oral language to be accompanied by gesture, and for sign language to be accompanied by mouthing. In addition, some language communities use both modes to convey lexical or grammatical meaning, each mode complementing the other. Such bimodal use of language is especially common in genres such as story-telling (with Plains Indian Sign Language and Australian Aboriginal sign languages used alongside oral language, for example), but also occurs in mundane conversation. For instance, many Australian languages have a rich set of case suffixes that provide details about the instrument used to perform an action. Others lack such grammatical precision in the oral mode, but supplement it with gesture to convey that information in the sign mode. In Iwaidja, for example, 'he went out for fish using a torch' is spoken as simply "he-hunted fish torch", but the word for 'torch' is accompanied by a gesture indicating that it was held. In another example, the ritual language Damin had a heavily reduced oral vocabulary of only a few hundred words, each of which was very general in meaning, but which were supplemented by gesture for greater precision (e.g., the single word for fish, l*i, was accompanied by a gesture to indicate the kind of fish).[68]
India, officially the Republic of India,[j][20] is a country in South Asia. It is the seventh-largest country by area; the most populous country since 2023;[21] and, since its independence in 1947, the world's most populous democracy.[22][23][24] Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Myanmar, Thailand, and Indonesia.

Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.[26][27][28] Their long occupation, predominantly in isolation as hunter-gatherers, has made the region highly diverse.[29] Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[30] By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest.[31][32] Its hymns recorded the early dawnings of Hinduism in India.[33] India's pre-existing Dravidian languages were supplanted in the northern regions.[34] By 400 BCE, caste had emerged within Hinduism,[35] and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.[36] Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires.[37] Widespread creativity suffused this era,[38] but the status of women declined,[39] and untouchability became an organised belief.[l][40] In South India, the Middle kingdoms exported Dravidian language scripts and religious cultures to the kingdoms of Southeast Asia.[41]

In the 1st millennium, Islam, Christianity, Judaism, and Zoroastrianism became established on India's southern and western coasts.[42] In the early centuries of the 2nd millennium Muslim armies from Central Asia intermittently overran India's northern plains.[43] The resulting Delhi Sultanate drew northern India into the cosmopolitan networks of medieval Islam.[44] In south India, the Vijayanagara Empire created a long-lasting composite Hindu culture.[45] In the Punjab, Sikhism emerged, rejecting institutionalised religion.[46] The Mughal Empire ushered in two centuries of economic expansion and relative peace,[47] and left a a rich architectural legacy.[48][49] Gradually expanding rule of the British East India Company turned India into a colonial economy but consolidated its sovereignty.[50] British Crown rule began in 1858. The rights promised to Indians were granted slowly,[51][52] but technological changes were introduced, and modern ideas of education and the public life took root.[53] A nationalist movement emerged in India, the first in the non-European British Empire and an influence on other nationalist movements.[54][55] Noted for nonviolent resistance after 1920,[56] it became the primary factor in ending British rule.[57] In 1947, the British Indian Empire was partitioned into two independent dominions,[58][59][60][61] a Hindu-majority dominion of India and a Muslim-majority dominion of Pakistan. A large-scale loss of life and an unprecedented migration accompanied the partition.[62]

India has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to over 1.4 billion in 2023.[63] During this time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. A comparatively destitute country in 1951,[64] India has become a fast-growing major economy and a hub for information technology services, with an expanding middle class.[65] India has reduced its poverty rate, though at the cost of increasing economic inequality.[66] It is a nuclear-weapon state that ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century.[67] Among the socio-economic challenges India faces are gender inequality, child malnutrition,[68] and rising levels of air pollution.[69] India's land is megadiverse with four biodiversity hotspots.[70] India's wildlife, which has traditionally been viewed with tolerance in its culture,[71] is supported in protected habitats.

Etymology
Main article: Names for India
According to the Oxford English Dictionary, the English proper noun "India" derives most immediately from the Classical Latin India, a reference to a loosely-defined historical region of Asia stretching from South Asia to the borders of China. Further etymons are: Hellenistic Greek India (Ἰνδία); Ancient Greek Indos (Ἰνδός), or the River Indus; Achaemenian Old Persian Hindu (an eastern province of the Achaemenid Empire); and Sanskrit Sindhu, or "river," but specifically the Indus river, and by extension its well-settled basin.[72] The Ancient Greeks referred to South Asians as Indoi, 'the people of the Indus'.[73]

The term Bharat (Bhārat; pronounced [ˈbʱaːɾət] ⓘ), mentioned in both Indian epic poetry and the Constitution of India,[74][75] is used in its variations by many Indian languages. A modern rendering of the historical name Bharatavarsha, which applied originally to North India,[76][77] Bharat gained increased currency from the mid-19th century as a native name for India.[74][78]

Hindustan ([ɦɪndʊˈstaːn] ⓘ) is a Middle Persian name for India that became popular by the 13th century,[79] and was used widely since the era of the Mughal Empire. The meaning of Hindustan has varied, referring to a region encompassing the northern Indian subcontinent (present-day northern India and Pakistan) or to India in its near entirety.[74][78][80]

History
Main article: History of India
Ancient India

Manuscript illustration, c. 1650, of the Sanskrit epic Ramayana, composed in story-telling fashion c. 400 BCE – c. 300 CE[81]
Based on coalescence of Mitochondrial DNA and Y Chromosome data, it is thought that the earliest extant lineages of anatomically modern humans or Homo sapiens on the Indian subcontinent had reached there from Africa between 80,000 and 50,000 years ago, and with high likelihood by 55,000 years ago.[82][26][27][28] However, the earliest known modern human fossils in South Asia date to about 30,000 years ago.[27] Evidence for domestication of food crops and animals, construction of permanent structures, and storage of agricultural surplus appeared in Mehrgarh and other sites in Balochistan, Pakistan after 6500 BCE.[83] These gradually developed into the Indus Valley Civilisation,[84][83] the first urban culture in South Asia,[85] which flourished during 2500–1900 BCE in Pakistan and western India.[86] Centred around cities such as Mohenjo-daro, Harappa, Dholavira, and Kalibangan, and relying on varied forms of subsistence, the civilisation engaged robustly in crafts production and wide-ranging trade.[85]

During the period 2000–500 BCE, many regions of the subcontinent transitioned from the Chalcolithic cultures to the Iron Age ones.[87] The Vedas, the oldest scriptures associated with Hinduism,[88] were composed during this period,[89] and historians have analysed these to posit a Vedic culture in the Punjab region and the upper Gangetic Plain.[87] Most historians also consider this period to have encompassed several waves of Indo-Aryan migration into the subcontinent from the north-west.[88] The caste system, which created a hierarchy of priests, warriors, and free peasants, but which excluded indigenous peoples by labelling their occupations impure, arose during this period.[90] On the Deccan Plateau, archaeological evidence from this period suggests the existence of a chiefdom stage of political organisation.[87] In South India, a progression to sedentary life is indicated by the large number of megalithic monuments dating from this period,[91] as well as by nearby traces of agriculture, irrigation tanks, and craft traditions.[91]


Cave 26 of the rock-cut Ajanta Caves
In the late Vedic period, around the 6th century BCE, the small states and chiefdoms of the Ganges Plain and the north-western regions had consolidated into 16 major oligarchies and monarchies that were known as the mahajanapadas.[92][93] The emerging urbanisation gave rise to non-Vedic religious movements, two of which became independent religions. Jainism came into prominence during the life of its exemplar, Mahavira.[94] Buddhism, based on the teachings of Gautama Buddha, attracted followers from all social classes excepting the middle class; chronicling the life of the Buddha was central to the beginnings of recorded history in India.[95][96][97]

In an age of increasing urban wealth, both religions held up renunciation as an ideal,[98] and both established long-lasting monastic traditions. Politically, by the 3rd century BCE, the kingdom of Magadha had annexed or reduced other states to emerge as the Maurya Empire.[99] The empire was once thought to have controlled most of the subcontinent except the far south, but its core regions are now thought to have been separated by large autonomous areas.[100][101] The Mauryan kings are known as much for their empire-building and determined management of public life as for Ashoka's renunciation of militarism and far-flung advocacy of the Buddhist dhamma.[102][103]

The Sangam literature of the Tamil language reveals that, between 200 BCE and 200 CE, the southern peninsula was ruled by the Cheras, the Cholas, and the Pandyas, dynasties that traded extensively with the Roman Empire and with West and Southeast Asia.[104][105] In North India, Hinduism asserted patriarchal control within the family, leading to increased subordination of women.[106][99] By the 4th and 5th centuries, the Gupta Empire had created a complex system of administration and taxation in the greater Ganges Plain; this system became a model for later Indian kingdoms.[107][108] Under the Guptas, a renewed Hinduism based on devotion, rather than the management of ritual, began to assert itself.[109] This renewal was reflected in a flowering of sculpture and architecture, which found patrons among an urban elite.[108] Classical Sanskrit literature flowered as well, and Indian science, astronomy, medicine, and mathematics made significant advances.[108]

Medieval India
Main article: Medieval India

Brihadeshwara temple, Thanjavur, completed in 1010 CE

The Qutub Minar, 73 m (240 ft) tall, completed by the Sultan of Delhi, Iltutmish
The Indian early medieval age, from 600 to 1200 CE, is defined by regional kingdoms and cultural diversity.[110] When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647 CE, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan.[111] When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal.[111] When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south.[111] No ruler of this period was able to create an empire and consistently control lands much beyond their core region.[110] During this time, pastoral peoples, whose land had been cleared to make way for the growing agricultural economy, were accommodated within caste society, as were new non-traditional ruling classes.[112] The caste system consequently began to show regional differences.[112]

In the 6th and 7th centuries, the first devotional hymns were created in the Tamil language.[113] They were imitated all over India and led to both the resurgence of Hinduism and the development of all modern languages of the subcontinent.[113] Indian royalty, big and small, and the temples they patronised drew citizens in great numbers to the capital cities, which became economic hubs as well.[114] Temple towns of various sizes began to appear everywhere as India underwent another urbanisation.[114] By the 8th and 9th centuries, the effects were felt in Southeast Asia, as South Indian culture and political systems were exported to lands that became part of modern-day Myanmar, Thailand, Laos, Brunei, Cambodia, Vietnam, Philippines, Malaysia, and Indonesia.[115] Indian merchants, scholars, and sometimes armies were involved in this transmission; Southeast Asians took the initiative as well, with many sojourning in Indian seminaries and translating Buddhist and Hindu texts into their languages.[115]

After the 10th century, Muslim Central Asian nomadic clans, using swift-horse cavalry and raising vast armies united by ethnicity and religion, repeatedly overran South Asia's north-western plains, leading eventually to the establishment of the Islamic Delhi Sultanate in 1206.[116] The sultanate was to control much of North India and to make many forays into South India. Although at first disruptive for the Indian elites, the sultanate largely left its vast non-Muslim subject population to its own laws and customs.[117][118]

By repeatedly repulsing Mongol raiders in the 13th century, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north.[119][120] The sultanate's raiding and weakening of the regional kingdoms of South India paved the way for the indigenous Vijayanagara Empire.[121] Embracing a strong Shaivite tradition and building upon the military technology of the sultanate, the empire came to control much of peninsular India,[122] and was to influence South Indian society for long afterwards.[121]

Early modern India

A distant view of the Taj Mahal from the Agra Fort

A two-mohur Company gold coin, issued in 1835, the obverse inscribed "William IIII, King"
In the early 16th century, northern India, then under mainly Muslim rulers,[123] fell again to the superior mobility and firepower of a new generation of Central Asian warriors.[124] The resulting Mughal Empire did not stamp out the local societies it came to rule. Instead, it balanced and pacified them through new administrative practices[125][126] and diverse and inclusive ruling elites,[127] leading to more systematic, centralised, and uniform rule.[128] Eschewing tribal bonds and Islamic identity, especially under Akbar, the Mughals united their far-flung realms through loyalty, expressed through a Persianised culture, to an emperor who had near-divine status.[127]

The Mughal state's economic policies, deriving most revenues from agriculture[129] and mandating that taxes be paid in the well-regulated silver currency,[130] caused peasants and artisans to enter larger markets.[128] The relative peace maintained by the empire during much of the 17th century was a factor in India's economic expansion,[128] resulting in greater patronage of painting, literary forms, textiles, and architecture.[131] Newly coherent social groups in northern and western India, such as the Marathas, the Rajputs, and the Sikhs, gained military and governing ambitions during Mughal rule, which, through collaboration or adversity, gave them both recognition and military experience.[132] Expanding commerce during Mughal rule gave rise to new Indian commercial and political elites along the coasts of southern and eastern India.[132] As the empire disintegrated, many among these elites were able to seek and control their own affairs.[133]

By the early 18th century, with the lines between commercial and political dominance being increasingly blurred, a number of European trading companies, including the English East India Company, had established coastal outposts.[134][135] The East India Company's control of the seas, greater resources, and more advanced military training and technology led it to increasingly assert its military strength and caused it to become attractive to a portion of the Indian elite; these factors were crucial in allowing the company to gain control over the Bengal region by 1765 and sideline the other European companies.[136][134][137][138] Its further access to the riches of Bengal and the subsequent increased strength and size of its army enabled it to annexe or subdue most of India by the 1820s.[139] India was then no longer exporting manufactured goods as it long had, but was instead supplying the British Empire with raw materials. Many historians consider this to be the onset of India's colonial period.[134] By this time, with its economic power severely curtailed by the British parliament and having effectively been made an arm of British administration, the East India Company began more consciously to enter non-economic arenas, including education, social reform, and culture.[140]
Historians consider India's modern age to have begun sometime between 1848 and 1885. The appointment in 1848 of Lord Dalhousie as Governor General of the East India Company set the stage for changes essential to a modern state. These included the consolidation and demarcation of sovereignty, the surveillance of the population, and the education of citizens. Technological changes—among them, railways, canals, and the telegraph—were introduced not long after their introduction in Europe.[141][142][143][144] Disaffection with the company also grew during this time and set off the Indian Rebellion of 1857. Fed by diverse resentments and perceptions, including invasive British-style social reforms, harsh land taxes, and summary treatment of some rich landowners and princes, the rebellion rocked many regions of northern and central India and shook the foundations of Company rule.[145][146]

Although the rebellion was suppressed by 1858, it led to the dissolution of the East India Company and the direct administration of India by the British government. Proclaiming a unitary state and a gradual but limited British-style parliamentary system, the new rulers also protected princes and landed gentry as a feudal safeguard against future unrest.[147][148] In the decades following, public life gradually emerged all over India, leading eventually to the founding of the Indian National Congress (generally referred to as the Congress) in 1885.[149][150][151][152]


Jawaharlal Nehru sharing a light moment with Mahatma Gandhi, Mumbai, 6 July 1946
The rush of technology and the commercialisation of agriculture in the second half of the 19th century was marked by economic setbacks, and many small farmers became dependent on the whims of far-away markets.[153] There was an increase in the number of large-scale famines,[154] and, despite the risks of infrastructure development borne by Indian taxpayers, little industrial employment was generated for Indians.[155] There were also salutary effects: commercial cropping, especially in the newly canalled Punjab, led to increased food production for internal consumption.[156] The railway network provided critical famine relief,[157] notably reduced the cost of moving goods,[157] and helped nascent Indian-owned industry.[156]

After World War I, in which approximately one million Indians served,[158] a new period began. It was marked by British reforms but also repressive legislation, by more strident Indian calls for self-rule, and by the beginnings of a nonviolent movement of non-co-operation, of which Mahatma Gandhi would become the leader and enduring symbol.[159] During the 1930s, slow legislative reform was enacted by the British; the Indian National Congress won victories in the resulting elections.[160] The next decade was beset with crises: Indian participation in World War II, the Congress's final push for non-co-operation, and an upsurge of Muslim nationalism. All were capped by the advent of independence in 1947, but tempered by the partition of India into two states: India and Pakistan.[161]

Vital to India's self-image as an independent nation was its constitution, completed in 1950, which put in place a secular and democratic republic.[162] Economic liberalisation, which began in the 1980s and with the collaboration with Soviet Union for technical knowledge,[163] has created a large urban middle class, transformed India into one of the world's fastest-growing economies,[164] and increased its geopolitical influence. Yet, India is also shaped by persistent poverty, both rural and urban;[165] by religious and caste-related violence;[166] by Maoist-inspired Naxalite insurgencies;[167] and by separatism in Jammu and Kashmir and in Northeast India.[168] It has unresolved territorial disputes with China and with Pakistan.[169] India's sustained democratic freedoms are unique among the world's newer nations; however, in spite of its recent economic successes, freedom from want for its disadvantaged population remains a goal yet to be achieved.[170]

Geography
Main articles: Geography of India and Himalayas

A panaromic view of the Garhwal- and Kumaon Himalayas. Rising above their surroundings in the view are among others, Trisul, Nanda Devi, and Nanda Kot. Nanda Devi, the highest peak entirely within India's borders, is at the centre of the Nanda Devi National Park, a UNESCO World Heritage Natural Site.

The Tungabhadra, with rocky outcrops, flows into the peninsular Krishna River.[171]
India accounts for the bulk of the Indian subcontinent, lying atop the Indian tectonic plate, and a part of the Indo-Australian Plate.[172] India's defining geologic processes began approximately 70 million years ago, when the Indian Plate, then part of the southern supercontinent Gondwana, began a north-eastward drift caused by seafloor spreading to its south-west, and later, south and south-east.[172] Simultaneously, the vast Tethyan oceanic crust, to its northeast, began to subduct under the Eurasian Plate.[172] The Indian continental crust, however, was obstructed and sheared horizontally. Its lower crust and mantle slid under, but the upper layer piled up in sheets (or nappes) ahead of the subduction zone.[173] This created the orogeny, or process of mountain building, of the Himalayas.[174] The middle and stiffer layer continued to push into Tibet, causing crustal thickening and giving rise to the Tibetan Plateau.[175] Immediately south of the emerging Himalayas, plate movement created a vast crescent-shaped trough that rapidly filled with river-borne sediment[176] and now constitutes the Indo-Gangetic Plain.[177] The original Indian plate makes its first appearance above the sediment in the ancient Aravalli range, which extends from the Delhi Ridge in a southwesterly direction. To the west lies the Thar Desert, the eastern spread of which is checked by the Aravallis.[178][179][180]

The remaining Indian Plate survives as peninsular India, the oldest and geologically most stable part of India. It extends as far north as the Satpura and Vindhya ranges in central India. These parallel chains run from the Arabian Sea coast in Gujarat in the west to the coal-rich Chota Nagpur Plateau in Jharkhand in the east.[181] To the south, the remaining peninsular landmass, the Deccan Plateau, is flanked on the west and east by coastal ranges known as the Western and Eastern Ghats;[182] the plateau contains the country's oldest rock formations, some over one billion years old. Constituted in such fashion, India lies to the north of the equator between 6° 44′ and 35° 30′ north latitude[m] and 68° 7′ and 97° 25′ east longitude.[183]

Major Himalayan-origin rivers that substantially flow through India include the Ganges and the Brahmaputra, both of which drain into the Bay of Bengal.[184] Important tributaries of the Ganges include the Yamuna and the Kosi. The Kosi's extremely low gradient, caused by long-term silt deposition, leads to severe floods and course changes.[185][186] Major peninsular rivers, whose steeper gradients prevent their waters from flooding, include the Godavari, the Mahanadi, the Kaveri, and the Krishna, which also drain into the Bay of Bengal;[187] and the Narmada and the Tapti, which drain into the Arabian Sea.[188]

India's coastline measures 7,517 kilometres (4,700 mi) in length; of this distance, 5,423 kilometres (3,400 mi) belong to peninsular India and 2,094 kilometres (1,300 mi) to the Andaman, Nicobar, and Lakshadweep island chains.[189] According to the Indian naval hydrographic charts, the mainland coastline consists of the following: 43% sandy beaches; 11% rocky shores, including cliffs; and 46% mudflats or marshy shores.[189]Coastal features include the marshy Rann of Kutch of western India and the alluvial Sundarbans delta of eastern India; the latter is shared with Bangladesh.[190] India has two archipelagos: the Lakshadweep, coral atolls off India's south-western coast; and the Andaman and Nicobar Islands, a volcanic chain in the Andaman Sea.[191]

Climate
Main article: Climate of India

Fishing boats are moored and lashed together during an approaching monsoon storm whose dark clouds can be seen overhead. The scene is a tidal creek in Anjarle, a coastal village in Maharashtra.
The Indian climate is strongly influenced by the Himalayas and the Thar Desert, both of which drive the economically and culturally pivotal summer and winter monsoons.[192] The Himalayas prevent cold Central Asian katabatic winds from blowing in, keeping the bulk of the Indian subcontinent warmer than most locations at similar latitudes.[193][194] The Thar Desert plays a crucial role in attracting the moisture-laden south-west summer monsoon winds that, between June and October, provide the majority of India's rainfall.[192]

Four major climatic groupings predominate in India: tropical wet, tropical dry, subtropical humid, and montane.[195] Temperatures in India have risen by 0.7 °C (1.3 °F) between 1901 and 2018.[196] Climate change in India is often thought to be the cause. The retreat of Himalayan glaciers has adversely affected the flow rate of the major Himalayan rivers, including the Ganges and the Brahmaputra.[197] According to some current projections, the number and severity of droughts in India will have markedly increased by the end of the present century.[198]

Biodiversity
Main articles: Forestry in India and Wildlife of India

India has the majority of the world's wild tigers, approximately 3,170 in 2022.[199]

A chital (Axis axis) stag in the Nagarhole National Park in a region covered by a moderately dense[n] forest

Three of the last Asiatic cheetahs in India were shot dead in 1948 in Surguja district, Madhya Pradesh, Central India, by Maharajah Ramanuj Pratap Singh Deo. The young male cheetahs, all from the same litter, were sitting together when they were shot at night.
India is a megadiverse country, a term employed for 17 countries that display high biological diversity and contain many species exclusively indigenous, or endemic, to them.[200] India is the habitat for 8.6% of all mammals, 13.7% of bird species, 7.9% of reptile species, 6% of amphibian species, 12.2% of fish species, and 6.0% of all flowering plant species.[201][202] Fully a third of Indian plant species are endemic.[203] India also contains four of the world's 34 biodiversity hotspots,[70] or regions that display significant habitat loss in the presence of high endemism.[o][204]

India's most dense forests, such as the tropical moist forest of the Andaman Islands, the Western Ghats, and Northeast India, occupy approximately 3% of its land area.[205][206] Moderately dense forest, whose canopy density is between 40% and 70%, occupies 9.39% of India's land area.[205][206] It predominates in the temperate coniferous forest of the Himalayas, the moist deciduous sal forest of eastern India, and the dry deciduous teak forest of central and southern India.[207] India has two natural zones of thorn forest, one in the Deccan Plateau, immediately east of the Western Ghats, and the other in the western part of the Indo-Gangetic plain, now turned into rich agricultural land by irrigation, its features no longer visible.[208] Among the Indian subcontinent's notable indigenous trees are the astringent Azadirachta indica, or neem, which is widely used in rural Indian herbal medicine,[209] and the luxuriant Ficus religiosa, or peepul,[210] which is displayed on the ancient seals of Mohenjo-daro,[211] and under which the Buddha is recorded in the Pali canon to have sought enlightenment.[212]

Many Indian species have descended from those of Gondwana, the southern supercontinent from which India separated more than 100 million years ago.[213] India's subsequent collision with Eurasia set off a mass exchange of species. However, volcanism and climatic changes later caused the extinction of many endemic Indian forms.[214] Still later, mammals entered India from Asia through two zoogeographic passes flanking the Himalayas.[215] This lowered endemism among India's mammals, which stands at 12.6%, contrasting with 45.8% among reptiles and 55.8% among amphibians.[202] Among endemics are the vulnerable[216] hooded leaf monkey[217] and the threatened Beddome's toad[218][219] of the Western Ghats.

India contains 172 IUCN-designated threatened animal species, or 2.9% of endangered forms.[220] These include the endangered Bengal tiger and the Ganges river dolphin. Critically endangered species include the gharial, a crocodilian; the great Indian bustard; and the Indian white-rumped vulture, which has become nearly extinct by having ingested the carrion of diclofenac-treated cattle.[221] Before they were extensively used for agriculture and cleared for human settlement, the thorn forests of Punjab were mingled at intervals with open grasslands that were grazed by large herds of blackbuck preyed on by the Asiatic cheetah; the blackbuck, no longer extant in Punjab, is now severely endangered in India, and the cheetah is extinct.[222] The pervasive and ecologically devastating human encroachment of recent decades has critically endangered Indian wildlife. In response, the system of national parks and protected areas, first established in 1935, was expanded substantially. In 1972, India enacted the Wildlife Protection Act[223] and Project Tiger to safeguard crucial wilderness; the Forest Conservation Act was enacted in 1980 and amendments added in 1988.[224] India hosts more than five hundred wildlife sanctuaries and eighteen biosphere reserves,[225] four of which are part of the World Network of Biosphere Reserves; its eighty-nine wetlands are registered under the Ramsar Convention.[226]

Government and politics
Politics
Main article: Politics of India
See also: Democracy in India

As part of Janadesh 2007, 25,000 pro–land reform landless people in Madhya Pradesh listen to Rajagopal P. V.[227]

US president Barack Obama addresses the members of the Parliament of India in New Delhi in November 2010.
India is a parliamentary republic with a multi-party system.[228] It has six recognised national parties, including the Indian National Congress (generally referred to as the Congress) and the Bharatiya Janata Party (BJP), and over 50 regional parties.[229] The Congress is considered the ideological centre in Indian political culture,[230] whereas the BJP is right-wing to far-right.[231][232][233] From 1950 to the late 1980s, Congress held a majority in India's parliament. Afterwards, it increasingly shared power with the BJP,[234] as well as with powerful regional parties, which forced multi-party coalition governments at the centre.[235]

In the general elections in 1951, 1957, and 1962, the Congress, led by Jawaharlal Nehru, won easy victories. On Nehru's death in 1964, Lal Bahadur Shastri briefly became prime minister; he was succeeded in 1966, by Nehru's daughter Indira Gandhi, who led the Congress to election victories in 1967 and 1971. Following public discontent with the state of emergency Indira Gandhi had declared in 1975, the Congress was voted out of power in 1977; Janata Party, which had opposed the emergency, was voted in. Its government lasted two years; Morarji Desai and Charan Singh served as prime ministers. After the Congress was returned to power in 1980, Indira Gandhi was assassinated and succeeded by Rajiv Gandhi, who won comfortably in the elections later that year. A National Front coalition led by the Janata Dal in alliance with the Left Front won the 1989 elections, with the subsequent government lasting just under two years, and V.P. Singh and Chandra Shekhar serving as prime ministers.[236] In the 1991 Indian general election, the Congress, as the largest single party, formed a minority government led by P. V. Narasimha Rao.[237]

After the 1996 Indian general election, the BJP formed a government briefly; it was followed by United Front coalitions, which depended on external political support. Two prime ministers served during this period: H.D. Deve Gowda and I.K. Gujral. In 1998, the BJP formed a coalition—the National Democratic Alliance (NDA). Led by Atal Bihari Vajpayee, the NDA became the first non-Congress, coalition government to complete a five-year term.[238] In the 2004 Indian general elections, no party won an absolute majority. Still, the Congress emerged as the largest single party, forming another successful coalition: the United Progressive Alliance (UPA). It had the support of left-leaning parties and MPs who opposed the BJP. The UPA returned to power in the 2009 general election with increased numbers, and it no longer required external support from India's communist parties.[239] Manmohan Singh became the first prime minister since Jawaharlal Nehru in 1957 and 1962 to be re-elected to a consecutive five-year term.[240] In the 2014 general election, the BJP became the first political party since 1984 to win an absolute majority.[241] In the 2019 general election, the BJP regained an absolute majority. In the 2024 general election, a BJP-led NDA coalition formed the government. Narendra Modi, a former chief minister of Gujarat, is in his third term as the prime minister of India and has served in the position since 26 May 2014.[242]

Government
Main article: Government of India
See also: Constitution of India

Rashtrapati Bhavan, the official residence of the President of India, was designed by British architects Edwin Lutyens and Herbert Baker for the Viceroy of India, and constructed between 1911 and 1931 during the British Raj.[243]
India is a federation with a parliamentary system governed under the Constitution of India. Federalism in India defines the power distribution between the union and the states. India's form of government, traditionally described as "quasi-federal" with a strong centre and weak states,[244] has grown increasingly federal since the late 1990s as a result of political, economic, and social changes.[245][246]

The Government of India comprises three branches: the Executive, Legislature, and Judiciary.[247] The President of India is the ceremonial head of state,[248] who is elected indirectly for a five-year term by an electoral college comprising members of national and state legislatures.[249][250] The Prime Minister of India is the head of government and exercises most executive power.[251] Appointed by the president,[252] the prime minister is supported by the party or political alliance with a majority of seats in the lower house of parliament.[251] The executive of the Indian government consists of the president, the vice-president, and the Union Council of Ministers—with the cabinet being its executive committee—headed by the prime minister. Any minister holding a portfolio must be a member of one of the houses of parliament.[248] In the Indian parliamentary system, the executive is subordinate to the legislature; the prime minister and their council are directly responsible to the lower house of the parliament. Civil servants act as permanent executives and all decisions of the executive are implemented by them.[253]

The legislature of India is the bicameral parliament. Operating under a Westminster-style parliamentary system, it comprises an upper house called the Rajya Sabha (Council of States) and a lower house called the Lok Sabha (House of the People).[254] The Rajya Sabha is a permanent body of 245 members who serve staggered six-year terms with elections every 2 years.[255] Most are elected indirectly by the state and union territorial legislatures in numbers proportional to their state's share of the national population.[252] The Lok Sabha's 543 members are elected directly by popular vote among citizens aged at least 18;[256] they represent single-member constituencies for five-year terms.[257] Several seats from each state are reserved for candidates from Scheduled Castes and Scheduled Tribes in proportion to their population within that state.[256]

India has a three-tier unitary independent judiciary[258] comprising the supreme court, headed by the Chief Justice of India, 25 high courts, and a large number of trial courts.[258] The supreme court has original jurisdiction over cases involving fundamental rights and over disputes between states and the centre and has appellate jurisdiction over the high courts.[259] It has the power to both strike down union or state laws which contravene the constitution[260] and invalidate any government action it deems unconstitutional.[261]

Administrative divisions
Main article: Administrative divisions of India
See also: Political integration of India

A clickable map of the 28 states and 8 union territories of India
India is a federal union comprising 28 states and 8 union territories.[12] All states, as well as the union territories of Jammu and Kashmir, Puducherry and the National Capital Territory of Delhi, have elected legislatures and governments following the Westminster system. The remaining five union territories are directly ruled by the central government through appointed administrators. In 1956, under the States Reorganisation Act, states were reorganised on a linguistic basis.[262] There are over a quarter of a million local government bodies at city, town, block, district and village levels.[263]

States
Andhra Pradesh
Arunachal Pradesh
Assam
Bihar
Chhattisgarh
Goa
Gujarat
Haryana
Himachal Pradesh
Jharkhand
Karnataka
Kerala
Madhya Pradesh
Maharashtra
Manipur
Meghalaya
Mizoram
Nagaland
Odisha
Punjab
Rajasthan
Sikkim
Tamil Nadu
Telangana
Tripura
Uttar Pradesh
Uttarakhand
West Bengal
Union territories
Andaman and Nicobar Islands
Chandigarh
Dadra and Nagar Haveli and Daman and Diu
Jammu and Kashmir
Ladakh
Lakshadweep
National Capital Territory of Delhi
Puducherry
Foreign relations
Main article: Foreign relations of India

In the 1950s and 60s, India played a pivotal role in the Non-Aligned Movement.[264] From left to right: Gamal Abdel Nasser of United Arab Republic (now Egypt), Josip Broz Tito of Yugoslavia and Jawaharlal Nehru in Belgrade, September 1961.

The Indian Air Force contingent marching at the 221st Bastille Day military parade in Paris, July 2009. The parade at which India was the foreign guest was led by India's oldest regiment, the Maratha Light Infantry, founded in 1768.[265]
India became a republic in 1950, remaining a member of the Commonwealth of Nations.[266][267] India strongly supported decolonisation in Africa and Asia in the 1950s; it played a leading role in the Non-Aligned Movement.[268] After initially cordial relations, India suffered a humiliating military defeat to China in a 1962 war.[269] Another military conflict followed in 1967 in which India successfully repelled a Chinese attack.[270]

India has had uneasy relations with its western neighbour, Pakistan. The two countries went to war in 1947, 1965, 1971, and 1999. Three of these wars were fought over the disputed territory of Kashmir. In contrast, the 1971 war followed India's support for the independence of Bangladesh.[271] After the 1965 war with Pakistan, India began to pursue close military and economic ties with the Soviet Union. By the late 1960s, the Soviet Union was its largest arms supplier.[272] India has played a key role in the South Asian Association for Regional Cooperation and the World Trade Organization.[citation needed] The nation has supplied over 290,000 military and police personnel in over 50 UN peacekeeping operations.[273]

China's nuclear test of 1964 and threats to intervene in support of Pakistan in the 1965 war caused India to produce nuclear weapons.[274] India conducted its first nuclear weapons test in 1974 and carried out additional underground testing in 1998. India has signed neither the Comprehensive Nuclear-Test-Ban Treaty nor the Nuclear Non-Proliferation Treaty, considering both to be flawed and discriminatory.[275] India maintains a "no first use" nuclear policy and is developing a nuclear triad capability as a part of its "Minimum Credible Deterrence" doctrine.[276][277]

Since the end of the Cold War, India has increased its economic, strategic, and military cooperation with the United States and the European Union.[278] In 2008, a civilian nuclear agreement was signed between India and the United States. Although India possessed nuclear weapons at the time and was not a party to the Nuclear Non-Proliferation Treaty, it received waivers from the International Atomic Energy Agency and the Nuclear Suppliers Group, ending earlier restrictions on India's nuclear technology and commerce; India subsequently signed co-operation agreements involving civilian nuclear energy with Russia,[279] France,[280] the United Kingdom,[281] and Canada.[282]

The President of India is the supreme commander of the nation's armed forces. With 1.45 million active troops, they are the world's second-largest military. It comprises the Indian Army, the Indian Navy, the Indian Air Force, and the Indian Coast Guard.[283] The official Indian defence budget for 2011 was US$36.03 billion, or 1.83% of GDP.[284] Defence expenditure was pegged at US$70.12 billion for fiscal year 2022–23 and, increased 9.8% on the previous fiscal year.[285][286] India is the world's second-largest arms importer; between 2016 and 2020, it accounted for 9.5% of the total global arms imports.[287] Much of the military expenditure was focused on defence against Pakistan and countering growing Chinese influence in the Indian Ocean.[288]

Economy
Main article: Economy of India

In 2019, 43% of India's total workforce was employed in agriculture.[289]

India is the world's largest producer of milk, with the largest population of cattle. In 2018, nearly 80% of India's milk was sourced from small farms with herd size between one and two, the milk harvested by hand milking.[291]

55% of India's female workforce was employed in agriculture in 2019.[290]
According to the International Monetary Fund (IMF), the Indian economy in 2024 was nominally worth $3.94 trillion; it is the fifth-largest economy by market exchange rates and is, at around $15.0 trillion, the third-largest by purchasing power parity (PPP).[16] With its average annual GDP growth rate of 5.8% over the past two decades, and reaching 6.1% during 2011–2012,[292] India is one of the world's fastest-growing economies.[293] However, due to its low GDP per capita—which ranks 136th in the world in nominal per capita income and 125th in per capita income adjusted for purchasing power parity (PPP)—the vast majority of Indians fall into the low-income group.[294][295]

Until 1991, all Indian governments followed protectionist policies that were influenced by socialist economics. Widespread state intervention and regulation largely walled the economy off from the outside world. An acute balance of payments crisis in 1991 forced the nation to liberalise its economy;[296] since then, it has moved increasingly towards a free-market system[297][298] by emphasising both foreign trade and direct investment inflows.[299] India has been a member of World Trade Organization since 1 January 1995.[300]

The 522-million-worker Indian labour force is the world's second largest, as of 2017.[283] The service sector makes up 55.6% of GDP, the industrial sector 26.3% and the agricultural sector 18.1%. India's foreign exchange remittances of US$100 billion in 2022,[301] highest in the world, were contributed to its economy by 32 million Indians working in foreign countries.[302] In 2006, the share of external trade in India's GDP stood at 24%, up from 6% in 1985.[297] In 2008, India's share of world trade was 1.7%;[303] In 2021, India was the world's ninth-largest importer and the sixteenth-largest exporter.[304] Between 2001 and 2011, the contribution of petrochemical and engineering goods to total exports grew from 14% to 42%.[305] India was the world's second-largest textile exporter after China in the 2013 calendar year.[306]

Averaging an economic growth rate of 7.5% for several years before 2007,[297] India has more than doubled its hourly wage rates during the first decade of the 21st century.[307] Some 431 million Indians have left poverty since 1985; India's middle classes are projected to number around 580 million by 2030.[308] In 2024, India's consumer market was the world's third largest.[309] India's nominal GDP per capita increased steadily from US$308 in 1991, when economic liberalisation began, to US$1,380 in 2010, to an estimated US$2,731 in 2024. It is expected to grow to US$3,264 by 2026.[16]

Industries

A tea garden in Sikkim. India, the world's second-largest producer of tea, is a nation of one billion tea drinkers, who consume 70% of India's tea output.
The Indian automotive industry, the world's second-fastest growing, increased domestic sales by 26% during 2009–2010,[310] and exports by 36% during 2008–2009.[311] In 2022, India became the world's third-largest vehicle market after China and the United States, surpassing Japan.[312] At the end of 2011, the Indian IT industry employed 2.8 million professionals, generated revenues close to US$100 billion equalling 7.5% of Indian GDP, and contributed 26% of India's merchandise exports.[313]

The pharmaceutical industry in India includes 3,000 pharmaceutical companies and 10,500 manufacturing units; India is the world's third-largest pharmaceutical producer, largest producer of generic medicines and supply up to 50–60% of global vaccines demand, these all contribute up to US$24.44 billions in exports and India's local pharmaceutical market is estimated up to US$42 billion.[314][315] India is among the top 12 biotech destinations in the world.[316][317] The Indian biotech industry grew by 15.1% in 2012–2013, increasing its revenues from ₹204.4 billion (Indian rupees) to ₹235.24 billion (US$3.94 billion at June 2013 exchange rates).[318]

Energy
Main article: Energy in India
See also: Energy policy of India
India's capacity to generate electrical power is 300 gigawatts, of which 42 gigawatts is renewable.[319] The country's usage of coal is a major cause of India's greenhouse gas emissions, but its renewable energy is competing strongly.[320][better source needed] India emits about 7% of global greenhouse gas emissions. This equates to about 2.5 tons of carbon dioxide per person per year, which is half the world average.[321][322] Increasing access to electricity and clean cooking with liquefied petroleum gas have been priorities for energy in India.[323]

Socio-economic challenges
Main articles: Poverty in India, Income inequality in India, and Debt bondage in India

Health workers about to begin another day of immunisation against infectious diseases in 2006. Eight years later, and three years after India's last case of polio, the World Health Organization declared India to be polio-free.[324]
Despite economic growth during recent decades, India continues to face socio-economic challenges. In 2006, India contained the largest number of people living below the World Bank's international poverty line of US$1.25 per day.[325] The proportion decreased from 60% in 1981 to 42% in 2005.[326] Under the World Bank's later revised poverty line, it was 21%-22.5 in 2011.[p][328][329] In 2019, the estimates had gone down to 10.2%.[329] In 2014, 30.7% of India's children under the age of five were underweight.[330] According to a Food and Agriculture Organization report in 2015, 15% of the population was undernourished.[331][332] The Midday Meal Scheme attempts to lower these rates.[333]

A 2018 Walk Free Foundation report estimated that nearly 8 million people in India were living in different forms of modern slavery, such as bonded labour, child labour, human trafficking, and forced begging.[334] According to the 2011 census, there were 10.1 million child labourers in the country, a decline of 2.6 million from 12.6 million in 2001.[335]

Since 1991, economic inequality between India's states has consistently grown: the per-capita net state domestic product of the richest states in 2007 was 3.2 times that of the poorest.[336] Corruption in India is perceived to have decreased. According to the Corruption Perceptions Index, India ranked 78th out of 180 countries in 2018, an improvement from 85th in 2014.[337][338]
With an estimated 1,428,627,663 residents in 2023, India is the world's most populous country.[13] 1,210,193,422 residents were reported in the 2011 provisional census report.[339] Its population grew by 17.64% from 2001 to 2011,[340] compared to 21.54% growth in the previous decade (1991–2001).[340] The human sex ratio, according to the 2011 census, is 940 females per 1,000 males.[339] The median age was 28.7 in 2020.[283]

The first post-colonial census, conducted in 1951, counted 361 million people.[341] Medical advances made in the last 50 years as well as increased agricultural productivity brought about by the "Green Revolution" have caused India's population to grow rapidly.[342] The life expectancy in India is 70 years to 71.5 years for women, and 68.7 years for men.[283] There are around 93 physicians per 100,000 people.[343]

Urbanisation
Main article: Urbanisation in India
Migration from rural to urban areas has been an important dynamic in India's recent history. The number of people living in urban areas grew by 31.2% between 1991 and 2001.[344] In 2001, over 70% lived in rural areas.[345][346] The level of urbanisation increased further from 27.81% in the 2001 census to 31.16% in the 2011 census. The slowing down of the overall population growth rate was due to the sharp decline in the growth rate in rural areas since 1991.[347] In the 2011 census, there were 53 million-plus urban agglomerations in India. Among them Mumbai, Delhi, Kolkata, Chennai, Bengaluru, Hyderabad and Ahmedabad, in decreasing order by population.[348]

According to several air quality reports, 83 out of the top 100 most polluted cities in the world are located in India.[349][350][351][352][353]

Languages
Main article: Languages of India
Among speakers of the Indian languages, 74% speak Indo-Aryan languages (the easternmost branch of the Indo-European languages), 24% speak Dravidian languages (indigenous to South Asia and spoken widely before the spread of Indo-Aryan languages), and 2% speak Austroasiatic languages or the Sino-Tibetan languages. India has no national language.[354] Hindi, with the largest number of speakers, is the official language of the government.[355][356] English is used extensively in business and administration and has the status of a "subsidiary official language";[6] it is important in education, especially as a medium of higher education. Each state and union territory has one or more official languages, and the constitution recognises in particular 22 "scheduled languages".

Religion
Main article: Religion in India
The 2011 census reported the religion in India with the largest number of followers was Hinduism (79.80% of the population), followed by Islam (14.23%); the remaining were Christianity (2.30%), Sikhism (1.72%), Buddhism (0.70%), Jainism (0.36%) and others[q] (0.9%).[11] India has the third-largest Muslim population—the largest for a non-Muslim majority country.[357][358]

Education
Main article: Education in India
See also: Literacy in India and History of education in the Indian subcontinent

Children awaiting school lunch in Rayka (also Raika), a village in rural Gujarat. The salutation Jai Bhim written on the blackboard honours the jurist, social reformer, and Dalit leader B. R. Ambedkar.
The literacy rate in 2011 was 74.04%: 65.46% among females and 82.14% among males.[359] The rural-urban literacy gap, which was 21.2 percentage points in 2001, dropped to 16.1 percentage points in 2011. The improvement in the rural literacy rate is twice that of urban areas.[347] Kerala is the most literate state with 93.91% literacy; while Bihar the least with 63.82%.[359] In the 2011 census, about 73% of the population was literate, with 81% for men and 65% for women. This compares to 1981 when the respective rates were 41%, 53% and 29%. In 1951, the rates were 18%, 27% and 9%. In 1921, the rates 7%, 12% and 2%. In 1891, they were 5%, 9% and 1%,[360][361] According to Latika Chaudhary, in 1911 there were under three primary schools for every ten villages. Statistically, more caste and religious diversity reduced private spending. Primary schools taught literacy, so local diversity limited its growth.[362]

The education system of India is the world's second-largest.[363] India has over 900 universities, 40,000 colleges[364] and 1.5 million schools.[365] In India's higher education system, a significant number of seats are reserved under affirmative action policies for the historically disadvantaged. In recent decades India's improved education system is often cited as one of the main contributors to its economic development.[366][367]

Health
Main article: Health in India
The life expectancy at birth has increased from 49.7 years in 1970–1975 to 72.0 years in 2023.[368][369] The under-five mortality rate for the country was 113 per 1,000 live births in 1994 whereas in 2018 it reduced to 41.1 per 1,000 live births.[368]

India bears a disproportionately large burden of the world's tuberculosis rates, with World Health Organization (WHO) statistics for 2022 estimating 2.8 million new infections annually, accounting for 26% of the global total.[370] It is estimated that approximately 40% of the population of India carry tuberculosis infection.[371]

In 2018 chronic obstructive pulmonary disease was the leading cause of death after heart disease. The 10 most polluted cities in the world are all in India with more than 140 million people breathing air 10 times or more over the WHO safe limit. In 2017, air pollution killed 1.24 million Indians.[372]

Culture
Main article: Culture of India
Society
Main articles: Caste system in India and Gender inequality in India

Muslims offer namaz at a mosque in Srinagar, Jammu and Kashmir.
The Indian caste system embodies much of the social stratification and many of the social restrictions found on the Indian subcontinent. Social classes are defined by thousands of endogamous hereditary groups, often termed as jātis, or "castes".[373] India abolished untouchability in 1950 with the adoption of the constitution and has since enacted other anti-discriminatory laws and social welfare initiatives.[r] However, the system continues to be dominant in India, and caste-based inequality, discrimination, segregation, and violence persist.[375][376]

Multi-generational patrilineal joint families have been the norm in India, though nuclear families are becoming common in urban areas.[377] An overwhelming majority of Indians have their marriages arranged by their parents or other family elders.[378] Marriage is thought to be for life,[378] and the divorce rate is extremely low,[379] with less than one in a thousand marriages ending in divorce.[380] Child marriages are common, especially in rural areas; many women wed before reaching 18, which is their legal marriageable age.[381] Female infanticide in India, and lately female foeticide, have created skewed gender ratios; the number of missing women in the country quadrupled from 15 million to 63 million in the 50 years ending in 2014, faster than the population growth during the same period.[382] According to an Indian government study, an additional 21 million girls are unwanted and do not receive adequate care.[383] Despite a government ban on sex-selective foeticide, the practice remains commonplace in India, the result of a preference for boys in a patriarchal society.[384] The payment of dowry, although illegal, remains widespread across class lines.[385] Deaths resulting from dowry, mostly from bride burning, are on the rise, despite stringent anti-dowry laws.[386]

Visual art
Main article: Indian art
India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and Southeast Asia, the last also greatly influenced by Hindu art.[387] Thousands of seals from the Indus Valley civilisation of the third millennium BCE have been found, usually carved with animals, but also some with human figures. The Pashupati seal, excavated in Mohenjo-daro, Pakistan, in 1928–29, is the best known.[388][389] After this there is a long period with virtually nothing surviving.[389][390] Almost all surviving ancient Indian art thereafter is in various forms of religious sculpture in durable materials, or coins. There was probably originally far more in wood, which is lost. In north India Mauryan art is the first imperial movement.[391][392][393]

In the first millennium CE, Buddhist art spread with Indian religions to Central, East and Southeast Asia, the last also greatly influenced by Hindu art.[394] Over the following centuries a distinctly Indian style of sculpting the human figure developed, with less interest in articulating precise anatomy than ancient Greek sculpture but showing smoothly flowing forms expressing prana ("breath" or life-force).[395][396] This is often complicated by the need to give figures multiple arms or heads, or represent different genders on the left and right of figures, as with the Ardhanarishvara form of Shiva and Parvati.[397][398]

Most of the earliest large sculpture is Buddhist, either excavated from Buddhist stupas such as Sanchi, Sarnath and Amaravati,[399] or is rock cut reliefs at sites such as Ajanta, Karla and Ellora. Hindu and Jain sites appear rather later.[400][401] In spite of this complex mixture of religious traditions, generally, the prevailing artistic style at any time and place has been shared by the major religious groups, and sculptors probably usually served all communities.[402] Gupta art, at its peak c. 300 CE – c. 500 CE, is often regarded as a classical period whose influence lingered for many centuries after; it saw a new dominance of Hindu sculpture, as at the Elephanta Caves.[403][404] Across the north, this became rather stiff and formulaic after c. 800 CE, though rich with finely carved detail in the surrounds of statues.[405] But in the South, under the Pallava and Chola dynasties, sculpture in both stone and bronze had a sustained period of great achievement; the large bronzes with Shiva as Nataraja have become an iconic symbol of India.[406][407]

Ancient paintings have only survived at a few sites, of which the crowded scenes of court life in the Ajanta Caves are some of the most important.[408][409] Painted manuscripts of religious texts survive from Eastern India from 10th century onwards, most of the earliest being Buddhist and later Jain. These significantly influenced later artistic styles.[410] The Persian-derived Deccan painting, starting just before the Mughal miniature, between them give the first large body of secular painting, with an emphasis on portraits, and the recording of princely pleasures and wars.[411][412] The style spread to Hindu courts, especially among the Rajputs, and developed a variety of styles, with the smaller courts often the most innovative, with figures such as Nihâl Chand and Nainsukh.[413][414] As a market developed among European residents, it was supplied by Company painting by Indian artists with considerable Western influence.[415][416] In the 19th century, cheap Kalighat paintings of gods and everyday life, done on paper, were urban folk art from Calcutta, which later saw the Bengal School of Art, reflecting the art colleges founded by the British, the first movement in modern Indian painting.[417][418]

Bhutesvara Yakshis, Buddhist reliefs from Mathura, 2nd century CE
Bhutesvara Yakshis, Buddhist reliefs from Mathura, 2nd century CE
 
Gupta terracotta relief, Krishna Killing the Horse Demon Keshi, 5th century
Gupta terracotta relief, Krishna Killing the Horse Demon Keshi, 5th century
 
Elephanta Caves, triple-bust (trimurti) of Shiva, 18 feet (5.5 m) tall, c. 550
Elephanta Caves, triple-bust (trimurti) of Shiva, 18 feet (5.5 m) tall, c. 550
 
Chola bronze of Shiva as Nataraja ("Lord of Dance"), Tamil Nadu, 10th or 11th century
Chola bronze of Shiva as Nataraja ("Lord of Dance"), Tamil Nadu, 10th or 11th century
 
Jahangir Receives Prince Khurram at Ajmer on His Return from the Mewar Campaign, Balchand, c. 1635
Jahangir Receives Prince Khurram at Ajmer on His Return from the Mewar Campaign, Balchand, c. 1635
 
Krishna Fluting to the Milkmaids, Kangra painting, 1775–1785
Krishna Fluting to the Milkmaids, Kangra painting, 1775–1785
Music
Main article: Music of India
page is in the middle of an expansion or major revamping
This article or section is undergoing significant expansion or restructuring. You are welcome to assist in its construction by editing it as well. If this article or section has not been edited in several days, please remove this template.
If you are actively editing this article or section, you can replace this template with {{in use|5 minutes}}. This article was last edited by Fowler&fowler (talk | contribs) 0 seconds ago. (Update timer)
India contains a wide array of musical practices. Indian classical music has Vedic origins, and split in the 13th century into the two main traditions of Hindustani and Carnatic music. Hindustani is associated with North India and more improvisational, featuring instruments such as the sitar and tabla, and Carnatic is South Indian and more focused on written compositions such as the kriti, while both styles contain common elements such as the raga melodic framework.[419]
The game underwent major development in the 18th century to become England's national sport.[41] Its success was underwritten by the twin necessities of patronage and betting.[42] Cricket was prominent in London as early as 1707 and, in the middle years of the century, large crowds flocked to matches on the Artillery Ground in Finsbury.[citation needed] The single wicket form of the sport attracted huge crowds and wagers to match, its popularity peaking in the 1748 season.[43] Bowling underwent an evolution around 1760 when bowlers began to pitch (bounce) the ball instead of rolling or skimming it towards the batter. This caused a revolution in bat design because, to deal with the bouncing ball, it was necessary to introduce the modern straight bat in place of the old "hockey stick" shape.[44][citation needed]

The Hambledon Club was founded in the 1760s and, for the next twenty years until the formation of Marylebone Cricket Club (MCC) and the opening of Lord's Old Ground in 1787, Hambledon was both the game's greatest club and its focal point.[citation needed] MCC quickly became the sport's premier club and the custodian of the Laws of Cricket. New Laws introduced in the latter part of the 18th century include the three-stump wicket and leg before wicket (lbw).[45]

The 19th century saw underarm bowling superseded by first roundarm and then overarm bowling. Both developments were controversial.[46] Organisation of the game at county level led to the creation of the county clubs, starting with Sussex in 1839.[47] In December 1889, the eight leading county clubs formed the official County Championship, which began in 1890.[48]


The first recorded photo of a cricket match taken on 25 July 1857 by Roger Fenton
The most famous player of the 19th century was W. G. Grace, who started his long and influential career in 1865. It was especially during the career of Grace that the distinction between amateurs and professionals became blurred by the existence of players like him who were nominally amateur but, in terms of their financial gain, de facto professional. Grace himself was said to have been paid more money for playing cricket than any professional.[citation needed]

The last two decades before the First World War have been called the "Golden Age of cricket". It is a nostalgic name prompted by the collective sense of loss resulting from the war, but the period did produce some great players and memorable matches, especially as organised competition at county and Test level developed.[49]

Cricket becomes an international sport

The first English team to tour overseas, on board ship to North America, 1859
In 1844, the first international match took place between what were essentially club teams,[50] from the United States and Canada, in New York; Canada won.[51][52] In 1859, a team of English players went to North America on the first overseas tour.[53] Meanwhile, the British Empire had been instrumental in spreading the game overseas, and by the middle of the 19th century it had become well established in Australia, the Caribbean, British India (which also includes present-day Pakistan and Bangladesh), New Zealand, North America and South Africa.[54]

In 1862, an English team made the first tour of Australia.[55] The first Australian team to travel overseas consisted of Aboriginal stockmen who toured England in 1868.[56]

In 1876–77, an England team took part in what was retrospectively recognised as the first-ever Test match at the Melbourne Cricket Ground against Australia.[57] The rivalry between England and Australia gave birth to The Ashes in 1882, which remains Test cricket's most famous contest.[58] Test cricket began to expand in 1888–89 when South Africa played England.[59]

Cricket in the 20th century

Don Bradman of Australia had a record Test batting average of 99.94.
The inter-war years were dominated by Australia's Don Bradman, statistically the greatest Test batter of all time. To curb his dominance, England employed bodyline tactics during the 1932–33 Ashes series. These involved bowling at the body of the batter and setting a field, resulting in batters having to choose between being hit or risk getting out. This series moved cricket from a game to a matter of national importance, with diplomatic cables being passed between the two countries over the incident.[60]

During this time, the number of Test nations continued to grow, with the West Indies, New Zealand and India being admitted as full Test members within a four-year period from 1928 to 1932.

An enforced break during the Second World War stopped Test Cricket for a time, although the Partition of India caused Pakistan to gain Test status in 1952. As teams began to travel more, the game quickly grew from 500 tests in 84 years to 1000 within the next 23.

Cricket entered a new era in 1963 when English counties introduced the limited overs variant.[61] As it was sure to produce a result, limited overs cricket was lucrative, and the number of matches increased.[62] The first Limited Overs International was played in 1971, and the governing International Cricket Council (ICC), seeing its potential, staged the first limited overs Cricket World Cup in 1975.[63]

Sri Lanka joined the ranks in 1982. Meanwhile, South Africa was banned by the ICC due to apartheid from 1970 until 1992. 1992 also brought about the introduction of the Zimbabwe team.[64]

Cricket in the 21st century

The Indian Premier League (IPL) was launched in 2008. It has become one of the richest sports leagues in the world, and has greatly increased the importance of T20 cricket and franchise leagues.[65]
The 21st century brought with it the Bangladesh Team, who made their Test debut in 2000. The game itself also grew, with a new format made up of 20-over innings being created. This format, called T20 cricket, quickly became a highly popular format, putting the longer formats at risk. The new shorter format also introduced franchise cricket, with new tournaments like the Indian Premier League and the Australian Big Bash League. The ICC has selected the T20 format as cricket's growth format, and has introduced a T20 World Cup which is played every two years;[66] T20 cricket has also been increasingly accepted into major events such as the Asian Games.[67] The resultant growth has seen cricket's fanbase cross one billion people, with 90% of them in South Asia.[2] T20's success has also spawned even shorter formats, such as 10-over cricket (T10) and 100-ball cricket, though not without controversy.[68]

Outside factors have also taken their toll on cricket. For example, the 2008 Mumbai attacks led India and Pakistan to suspend their bilateral series indefinitely. The 2009 attack on the Sri Lankan team during their tour of Pakistan led to Pakistan being unable to host matches until 2019.[69][70][71][72]

In 2017, Afghanistan and Ireland became the 11th and 12th Test nations.[73][74]

Laws and gameplay
Main article: Laws of Cricket

A typical cricket field
In cricket, the rules of the game are codified in The Laws of Cricket (hereinafter called "the Laws"), which has a global remit. There are 42 Laws (always written with a capital "L"). The earliest known version of the code was drafted in 1744, and since 1788, it has been owned and maintained by its custodian, the Marylebone Cricket Club (MCC) in London.[75]

Playing area
Main articles: Cricket field, Cricket pitch, Crease (cricket), and Wicket
Cricket is a bat-and-ball game played on a cricket field (see image of cricket pitch and creases) between two teams of eleven players each.[76] The field is usually circular or oval in shape, and the edge of the playing area is marked by a boundary, which may be a fence, part of the stands, a rope, a painted line, or a combination of these; the boundary must if possible be marked along its entire length.[77]

In the approximate centre of the field is a rectangular pitch (see image, below) on which a wooden target called a wicket is sited at each end; the wickets are placed 22 yards (20 m) apart.[78] The pitch is a flat surface 10 feet (3.0 m) wide, with very short grass that tends to be worn away as the game progresses (cricket can also be played on artificial surfaces, notably matting). Each wicket is made of three wooden stumps topped by two bails.[79]


Cricket pitch and creases
As illustrated, the pitch is marked at each end with four white painted lines: a bowling crease, a popping crease and two return creases. The three stumps are aligned centrally on the bowling crease, which is eight feet eight inches long. The popping crease is drawn four feet in front of the bowling crease and parallel to it; although it is drawn as a 12 ft (3.7 m) line (six feet on either side of the wicket), it is, in fact, unlimited in length. The return creases are drawn at right angles to the popping crease so that they intersect the ends of the bowling crease; each return crease is drawn as an 8 ft (2.4 m) line, so that it extends four feet behind the bowling crease, but is also, in fact, unlimited in length.[80]

Match structure
Main article: Innings
Before a match begins, the team captains (who are also players) toss a coin to decide which team will bat first and so take the first innings.[81] "Innings" is the term used for each phase of play in the match.[81] In each innings, one team bats, attempting to score runs, while the other team bowls and fields the ball, attempting to restrict the scoring and dismiss the batters.[82][83] When the first innings ends, the teams change roles; there can be two to four innings depending upon the type of match. A match with four scheduled innings is played over three to five days; a match with two scheduled innings is usually completed in a single day.[81] During an innings, all eleven members of the fielding team take the field, but usually only two members of the batting team are on the field at any given time.[a] The order of batters is usually announced just before the match, but it can be varied.[76]

The main objective of each team is to score more runs than their opponents, but in some forms of cricket, it is also necessary to dismiss all but one of the opposition batters (making their team 'all out') in their final innings in order to win the match, which would otherwise be drawn (not ending with a winner or tie.)[86]

Clothing and equipment
Main article: Cricket clothing and equipment

English cricketer W. G. Grace "taking guard" in 1883. His pads and bat are very similar to those used today. The gloves have evolved somewhat. Many modern players use more defensive equipment than were available to Grace, most notably helmets and arm guards.
The wicket-keeper (a specialised fielder behind the batter) and the batters wear protective gear because of the hardness of the ball, which can be delivered at speeds of more than 145 kilometres per hour (90 mph) and presents a major health and safety concern. Protective clothing includes pads (designed to protect the knees and shins), batting gloves or wicket-keeper's gloves for the hands, a safety helmet for the head, and a box for male players inside the trousers (to protect the crotch area).[87] Some batters wear additional padding inside their shirts and trousers such as thigh pads, arm pads, rib protectors and shoulder pads. The only fielders allowed to wear protective gear are those in positions very close to the batter (i.e., if they are alongside or in front of him), but they cannot wear gloves or external leg guards.[88]

Subject to certain variations, on-field clothing generally includes a collared shirt with short or long sleeves; long trousers; woolen pullover (if needed); cricket cap (for fielding) or a safety helmet; and spiked shoes or boots to increase traction. The kit is traditionally all white, and this remains the case in Test and first-class cricket, but in limited overs cricket, team colours are now worn instead.[89]

Bat and ball
Main articles: Cricket bat and Cricket ball
New white ball
Used red ball
Used pink ball
The three types of cricket balls used in international matches, all of the same size:
i) A new white ball. White balls are mainly used in limited overs cricket, especially in matches played at night, under floodlights (left).
ii) A used red ball. Red balls are used in day Test cricket, first-class cricket and some other forms of cricket (center).

iii) A used pink ball. Pink balls are used in day/night Test cricket (right).
The essence of the sport is that a bowler delivers (i.e., bowls) the ball from their end of the pitch towards the batter who, armed with a bat, is "on strike" at the other end (see next sub-section: Basic gameplay).

The bat is made of wood, usually Salix alba (white willow), and has the shape of a blade topped by a cylindrical handle. The blade must not be more than 4.25 inches (10.8 cm) wide and the total length of the bat not more than 38 inches (97 cm). There is no standard for the weight, which is usually between 2 lb 7 oz and 3 lb (1.1 and 1.4 kg).[90][91]

The ball is a hard leather-seamed spheroid, with a circumference of 9 inches (23 cm). The ball has a "seam": six rows of stitches attaching the leather shell of the ball to the string and cork interior. The seam on a new ball is prominent and helps the bowler propel it in a less predictable manner. During matches, the quality of the ball deteriorates to a point where it is no longer usable; during the course of this deterioration, its behaviour in flight will change and can influence the outcome of the match. Players will, therefore, attempt to modify the ball's behaviour by modifying its physical properties. Polishing the ball and wetting it with sweat or saliva was legal, even when the polishing was deliberately done on one side only to increase the ball's swing through the air. The use of saliva has since been made illegal due to the COVID-19 pandemic.[92] The acts of rubbing other substances into the ball, scratching the surface or picking at the seams constitute illegal ball tampering.[93]

Player roles
Basic gameplay: bowler to batter
During normal play, thirteen players and two umpires are on the field. Two of the players are batters and the rest are all eleven members of the fielding team. The other nine players in the batting team are off the field in the pavilion. The image with overlay below shows what is happening when a ball is being bowled and which of the personnel are on or close to the pitch.[94]

123456778910111212
1	Umpire
2	Wicket
3	Non-striking batter
4	Bowler
5	Ball
6	Pitch
7	Popping crease
8	Striking batter
9	Wicket
10	Wicket-keeper
11	First slip
12	Return crease
In the photo, the two batters (3 and 8, wearing yellow) have taken position at each end of the pitch (6). Three members of the fielding team (4, 10 and 11, wearing dark blue) are in shot. One of the two umpires (1, wearing white hat) is stationed behind the wicket (2) at the bowler's (4) end of the pitch. The bowler (4) is bowling the ball (5) from his end of the pitch to the batter (8) at the other end who is called the "striker". The other batter (3) at the bowling end is called the "non-striker". The wicket-keeper (10), who is a specialist, is positioned behind the striker's wicket (9), and behind him stands one of the fielders in a position called "first slip" (11). While the bowler and the first slip are wearing conventional kit only, the two batters and the wicket-keeper are wearing protective gear, including safety helmets, padded gloves and leg guards (pads). The wicket-keeper is the only fielding player able to wear protective gloves.

While the umpire (1) in shot stands at the bowler's end of the pitch, his colleague stands in the outfield, usually in or near the fielding position called "square leg", so that he is in line with the popping crease (7) at the striker's end of the pitch. The bowling crease (not numbered) is the one on which the wicket is located between the return creases (12). The bowler (4) intends to hit the wicket (9) with the ball (5) or at least prevent the striker (8) from scoring runs. The striker (8) intends, by using his bat, to defend his wicket and, if possible, hit the ball away from the pitch in order to score runs.

Some players are skilled in both batting and bowling, so are termed all-rounders. Bowlers are classified according to their style and speed, generally as fast bowlers, seam bowlers or spinners. Batters are classified according to whether they are right-handed or left-handed, with switch-hitting uncommon and largely utilised as a tactic, where a batter changes stance shortly before the bowler releases the ball.[95]

Overs
Main article: Over (cricket)
The Laws state that, throughout an innings, "the ball shall be bowled from each end alternately in overs of 6 balls".[96] The name "over" came about because the umpire calls "Over!" when six legal balls (deliveries) have been bowled. At this point, another bowler is deployed at the other end, and the fielding side changes ends while the batters do not. A bowler cannot bowl two successive overs, although a bowler can (and usually does) bowl alternate overs, from the same end, for several overs which are termed a "spell"; if the captain wants a bowler to "change ends", another bowler must temporarily fill in so that the change is not immediate. The batters do not change ends at the end of the over, and so the one who was non-striker is now the striker and vice versa. The umpires also change positions so that the one who was at "square leg" now stands behind the wicket at the nonstriker's end and vice versa.[96]

Fielding
Main article: Fielding (cricket)

Fielding positions in cricket for a right-handed batter
Of the eleven fielders, three are in shot in the image above. The other eight are elsewhere on the field, their positions determined on a tactical basis by the captain or the bowler. Fielders often change position between deliveries, again as directed by the captain or bowler.[88]

If a fielder is injured or becomes ill during a match, a substitute is allowed to field instead of the aforementioned fielder, but the substitute cannot bowl or act as a captain, except in the case of concussion substitutes in international cricket.[85] The substitute leaves the field when the injured player is fit to return.[97] The Laws of Cricket were updated in 2017 to allow substitutes to act as wicket-keepers.[98]

Batting and scoring
Main articles: Batting (cricket), Run (cricket), and Extra (cricket)

The directions in which a right-handed batter, facing down the page, intends to send the ball when playing various cricketing shots. The diagram for a left-handed batter is a mirror image of this one.
Batters take turns to bat via a batting order which is decided beforehand by the team captain and presented to the umpires, though the order remains flexible when the captain officially nominates the team.[76] Substitute batters are generally not allowed,[97] except in the case of concussion substitutes in international cricket.[85]

In order to begin batting the batter first adopts a batting stance. Standardly, this involves adopting a slight crouch with the feet pointing across the front of the wicket, looking in the direction of the bowler, and holding the bat so it passes over the feet and so its tip can rest on the ground near to the toes of the back foot.[99]

A skilled batter can use a wide array of "shots" or "strokes" in both defensive and attacking mode. The idea is to hit the ball to the best effect with the flat surface of the bat's blade. If the ball touches the side of the bat, it is called an "edge". The batter does not have to play a shot and can allow the ball to go through to the wicket-keeper. Equally, the batter does not have to attempt a run when hitting the ball with their bat. Batters do not always seek to hit the ball as hard as possible, and a good player can score runs by simply making a deft stroke with a turn of the wrists, or by simply "blocking" the ball but directing it away from fielders so that the player has time to take a run. A wide variety of shots are played, the batter's repertoire including strokes named according to the style of swing and the direction aimed: e.g., "cut", "drive", "hook", and "pull".[100]


Sachin Tendulkar is the only player to have scored one hundred international centuries.
The batter on strike (i.e., the "striker") must prevent the ball from hitting the wicket and try to score runs by hitting the ball with their bat so that the batter and their partner have time to switch places, with each of them running from one end of the pitch to the other before the fielding side can return the ball and attempt a run out (throwing the ball at one of the wickets before the run is scored.) To register a run, both runners must touch the ground behind the popping crease with either their bats or their bodies (the batters carry their bats as they run) before a fielder can throw the ball at the nearby wicket. Each completed run increments the score of both the team and the striker.[101]
he decision to attempt a run is ideally made by the batter who has the better view of the ball's progress, and this is communicated by calling, usually "yes", "no" or "wait". More than one run can be scored from a single hit. Hits worth one to three runs are common, but the size of the field is such that it is usually difficult to run four or more.[101] To compensate for this, hits that reach the boundary of the field are automatically awarded four runs if the ball touches the ground en route to the boundary or six runs if the ball clears the boundary without touching the ground within the boundary. In these cases the batters do not need to run.[102] Hits for five are unusual and generally rely on the help of "overthrows" by a fielder returning the ball.


Batters attempting a run while a fielder awaits a throw.
If an odd number of runs is scored by the striker, the two batters have changed ends, and the one who was non-striker is now the striker. Only the striker can score individual runs, but all runs are added to the team's total.[101]

Additional runs can be gained by the batting team as extras (called "sundries" in Australia) due to errors made by the fielding side. This is achieved in four ways: no-ball, a penalty of one extra conceded by the bowler if they break the rules (often by failing to bowl the ball before their front foot passes the popping crease at their end);[103] wide, a penalty of one extra conceded by the bowler if they bowl so that the ball is out of the batter's reach;[104] bye, an extra awarded if the batter misses the ball and it goes past the wicket-keeper and gives the batters time to run in the conventional way;[105] and leg bye, as for a bye except that the ball has hit the batter's body, though not their bat.[105] If the bowler has bowled an illegal delivery (i.e., a no-ball or a wide), the bowler's team incurs an additional penalty because that ball (i.e., delivery) has to be bowled again, and hence the batting side has the opportunity to score more runs from this extra ball. In addition, the ways in which the batters can be dismissed on an illegal delivery greatly narrow down; in the case of a no-ball, which is the more egregious type of illegal delivery, the only common way in which the batters can be dismissed is by being run out.[103][104]

Dismissals
Main article: Dismissal (cricket)

The umpire signaling a dismissal
There are nine ways in which a batter can be dismissed: five relatively common and four extremely rare. When a batter is dismissed, they are said to have 'lost their wicket', and are barred from batting again in that inning; their team is also said to have 'lost a wicket'. Once a team has lost 10 wickets, its innings is over.


Most common dismissals involve the wickets, such as when the ball is bowled at the striker's wicket.
The common forms of dismissal are:

bowled – when the striker fails to prevent a delivery from hitting their wicket[106]
caught – when a ball struck by the bat is caught by a fielder before it hits the ground[107]
leg before wicket – when the striker's body 'unfairly' prevents a delivery from hitting the wicket[108]
run out – generally when the ball is thrown at a wicket by a fielder while the batters are running between the wickets[109]
stumped – a special type of run out involving the wicket-keeper and striker[110]
Rare methods are:

hit wicket – a striker hitting their own wicket[111]
hit the ball twice[112]
obstructing the field[113]
timed out – a batter failing to enter the field in a timely manner[114]
The Laws state that the fielding team, usually the bowler in practice, must appeal for a dismissal before the umpire can give their decision. If the batter is out, the umpire raises a forefinger and says "Out!"; otherwise, the umpire will shake their head and say "Not out".[115] There is, effectively, a tenth method of dismissal, retired out (self-dismissal - generally permanent except in cases of injury), which is not an on-field dismissal as such but rather a retrospective one for which no fielder is credited.[116]

Bowling
Main article: Bowling (cricket)

Bowlers generate momentum by running, and then release the ball upon reaching their "delivery stride".
Part of a series on
Bowling techniques
Fast bowling
Techniques:
SeamSwing
Deliveries:

BouncerInswingerKnuckle ballLeg cutterOff cutterOutswingerReverse swingSlower ballYorker
Spin bowling
Techniques:
Finger off spinleft-arm orthodox
Wrist leg spinleft-arm unorthodox
Deliveries:

Arm ballCarrom ballDoosraFlipperGooglyLeg breakOff breakSliderTeesraTopspinner
vte
Most bowlers are considered specialists in that they are selected for the team because of their skill as a bowler, although some are all-rounders, and even specialist batters bowl occasionally. These specialists bowl "spells" that are generally 4 to 8 overs long in order not to physically exhaust the bowler, cause muscle strain and stress the skeleton. The rules prevent a single bowler from bowling consecutive overs, resulting in at least two bowlers alternating each over. If the captain wants a bowler to "change ends", another bowler must temporarily fill in so that the change is not immediate.[96] The action of bowling the ball is akin to throwing, with the caveat that a bowler's elbow extension is almost entirely restricted, resulting in most bowlers maintaining a straight arm when releasing the ball during their delivery stride. Additionally, while the bowler is not required to pitch (bounce) the ball, a full toss (non-bouncing) delivery that reaches the striker above waist height is penalised as a no-ball.

A bowler reaches their delivery stride by means of a "run-up", and an over is deemed to have begun when the bowler starts their run-up for the first delivery of that over, the ball then being "in play".[96] Fast bowlers, or pacemen, need momentum, taking a lengthy run up, while bowlers with a slow delivery take no more than a couple of steps before bowling. The fastest bowlers can deliver the ball at a speed of over 145 kilometres per hour (90 mph), and they sometimes rely on sheer speed to try to defeat the batter, who is forced to react very quickly.[117] Other fast bowlers rely on a mixture of speed and guile by making the ball seam or swing (i.e., curve) in flight. This type of delivery can deceive a batter into miscuing their shot, for example, so that the ball just touches the edge of the bat and can then be "caught behind" by the wicket-keeper or a slip fielder.[117] At the other end of the bowling scale is the spin bowler, who bowls at a relatively slow pace and relies entirely on guile to deceive the batter. A spinner will often "buy their wicket" by "tossing one up" (in a slower, steeper parabolic path) to lure the batter into making a poor shot. The batter has to be very wary of such deliveries, as the batter is often "flighted" or spun so that the ball will not behave quite as the batter expects it to, and the batter could be "trapped" into getting themself out. Accidental full toss deliveries can also get wickets, as the failure of the ball to bounce can surprise a batsman or induce a poor stroke in an effort to punish the poor delivery with a boundary hit.[118] In between the pacemen and the spinners are the medium-paced seamers, who rely on persistent accuracy to try to contain the rate of scoring and wear down the batter's concentration.[117]

Specialist roles
Main articles: Captain (cricket) and Wicket-keeper

The wicket-keeper has gloves to receive deliveries that the striker fails to hit. He may also elect to wear protection such as leg guards and a helmet
The captain is often the most experienced player in the team, certainly the most tactically astute, and can possess any of the main skillsets as a batter, a bowler or a wicket-keeper. Within the Laws, the captain has certain responsibilities in terms of nominating their players to the umpires before the match and ensuring that the captain's players conduct themselves "within the spirit and traditions of the game as well as within the Laws".[76]

The wicket-keeper (sometimes called simply the "keeper") is a specialist fielder subject to various rules within the Laws about their equipment and demeanour. The wicket-keeper is the only member of the fielding side who can effect a stumping and is the only one permitted to wear gloves and external leg guards.[119]

Depending on their primary skills, the other ten players in the team tend to be classified as specialist batters or specialist bowlers. Generally, a team will include five or six specialist batters, and four or five specialist bowlers, plus the wicket-keeper.[120][121]

Match closure
Main article: Result (cricket)

Match results are sometimes determined by a final dismissal, either because the batting team's last wicket falls, or in a limited-overs match, the chance to score is denied on the final delivery.
There are a number of ways that a cricket match can end and its result be described, depending on whether the team batting first or last wins as well as the format of the game.

If the team batting last is 'all out' having scored fewer runs than their opponents, they are said to have "lost by n runs" (where n is the difference between the aggregate number of runs scored by the teams). If the team that bats last scores enough runs to win, it is said to have "won by n wickets", where n is the number of wickets left to fall (batters yet to be dismissed) until the team would have been all out. For example, a team that passes its opponents' total having lost six wickets (i.e., six of their batters have been dismissed) wins the match "by four wickets", since the team would only have been prevented from scoring the winning runs if four more of its batters had been dismissed, which would have resulted in all but one of its eleven batters being dismissed.[86]

In a two-innings-a-side match, one team's combined first and second innings total may be less than the other side's first innings total. The team with the greater score is then said to have "won by an innings and n runs" and does not need to bat again: n is the difference between the two teams' aggregate scores. If the team batting last is all out and both sides have scored the same number of runs, then the match is a tie; this result is quite rare in matches of two innings a side with only 62 happening in first-class matches from the earliest known instance in 1741 until January 2017. In the traditional form of the game, if the time allotted for the match expires before either side can win, then the game is declared a draw.[86]

If the match has only a single innings per side, then usually a maximum number of overs applies to each innings. Such a match is called a "limited overs" or "one-day" match, and the side scoring more runs wins regardless of the number of wickets lost, so that a draw cannot occur. In some cases, ties are broken by having each team bat for a one-over innings known as a Super Over; subsequent Super Overs may be played if the first Super Over ends in a tie. If this kind of match is temporarily interrupted by bad weather, then a complex mathematical formula, known as the Duckworth–Lewis–Stern (DLS) method after its developers, is often used to recalculate a new target score. A one-day match can also be declared a "no-result" if fewer than a previously agreed number of overs have been bowled by either team, in circumstances that make normal resumption of play impossible, for example, wet weather.[86]

In all forms of cricket, the umpires can abandon the match if bad light or rain makes it impossible to continue.[122] There have been instances of entire matches, even Test matches scheduled to be played over five days, being lost to bad weather without a ball being bowled, for example, the third Test of the 1970/71 series in Australia.[123]

Innings
Main article: Innings

The final innings of the match generally ends when the batting team either scores the winning runs (known as reaching its target score) or runs out of "resources" (either losing 10 wickets or running out of time/overs to bat) and thus can not win.
The innings (ending with 's' in both singular and plural form) is the term used for each phase of play during a match. Depending on the type of match being played, each team has either one or two innings. Sometimes all eleven members of the batting side take a turn to bat but, for various reasons, an innings can end before they have all done so. The innings terminates if the batting team is "all out", a term defined by the Laws: "At the fall of a wicket or the retirement of a batter, further balls remain to be bowled but no further batter is available to come in".[81] In this situation, one of the batters has not been dismissed and is termed not out; this is because he has no partners left and there must always be two active batters while the innings is in progress.

An innings may end early while there are still two not out batters:[81]

the batting team's captain may declare the innings closed, even though some of the captain's players have not had a turn to bat: this is a tactical decision by the captain, usually because the captain believes that their team have scored sufficient runs and need time to dismiss the opposition in their innings
the set number of overs (i.e., in a limited overs match) have been bowled
the match has ended prematurely due to bad weather or running out of time
in the final innings of the match, the batting side has reached its target (i.e., scored more runs than the opposition) and won the game.
Umpires and scorers
Main articles: Umpire (cricket), Scoring (cricket), and Cricket statistics

An umpire signals a decision to the scorers.

The Adelaide Oval cricket scoreboard during an Ashes Test in Australia
The game on the field is regulated by the two umpires, one of whom stands behind the wicket at the bowler's end and the other in a position called "square leg", which is about 15–20 m (49–66 ft) away from the batter on strike and in line with the popping crease on which that umpire is taking guard. The umpires have several responsibilities, including adjudication on whether a ball has been correctly bowled (i.e., not a no-ball or a wide); when a run is scored; whether a batter is out (the fielding side must first appeal to the umpire, usually with the phrase "How's that?" or "Howzat?"); when intervals start and end; and the suitability of the pitch, field and weather for playing the game. The umpires are authorised to interrupt or even abandon a match due to circumstances likely to endanger the players, such as a damp pitch or deterioration of the light.[122]

Off the field in televised matches, there is usually a third umpire who can make decisions on certain incidents with the aid of video evidence. The third umpire is mandatory under the playing conditions for Test and Limited Overs International matches played between two ICC full member countries. These matches also have a match referee whose job is to ensure that play is within the Laws and the spirit of the game.[122]

The match details, including runs and dismissals, are recorded by two official scorers, one representing each team. The scorers are directed by the hand signals of an umpire (see image, right). For example, the umpire raises a forefinger to signal that the batter is out (has been dismissed); the umpire raises both arms above their head if the batter has hit the ball for six runs. The scorers are required by the Laws to record all runs scored, wickets taken, and overs bowled; in practice, they also note significant amounts of additional data relating to the game.[124]

A match's statistics are summarised on a scorecard. Prior to the popularisation of scorecards, most scoring was done by men sitting on vantage points cuttings notches on tally sticks, and runs were originally called notches.[125] According to historian Rowland Bowen, the earliest known scorecard templates were introduced in 1776 by T. Pratt of Sevenoaks and soon came into general use.[126] It is believed that scorecards were printed and sold at Lord's for the first time in 1846.[127]run increments the score of both the team and the striker.[101]